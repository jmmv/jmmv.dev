<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>bazel on Julio Merino (jmmv.dev)</title><link>https://jmmv.dev/tags/bazel/index/</link><description>Recent content in bazel on Julio Merino (jmmv.dev)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 20 Oct 2023 09:00:00 -0700</lastBuildDate><atom:link href="https://jmmv.dev/tags/bazel/index/feed.xml" rel="self" type="application/rss+xml"/><item><title>Build farm visualizations</title><link>https://jmmv.dev/2023/10/build-farm-visualizations.html</link><pubDate>Fri, 20 Oct 2023 09:00:00 -0700</pubDate><guid>https://jmmv.dev/2023/10/build-farm-visualizations.html</guid><description>If you have followed our recent infrastructure posts, you know by now that we are actively migrating Snowflake&amp;rsquo;s build to Bazel. What we haven&amp;rsquo;t shared yet is that we have deployed our own Build Barn cluster to support Bazel&amp;rsquo;s remote execution features. We have chosen to run our own build farm service for resource governance and security purposes, but also because the behavior of this system impacts the developer experience so directly that we want to have full in-house control and knowledge of it.</description></item><item><title>Analyzing OOMs in IntelliJ with Bazel</title><link>https://jmmv.dev/2023/10/analyzing-ooms-in-intellij-with-bazel.html</link><pubDate>Sat, 07 Oct 2023 12:00:00 -0700</pubDate><guid>https://jmmv.dev/2023/10/analyzing-ooms-in-intellij-with-bazel.html</guid><description>&lt;p>A few months ago, we described how we fixed &lt;a href="https://jmmv.dev/2023/03/addressing-bazel-ooms.html">three different OOM scenarios&lt;/a> in our ongoing migration to the Bazel build system here at Snowflake. Things had been sailing along just fine since then&amp;hellip; but a new issue showed up recently: our &lt;a href="https://ij.bazel.build/">IntelliJ with Bazel (IjwB)&lt;/a> Java project started showing OOMs during its sync phase.&lt;/p>
&lt;p>The reason this issue surfaced now is because, as we continue our migration to Bazel, our IjwB project has grown in size. Months ago, our project only covered a Java binary, but now that we have migrated all of its unit and integration tests as well, the project covers them too. It is common for tests to be more expensive to build and run than the binary they validate&amp;mdash;tests depend on the binary&amp;rsquo;s dependencies &lt;em>plus&lt;/em> many other helper tools for testing&amp;mdash;and these caused the project to grow too big to fit in our development environments. Or did they?&lt;/p></description></item><item><title>Addressing Bazel OOMs</title><link>https://jmmv.dev/2023/03/addressing-bazel-ooms.html</link><pubDate>Thu, 16 Mar 2023 14:00:00 -0700</pubDate><guid>https://jmmv.dev/2023/03/addressing-bazel-ooms.html</guid><description>&lt;p>Here at Snowflake, the Developer Productivity organization (DPE for short) is tackling some important problems we face as a company: namely, lengthening build times and complex development environments. A key strategy we are pursuing to resolve these is the migration of key build processes from CMake and Maven to &lt;a href="https://bazel.build/">Bazel&lt;/a>.&lt;/p>
&lt;p>We are still in the early stages of this migration and cannot yet share many details or a success story, but we can start explaining some of the issues we encounter as we work through this ambitious project.&lt;/p></description><enclosure url="https://jmmv.dev/images/2023-03-16-header.jpg" length="345376" type="image/jpeg"/></item><item><title>Defining build time SLIs and SLOs</title><link>https://jmmv.dev/2021/03/build-time-slis-slos.html</link><pubDate>Fri, 12 Mar 2021 06:30:00 -0800</pubDate><guid>https://jmmv.dev/2021/03/build-time-slis-slos.html</guid><description>Companies grow, and with them do the software projects that support them. It should be no surprise that larger programs require longer build times. And, if I had to guess, you have seen how those build times eventually grow to unbearable levels, reducing productivity and degrading quality. In this post, I examine how we can leverage the common techniques we use for production services&amp;mdash;namely SLIs and SLOs&amp;mdash;to keep build times on track.</description></item><item><title>How does Google keep build times low?</title><link>https://jmmv.dev/2021/02/google-monorepos-and-caching.html</link><pubDate>Fri, 26 Feb 2021 09:50:00 -0800</pubDate><guid>https://jmmv.dev/2021/02/google-monorepos-and-caching.html</guid><description>Monorepos are an interesting beast. If mended properly, they enable a level of uniformity and code quality that is hard to achieve otherwise. If left unattended, however, they become unmanageable monsters of tangled dependencies, slow builds, and frustrating developer experiences. Whether you have a good or bad experience directly depends on the level of engineering support behind the monorepo. Simply put, monorepos require dedicated teams and tools to run nicely. In this post, I will look at how almost-perfect caching plays a key role in keeping build times manageable under such an environment.</description></item><item><title>How does Google avoid clean builds?</title><link>https://jmmv.dev/2020/12/google-no-clean-builds.html</link><pubDate>Thu, 31 Dec 2020 09:30:00 -0800</pubDate><guid>https://jmmv.dev/2020/12/google-no-clean-builds.html</guid><description>During my 11 years at Google, I can confidently count the number of times I had to do a &amp;ldquo;clean build&amp;rdquo; with one hand: their build system is so robust that incremental builds always work. Phrases like &amp;ldquo;clean everything and try building from scratch&amp;rdquo; are unheard of. So&amp;hellip; you can color me skeptical when someone says that incremental build problems are due to bugs in the build files and not due to a suboptimal build system. The answer lies in having a robust build system, and in this post I&amp;rsquo;ll examine the common causes behind incremental build breakages, what the build system can do to avoid them, and how Bazel accomplishes most of them.</description></item><item><title>The final boss: Bazel's own JNI code</title><link>https://jmmv.dev/2020/10/bazel-jni.html</link><pubDate>Fri, 09 Oct 2020 05:40:00 -0700</pubDate><guid>https://jmmv.dev/2020/10/bazel-jni.html</guid><description>As you might have read elsewhere, I&amp;rsquo;m leaving the Bazel team and Google in about a week. My plan for these last few weeks was to hand things off as cleanly as possible&amp;hellip; but I was also nerd-sniped by a bug that came my way a fortnight ago. Fixing it has been my self-inflicted punishment for leaving, and oh my, it has been painful. Very painful.
Let me tell you the story of this final boss.</description></item><item><title>Bazel output streaming, Ctrl+C, and test flakiness</title><link>https://jmmv.dev/2020/09/bazel-test-streaming-bug.html</link><pubDate>Fri, 18 Sep 2020 07:10:00 -0400</pubDate><guid>https://jmmv.dev/2020/09/bazel-test-streaming-bug.html</guid><description>About two weeks ago, I found a very interesting bug in Bazel&amp;rsquo;s test output streaming functionality while writing tests for a new feature related to Ctrl+C interrupts. I fixed the bug, wrote a test for it, and&amp;hellip; the test itself came back as flaky, which made me find another very subtle bug in the test that needed a one-line fix. This is the story of both.
Bazel has a feature known as test output streaming: by default, Bazel captures the outputs (stdout and stderr) of the tests it runs, saves those in local log files, and tells the user where they are when a test fails.</description></item><item><title>Bazel UI locking and file downloads</title><link>https://jmmv.dev/2020/09/bazel-ui-locking.html</link><pubDate>Tue, 01 Sep 2020 16:40:00 -0400</pubDate><guid>https://jmmv.dev/2020/09/bazel-ui-locking.html</guid><description>&lt;p>About a month ago, I was benchmarking the impact of a new Bazel feature and I noticed that a test build that should have taken only a few seconds took almost 10 minutes. My Internet connection was flaking out indeed, but something else didn&amp;rsquo;t seem right. So I looked and found that Bazel was doing network calls within a critical section, and these were the root cause behind the massive slowdown. But how did we get such an obvious no-no into the codebase? Read on to see how this happened and how gnarly it was to fix!&lt;/p></description></item><item><title>Shipping Bazel's new dynamic scheduler</title><link>https://jmmv.dev/2020/06/shipping-bazel-new-dynamic-scheduler.html</link><pubDate>Fri, 12 Jun 2020 16:00:00 -0400</pubDate><guid>https://jmmv.dev/2020/06/shipping-bazel-new-dynamic-scheduler.html</guid><description>Back in September 2019, I embarked into the task of rewriting Bazel&amp;rsquo;s dynamic scheduler to deal with slow and flaky networks. Initial testing had shown that dynamic builds might become slower, and it was all due to this feature having been designed for a different use case (in-office, high-speed network). We had to fix two different issues in the scheduler.
The first fix was making the downloads of the remote artifacts happen without holding the output lock.</description></item><item><title>Running codesign over SSH with a new key</title><link>https://jmmv.dev/2020/05/codesign-and-ssh.html</link><pubDate>Fri, 29 May 2020 15:30:00 -0400</pubDate><guid>https://jmmv.dev/2020/05/codesign-and-ssh.html</guid><description>I just spent sometime between 30 minutes and 1 hour convincing the Mac Pro that sits in my office to successfully codesign an iOS app via Bazel. This was after having to update the signing key to a newer one and after rebooting the machine due to the macOS 10.15.5 upgradeâ€”all remotely thanks to COVID-19.
The build of the app was failing with an errSecInternalComponent error printed by codesign. It is not the first time I face this, but in all previous cases, I had either been at the computer to click through security popups, had had functional Chrome Remote Desktop access, or did not have to install a new signing key remotely.</description></item><item><title>Ensuring system rewrites are truly necessary</title><link>https://jmmv.dev/2020/01/system-rewrites-and-tuning.html</link><pubDate>Fri, 24 Jan 2020 17:10:00 +0000</pubDate><guid>https://jmmv.dev/2020/01/system-rewrites-and-tuning.html</guid><description>You probably know that software rewrites, while very tempting, are expensive and can be the mistake that kills a project or a company. Yet they are routinely proposed as the solution to all problems. Is there anything you can do to minimize the risk? In this post, I propose that you actively improve the old system to ensure the new system cannot make progress in a haphazard way. This forces the new system to be designed in such a way that delivers breakthrough improvements and not just incremental improvements.</description></item><item><title>The OSXFUSE, hard links, and dladdr puzzle</title><link>https://jmmv.dev/2020/01/osxfuse-hardlinks-dladdr.html</link><pubDate>Fri, 17 Jan 2020 16:30:00 +0500</pubDate><guid>https://jmmv.dev/2020/01/osxfuse-hardlinks-dladdr.html</guid><description>Hello everyone and welcome to this new decade!
It&amp;rsquo;s already 2020 and I&amp;rsquo;m only 17 days late in writing a first post. I was planning to start with an opinion article, but as its draft is taking longer than I wanted&amp;hellip; I&amp;rsquo;ll present you the story of a recent crazy bug that has kept me busy for the last couple of days.
Java crashes with Bazel and sandboxfs On a machine running macOS Catalina, install sandboxfs and build Bazel with sandboxfs enabled, like this:</description></item><item><title>Tree artifacts and transient files</title><link>https://jmmv.dev/2019/12/bazel-dynamic-execution-tree-artifacts.html</link><pubDate>Tue, 31 Dec 2019 13:20:00 +0000</pubDate><guid>https://jmmv.dev/2019/12/bazel-dynamic-execution-tree-artifacts.html</guid><description>To conclude the deep dive into Bazel&amp;rsquo;s dynamic spawn strategy, let&amp;rsquo;s look at the nightmare that tree artifacts have been with the local lock-free feature. And, yes, I&amp;rsquo;m double-posting today because I really want to finish these series before the end of the decade1!
Tree artifacts are a fancy name for action outputs that are directories, not files. What&amp;rsquo;s special about them is that Bazel does not know a priori what the directory contents are: the rule behind the action just specifies that there will be a directory with files, and Bazel has to treat that as the unit of output from the action.</description></item><item><title>Lifting the local lock for dynamic execution</title><link>https://jmmv.dev/2019/12/bazel-dynamic-execution-local-lockfree.html</link><pubDate>Tue, 31 Dec 2019 07:00:00 +0000</pubDate><guid>https://jmmv.dev/2019/12/bazel-dynamic-execution-local-lockfree.html</guid><description>In the previous post, we saw how accounting for artifact download times makes the dynamic strategy live to its promise of delivering the best of local and remote build times.
Or does it? If you think about it closely, that change made it so that builds that were purely local couldn&amp;rsquo;t be made worse by enabling the dynamic scheduler: the dynamic strategy would always favor the local branch of a spawn if the remote one took a long time.</description></item></channel></rss>