<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Performance on Julio Merino (jmmv.dev)</title><link>https://jmmv.dev/tags/performance/index.html</link><description>Recent content in Performance on Julio Merino (jmmv.dev)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 22 Mar 2019 15:00:00 +0100</lastBuildDate><atom:link href="https://jmmv.dev/tags/performance/index/feed.xml" rel="self" type="application/rss+xml"/><item><title>Optimizing tree deletions in Bazel</title><link>https://jmmv.dev/2019/03/optimizing-tree-deletions.html</link><pubDate>Fri, 22 Mar 2019 15:00:00 +0100</pubDate><guid>https://jmmv.dev/2019/03/optimizing-tree-deletions.html</guid><description>&lt;p>Bazel likes creating very deep and large trees on disk during a build. One example is the output tree, which naturally contains all the artifacts of your build. Another, more problematic example is the symlink forest trees created for every action when sandboxing is enabled. As garbage gets created, it must be deleted.&lt;/p>
&lt;p>It turns out, however, that deleting file system trees can be very expensive&amp;mdash;and especially so on macOS. In fact, calls to our &lt;code>deleteTree&lt;/code> algorithm routinely showed up in my profiling runs when trying to diagnose slowdowns using the &lt;a href="https://blog.bazel.build/2019/02/01/dynamic-spawn-scheduler.html">dynamic scheduler&lt;/a>. One thing I quickly wondered is: why can I easily catch Bazel stuck in the tree deletion but I can never catch it busily creating such a tree? Is tree deletion inherently slow or are we doing something stupid?&lt;/p></description></item><item><title>Darwin's QoS service classes and performance</title><link>https://jmmv.dev/2019/03/macos-threads-qos-and-bazel.html</link><pubDate>Wed, 06 Mar 2019 17:30:00 +0100</pubDate><guid>https://jmmv.dev/2019/03/macos-threads-qos-and-bazel.html</guid><description>&lt;p>Since the publication of Bazel a few years ago, users have reported (and I myself have experienced) general slowdowns when Bazel is running on Macs: things like the window manager stutter and others like the web browser cannot load new pages. Similarly, after the introduction of the &lt;a href="https://blog.bazel.build/2019/02/01/dynamic-spawn-scheduler.html">dynamic spawn scheduler&lt;/a>, some users reported &lt;em>slower&lt;/em> builds than pure remote or pure local builds, which made no sense.&lt;/p>
&lt;p>All along we guessed that these problems were caused by Bazel&amp;rsquo;s abuse of system threads, as it used to spawn 200 &lt;em>runnable&lt;/em> threads during analysis and used to run 200 concurrent compiler subprocesses. We tackled the problem by reducing Bazel&amp;rsquo;s abuse (e.g. &lt;a href="https://github.com/bazelbuild/bazel/commit/ac880418885061d1039ad6b3d8c28949782e02d6">commit ac88041&lt;/a>) of system resources&amp;hellip; and while we saw an improvement, the issue remained.&lt;/p></description></item><item><title>CVS and fragmentation</title><link>https://jmmv.dev/2007/01/cvs-and-fragmentation.html</link><pubDate>Sun, 07 Jan 2007 10:07:00 -0500</pubDate><guid>https://jmmv.dev/2007/01/cvs-and-fragmentation.html</guid><description>First of all, happy new year to everybody!&lt;br />&lt;br />I've recently got a &lt;a href="http://www.apple.com/macbookpro/">MacBook Pro&lt;/a> and, while this little machine is great overall, the 5400 RPM hard disk is a noticeable performance bottleneck. Many people I've talked to say that the difference from 5400 to 7200 RPM should not be noticeable because:&lt;ul>&lt;li>These 2.5-inch drives use perpendicular recording, hence storing data with a higher bit density. This means that, theorically, they can read/write data more quickly achieving speeds similar to 7200 RPM drives.&lt;/li>&lt;li>Modern file systems prevent fragmentation, &lt;a href="http://docs.info.apple.com/article.html?artnum=25668">as described here for HFS+&lt;/a>.&lt;/li>&lt;/ul>To me, these two reasons are valid as long as you manage large files: the file system will try to keep them physically close and the disk will be able to transfer sequential data fairly quickly.&lt;br />&lt;br />But unfortunately, these ideas break when you have to deal with thousands of tiny files around (or when you flood the drive with requests from different applications, but this is not what I want to talk about today). The easiest way to demonstrate this is to use CVS to manage a copy of &lt;a href="http://www.pkgsrc.org/">pkgsrc&lt;/a> on such drives.&lt;br />&lt;br />Let's start by checking out a fresh copy of pkgsrc from the CVS repository. As long as the file system has a lot of free space (and has not been "polluted" by erased files), this will run quite fast because it will store all new files physically close (theorically in consecutive cylinders). Hence, we take advantage of the higher bit densities and the file system's file allocation policy. Just after the check out operation (or unarchiving of a tarball of the tree), run an update (&lt;tt>cvs -z3 -q update -dP&lt;/tt>) and write down the amount of time it takes. In my specific tests, the update took around 5 minutes, which is a good measure; in fact, it is almost the same I got in my desktop machine with a 7200 RPM disk.&lt;br />&lt;br />Now start using pkgsrc by building a "big" package; I've been doing tests with mencoder, which has a bunch of dependencies and boost, which installs a ton of files. The object files generated during the builds, as well as the resulting files, will be physically stored "after" pkgsrc. It is likely that there will be "holes" in the disk because you'll be removing the work directories but not the installed files, which will result in a lot of files stored non-contiguously. To make things worse, keep using your machine for a couple of days.&lt;br />&lt;br />Then, do another update of the whole tree. In my specific tests, the process now takes around 10 minutes. Yes, it has doubled the original measure. This problem was also present with faster disks, but not as noticeable. But do we have to blame the drive for such a slowdown or maybe, just maybe, it is CVS's fault?&lt;br />&lt;br />The pkgsrc repository contains &lt;i>lots&lt;/i> of empty directories that were once populated. However, CVS does not handle such entries very well. During an update, CVS recreates these empty directories locally and, at the end of the process, it erases them provided that you passed the &lt;tt>-P&lt;/tt> (prune) option. Furthermore, every such directory will end up consuming, at least, 5 inodes on the local disk because it will contain a &lt;tt>CVS&lt;/tt> control directory (which typically stores 3 tiny files). This continuous creation and deletion of directories and files fragment the original tree by spreading the updated files all around.&lt;br />&lt;br />Sincerely, I don't know &lt;i>why&lt;/i> CVS works like this (anyone?), but I bet that switching to a superior VCS could mitigate this problem. A temporary solution can be the usage of disk images, holding each source tree individually and keeping its total size as tight as possible. This way one can expect the image to be permanently stored in a contiguous disk area.&lt;br />&lt;br />Oh, and by the way: Boot Camp really suffers from the slow drive because it creates the Windows partition at the end of the disk; that is, its inner part, which typically has slower access times. (Well, I'm not sure if it'd make any difference if the partition was created at the beginning.) Launching a game such as Half-Life 2 takes forever; fortunately, when it is up it is fast enough.&lt;br />&lt;br />&lt;b>Update (January 9th)&lt;/b>: As "r." kindly points out, the slower part of the disk is the inner one, not the outer one as I had previously written (had a lapsus because CDs are written the other way around). And the reason is this: current disks use &lt;a href="http://en.wikipedia.org/wiki/Zone_bit_recording">Zone Bit Recording (ZBR)&lt;/a>, a technique that fits a different amount of sectors depeding on the track's length. Hence, outer (longer) tracks have more sectors allocated to them and can transfer more data in a single disk rotation.</description></item></channel></rss>