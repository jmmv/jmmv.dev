<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Julio Merino</title>
    <description>This is Julio Merino's homepage.  This site is primarily my personal blog and, secondarily, a gateway to my presence in the web.
</description>
    <link>http://julio.meroh.net/</link>
    <atom:link href="http://julio.meroh.net/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 25 Aug 2017 20:29:00 +0000</pubDate>
    <lastBuildDate>Fri, 25 Aug 2017 20:29:00 +0000</lastBuildDate>
    <generator>Jekyll v3.5.0</generator>
    
      <item>
        <title>Introducing sandboxfs</title>
        
          <description>
            sandboxfs is a FUSE-based file system that exposes an arbitrary view of the
host’s file system under the mount point, and offers access controls that
differ from those of the host. You can think of sandboxfs as an advanced
version of bindfs (or mount --bind or mount_null(8)
depending on your system) in which you can combine and nest directories under
an arbitrary layout.

The primary use case for this project is to provide a better file system
sandboxing technique for the Bazel build system. The goal here is to run each
build action (think compiler invocation) in a sandbox so that its inputs and
outputs are tightly controlled, and sandboxfs attempts to do this in a more
efficient manner than the current symlinks-based implementation.

But what makes sandboxfs more exciting are its potential secondary use cases
outside of Bazel. For example, my personal goal is to use sandboxfs within
pkg_comp 2.x to simplify
the logic in isolating pkgsrc builds and make the isolation more robust and
accurate.

So how did this happen? This summer I had the pleasure of hosting Pallav
Agarwal as an intern in the
Bazel team working from our Google NYC office. I
leveraged my experience last year with sourcachefs to design and lead the implementation of
sandboxfs. With this and Pallav’s strong skills, we got to a feature-complete
implementation by the beginning of August.

And today, after a bit of extra work to make the full codebase open-sourceable
and collecting all necessary approvals, I am happy to announce that sandboxfs
is public!

Read https://blog.bazel.build/2017/08/25/introducing-sandboxfs.html for more details and head to the
https://github.com/bazelbuild/sandboxfs/ to check out the code.

Enjoy and stay tuned for further integrations! sandboxfs is a cool thing on its
own, but until it serves as the base for builds within Bazel and pkg_comp, it’s
not of much use. Or is it? If you have other use cases, let me know!

            &lt;a href=&quot;http://julio.meroh.net/2017/08/introducing-sandboxfs.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;sandboxfs is a FUSE-based file system that exposes an arbitrary view of the
host’s file system under the mount point, and offers access controls that
differ from those of the host. You can think of sandboxfs as an advanced
version of &lt;a href=&quot;https://bindfs.org/&quot;&gt;bindfs&lt;/a&gt; (or &lt;code class=&quot;highlighter-rouge&quot;&gt;mount --bind&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;mount_null(8)&lt;/code&gt;
depending on your system) in which you can combine and nest directories under
an arbitrary layout.&lt;/p&gt;

&lt;p&gt;The primary use case for this project is to provide a better file system
sandboxing technique for the Bazel build system. The goal here is to run each
build action (think compiler invocation) in a sandbox so that its inputs and
outputs are tightly controlled, and sandboxfs attempts to do this in a more
efficient manner than the current symlinks-based implementation.&lt;/p&gt;

&lt;p&gt;But what makes sandboxfs more exciting are its potential secondary use cases
outside of Bazel. For example, my personal goal is to use sandboxfs within
&lt;a href=&quot;/2017/02/introducing-pkg_comp-2.0.html&quot;&gt;pkg_comp 2.x&lt;/a&gt; to simplify
the logic in isolating pkgsrc builds and make the isolation more robust and
accurate.&lt;/p&gt;

&lt;p&gt;So how did this happen? This summer I had the pleasure of hosting &lt;a href=&quot;https://github.com/pallavagarwal07&quot;&gt;Pallav
Agarwal&lt;/a&gt; as an intern in the
&lt;a href=&quot;https://bazel.build/&quot;&gt;Bazel&lt;/a&gt; team working from our Google NYC office. I
leveraged my &lt;a href=&quot;/2017/07/introducing-sourcachefs.html&quot;&gt;experience last year with sourcachefs&lt;/a&gt; to design and lead the implementation of
sandboxfs. With this and Pallav’s strong skills, we got to a feature-complete
implementation by the beginning of August.&lt;/p&gt;

&lt;p&gt;And today, after a bit of extra work to make the full codebase open-sourceable
and collecting all necessary approvals, I am happy to announce that sandboxfs
is public!&lt;/p&gt;

&lt;p&gt;Read &lt;a href=&quot;the
official announcement&quot;&gt;https://blog.bazel.build/2017/08/25/introducing-sandboxfs.html&lt;/a&gt; for more details and head to the
&lt;a href=&quot;project page&quot;&gt;https://github.com/bazelbuild/sandboxfs/&lt;/a&gt; to check out the code.&lt;/p&gt;

&lt;p&gt;Enjoy and stay tuned for further integrations! sandboxfs is a cool thing on its
own, but until it serves as the base for builds within Bazel and pkg_comp, it’s
not of much use. Or is it? If you have other use cases, let me know!&lt;/p&gt;
</description>
        
        <pubDate>Fri, 25 Aug 2017 20:25:18 +0000</pubDate>
        <link>http://julio.meroh.net/2017/08/introducing-sandboxfs.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2017/08/introducing-sandboxfs.html</guid>
        
        
        <category>bazel</category>
        
        <category>software</category>
        
      </item>
    
      <item>
        <title>Introducing sourcachefs</title>
        
          <description>
            You may remember a post from over a year ago titled Analysis of SSHFS performance for large builds, in which I outlined how Google exposes its gigantic source monorepo via a FUSE file system and in which I analyzed the performance of large builds using SSHFS to access such file system.

As part of those experiments, I played with pCacheFS, a Python-based FUSE file system that provides a persistent caching layer on to top of a slow mount point.  Pure SSHFS benchmarks were poor performance-wise, and pCacheFS benchmarks were not better.  In the former case, the issues were because of the lack of caching and single-threaded server operation, and in the latter because of slow performance of a CPU-bound multi-threaded Python app.

As a result, I wrote a new file system to act as a persistent caching layer on top of an SSHFS mount, and I called that file system sourcachefs (note the missing e for a cool pun).  sourcachefs implements the basic same idea behind pCacheFS but in Go for efficiency, with more features, and with a lot of performance tuning into it.

Further experiments with sourcachefs did show up performance improvements in the builds involved, but sourcachefs was never deployed widely within Google for complex reasons I won’t get into (plus we got a much better solution to the problem we were originally facing).  At this point, sourcachefs is just a personal project of mine that has no involvement with my daily job, but very soon you’ll see the publication of a different project that shares a lot in common with sourcachefs!

I digress.  Today, after over a year of finding little chunks of time during long flights to clean up the code base, it has become minimally worthy of publication… so I have pushed the code to GitHub.  Would be a pity for it to be lost, even if it’s just my first Go incomplete experiment.

Head to http://github.com/jmmv/sourcachefs/ for details.

— Julio, from DL439 at 36000 feet.

            &lt;a href=&quot;http://julio.meroh.net/2017/07/introducing-sourcachefs.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;You may remember a post from over a year ago titled &lt;a href=&quot;/2016/02/sshfs-performance-analysis-for-builds.html&quot;&gt;Analysis of SSHFS performance for large builds&lt;/a&gt;, in which I outlined how Google exposes its gigantic source monorepo via a FUSE file system and in which I analyzed the performance of large builds using SSHFS to access such file system.&lt;/p&gt;

&lt;p&gt;As part of those experiments, I played with &lt;a href=&quot;https://github.com/ibizaman/pcachefs&quot;&gt;pCacheFS&lt;/a&gt;, a Python-based FUSE file system that provides a persistent caching layer on to top of a slow mount point.  Pure SSHFS benchmarks were poor performance-wise, and pCacheFS benchmarks were not better.  In the former case, the issues were because of the lack of caching and single-threaded server operation, and in the latter because of slow performance of a CPU-bound multi-threaded Python app.&lt;/p&gt;

&lt;p&gt;As a result, I wrote a new file system to act as a persistent caching layer on top of an SSHFS mount, and I called that file system &lt;strong&gt;sourcachefs&lt;/strong&gt; (note the missing &lt;em&gt;e&lt;/em&gt; for a cool pun).  sourcachefs implements the basic same idea behind pCacheFS but in Go for efficiency, with more features, and with a lot of performance tuning into it.&lt;/p&gt;

&lt;p&gt;Further experiments with sourcachefs &lt;em&gt;did&lt;/em&gt; show up performance improvements in the builds involved, but sourcachefs was never deployed widely within Google for complex reasons I won’t get into (plus we got a much better solution to the problem we were originally facing).  At this point, sourcachefs is just a personal project of mine that has no involvement with my daily job, but very soon you’ll see the publication of a different project that shares a lot in common with sourcachefs!&lt;/p&gt;

&lt;p&gt;I digress.  Today, after over a year of finding little chunks of time during long flights to clean up the code base, it has become minimally worthy of publication… so I have pushed the code to GitHub.  Would be a pity for it to be lost, even if it’s just my first Go incomplete experiment.&lt;/p&gt;

&lt;p&gt;Head to &lt;a href=&quot;http://github.com/jmmv/sourcachefs/&quot;&gt;http://github.com/jmmv/sourcachefs/&lt;/a&gt; for details.&lt;/p&gt;

&lt;p&gt;— Julio, from &lt;a href=&quot;https://flightaware.com/live/flight/DAL439/history/20170730/1925Z/KJFK/KSFO&quot;&gt;DL439&lt;/a&gt; at 36000 feet.&lt;/p&gt;
</description>
        
        <pubDate>Mon, 31 Jul 2017 00:30:00 +0000</pubDate>
        <link>http://julio.meroh.net/2017/07/introducing-sourcachefs.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2017/07/introducing-sourcachefs.html</guid>
        
        
        <category>software</category>
        
      </item>
    
      <item>
        <title>Easy pkgsrc on macOS with pkg_comp 2.0</title>
        
          <description>
            This is a tutorial to guide you through the shiny new pkg_comp 2.0 on macOS using the macOS-specific self-installer.

Goals: to use pkg_comp 2.0 to build a binary repository of all the packages you are interested in; to keep the repository fresh on a daily basis; and to use that repository with pkgin to maintain your macOS system up-to-date and secure.



This tutorial is specifically targeted at macOS and relies on the macOS-specific self-installer package.  For a more generic tutorial that uses the pkg_comp-cron package in pkgsrc, see Keeping NetBSD up-to-date with pkg_comp 2.0.

Getting started

First download and install the standalone macOS installer package.  To find the right file, navigate to the releases page on GitHub, pick the most recent release, and download the file with a name of the form pkg_comp-&lt;version&gt;-macos.pkg.

Then double-click on the file you downloaded and follow the installation instructions.  You will be asked for your administrator password because the installer has to place files under /usr/local/; note that pkg_comp requires root privileges anyway to run (because it uses chroot(8) internally), so you will have to grant permission at some point or another.

The installer modifies the default PATH (by creating /etc/paths.d/pkg_comp) to include pkg_comp’s own installation directory and pkgsrc’s installation prefix.  Restart your shell sessions to make this change effective, or update your own shell startup scripts accordingly if you don’t use the standard ones.

Lastly, make sure to have Xcode installed in the standard /Applications/Xcode.app location and that all components required to build command-line apps are available.  Tip: try running cc from the command line and seeing if it prints its usage message.

Adjusting the configuration

The macOS flavor of pkg_comp is configured with an installation prefix of /usr/local/, which means that the executable is located in /usr/local/sbin/pkg_comp and the configuration files are in /usr/local/etc/pkg_comp/.  This is intentional to keep the pkg_comp installation separate from your pkgsrc installation so that it can run no matter what state your pkgsrc installation is in.

The configuration files are as follows:


  
    /usr/local/etc/pkg_comp/default.conf: This is pkg_comp’s own configuration file and the defaults configured by the installer should be good to go for macOS.

    In particular, packages are configured to go into /opt/pkg/ instead of the traditional /usr/pkg/.  This is a necessity because the latter is not writable starting with OS X El Capitan thanks to System Integrity Protection (SIP).
  
  
    /usr/local/etc/pkg_comp/sandbox.conf: This is the configuration file for sandboxctl, which is the support tool that pkg_comp uses to manage the compilation sandbox.  The default settings configured by the installer should be good.
  
  
    /usr/local/etc/pkg_comp/extra.mk.conf: This is pkgsrc’s own configuration file.  In here, you should configure things like the licenses that are acceptable to you and the package-specific options you’d like to set.  You should not configure the layout of the installed files (e.g. LOCALBASE) because that’s handled internally by pkg_comp as specified in default.conf.
  
  
    /usr/local/etc/pkg_comp/list.txt: This determines the set of packages you want to build automatically (either via the auto command or your periodic cron job).  The automated builds will fail unless you list at least one package.

    Make sure to list pkgin here to install a better binary package management tool.  You’ll find this very handy to keep your installation up-to-date.
  


Note that these configuration files use the /var/pkg_comp/ directory as the dumping ground for: the pkgsrc tree, the downloaded distribution files, and the built binary packages.  We will see references to this location later on.

The cron job

The installer configures a cron job that runs as root to invoke pkg_comp daily.  The goal of this cron job is to keep your local packages repository up-to-date so that you can do binary upgrades at any time.  You can edit the cron job configuration interactively by running sudo crontab -e.

This cron job won’t have an effect until you have populated the list.txt file as described above, so it’s safe to let it enabled until you have configured pkg_comp.

If you want to disable the periodic builds, just remove the pkg_comp entry from the crontab.

On slow machines, or if you are building a lot of packages, you may want to consider decreasing the build frequency from @daily to @weekly.

Sample configuration

Here is what the configuration looks like on my Mac Mini as dumped by the config subcommand.  Use this output to get an idea of what to expect.  I’ll be using the values shown here in the rest of the tutorial:

$ pkg_comp config
AUTO_PACKAGES = autoconf automake bash colordiff dash emacs24-nox11 emacs25-nox11 fuse-bindfs fuse-sshfs fuse-unionfs gdb git-base git-docs glib2 gmake gnuls libtool-base lua52 mercurial mozilla-rootcerts mysql56-server pdksh pkg_developer pkgconf pkgin ruby-jekyll ruby-jekyll-archives ruby-jekyll-paginate scmcvs smartmontools sqlite3 tmux vim
CVS_ROOT = :ext:anoncvs@anoncvs.NetBSD.org:/cvsroot
CVS_TAG is undefined
DISTDIR = /var/pkg_comp/distfiles
EXTRA_MKCONF = /usr/local/etc/pkg_comp/extra.mk.conf
FETCH_VCS = git
GIT_BRANCH = trunk
GIT_URL = https://github.com/jsonn/pkgsrc.git
LOCALBASE = /opt/pkg
NJOBS = 4
PACKAGES = /var/pkg_comp/packages
PBULK_PACKAGES = /var/pkg_comp/pbulk-packages
PKG_DBDIR = /opt/pkg/libdata/pkgdb
PKGSRCDIR = /var/pkg_comp/pkgsrc
SANDBOX_CONFFILE = /usr/local/etc/pkg_comp/sandbox.conf
SYSCONFDIR = /opt/pkg/etc
UPDATE_SOURCES = true
VARBASE = /opt/pkg/var

DARWIN_NATIVE_WITH_XCODE = true
SANDBOX_ROOT = /var/pkg_comp/sandbox
SANDBOX_TYPE = darwin-native



Building your own packages by hand

Now that you are fully installed and configured, you’ll build some stuff by hand to ensure the setup works before the cron job comes in.

The simplest usage form, which involves full automation and assumes you have listed at least one package in list.txt, is something like this:

$ sudo pkg_comp auto



This trivially-looking command will:


  clone or update your copy of pkgsrc;
  create the sandbox;
  bootstrap pkgsrc and pbulk;
  use pbulk to build the given packages; and
  destroy the sandbox.


After a successful invocation, you’ll be left with a collection of packages in the /var/pkg_comp/packages/ directory.

If you’d like to restrict the set of packages to build during a manually-triggered build, provide those as arguments to auto.  This will override the contents of AUTO_PACKAGES (which was derived from your list.txt file).

But what if you wanted to invoke all stages separately, bypassing auto?  The command above would be equivalent to:

$ sudo pkg_comp fetch
$ sudo pkg_comp sandbox-create
$ sudo pkg_comp bootstrap
$ sudo pkg_comp build &lt;package names here&gt;
$ sudo pkg_comp sandbox-destroy



Go ahead and play with these.  You can also use the sandbox-shell command to interactively enter the sandbox.  See pkg_comp(8) for more details.

Lastly note that the root user will receive email messages if the periodic pkg_comp cron job fails, but only if it fails.  That said, you can find the full logs for all builds, successful or not, under /var/pkg_comp/log/.

Installing the resulting packages

Now that you have built your first set of packages, you will want to install them.  This is easy on macOS because you did not use pkgsrc itself to install pkg_comp.

First, unpack the pkgsrc installation.  You only have to do this once:

$ cd /
$ sudo tar xzvpf /var/pkg_comp/packages/bootstrap.tgz



That’s it.  You can now install any packages you like:

$ PKG_PATH=file:///var/pkg_comp/packages/All sudo pkg_add pkgin &lt;other package names&gt;



The command above assume you have restarted your shell to pick up the correct path to the pkgsrc installation.  If the call to pkg_add fails because of a missing binary, try restarting your shell or explicitly running the binary as /opt/pkg/sbin/pkg_add.

Keeping your system up-to-date

Thanks to the cron job that builds your packages, your local repository under /var/pkg_comp/packages/ will always be up-to-date; you can use that to quickly upgrade your system with minimal downtime.

Assuming you are going to use pkgtools/pkgin as recommended above (and why not?), configure your local repository:

$ sudo /bin/sh -c "echo file:///var/pkg_comp/packages/All &gt;&gt;/opt/pkg/etc/pkgin/repositories.conf"



And, from now on, all it takes to upgrade your system is:

$ sudo pkgin update
$ sudo pkgin upgrade



Enjoy!

            &lt;a href=&quot;http://julio.meroh.net/2017/02/pkg_comp-2.0-tutorial-macos.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;This is a tutorial to guide you through the &lt;a href=&quot;/2017/02/introducing-pkg_comp-2.0.html&quot;&gt;shiny new pkg_comp 2.0&lt;/a&gt; on macOS &lt;a href=&quot;https://github.com/jmmv/pkg_comp/blob/master/INSTALL.md#using-the-macos-installer&quot;&gt;using the macOS-specific self-installer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt; to use pkg_comp 2.0 to build a binary repository of all the packages you are interested in; to keep the repository fresh on a daily basis; and to use that repository with pkgin to maintain your macOS system up-to-date and secure.&lt;/p&gt;

&lt;!--end-of-excerpt--&gt;

&lt;p&gt;This tutorial is specifically targeted at macOS and relies on the macOS-specific self-installer package.  For a more generic tutorial that uses the &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-cron&lt;/code&gt; package in pkgsrc, see &lt;a href=&quot;/2017/02/pkg_comp-2.0-tutorial-netbsd.html&quot;&gt;Keeping NetBSD up-to-date with pkg_comp 2.0&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;getting-started&quot;&gt;Getting started&lt;/h1&gt;

&lt;p&gt;First download and install the standalone macOS installer package.  To find the right file, navigate to the &lt;a href=&quot;https://github.com/jmmv/pkg_comp/releases&quot;&gt;releases page on GitHub&lt;/a&gt;, pick the most recent release, and download the file with a name of the form &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-&amp;lt;version&amp;gt;-macos.pkg&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then double-click on the file you downloaded and follow the installation instructions.  You will be asked for your administrator password because the installer has to place files under &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/&lt;/code&gt;; note that pkg_comp requires root privileges anyway to run (because it uses &lt;code class=&quot;highlighter-rouge&quot;&gt;chroot(8)&lt;/code&gt; internally), so you will have to grant permission at some point or another.&lt;/p&gt;

&lt;p&gt;The installer modifies the default &lt;code class=&quot;highlighter-rouge&quot;&gt;PATH&lt;/code&gt; (by creating &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/paths.d/pkg_comp&lt;/code&gt;) to include pkg_comp’s own installation directory and pkgsrc’s installation prefix.  Restart your shell sessions to make this change effective, or update your own shell startup scripts accordingly if you don’t use the standard ones.&lt;/p&gt;

&lt;p&gt;Lastly, make sure to have Xcode installed in the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;/Applications/Xcode.app&lt;/code&gt; location and that all components required to build command-line apps are available.  Tip: try running &lt;code class=&quot;highlighter-rouge&quot;&gt;cc&lt;/code&gt; from the command line and seeing if it prints its usage message.&lt;/p&gt;

&lt;h1 id=&quot;adjusting-the-configuration&quot;&gt;Adjusting the configuration&lt;/h1&gt;

&lt;p&gt;The macOS flavor of pkg_comp is configured with an installation prefix of &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/&lt;/code&gt;, which means that the executable is located in &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/sbin/pkg_comp&lt;/code&gt; and the configuration files are in &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/etc/pkg_comp/&lt;/code&gt;.  This is intentional to keep the pkg_comp installation separate from your pkgsrc installation so that it can run no matter what state your pkgsrc installation is in.&lt;/p&gt;

&lt;p&gt;The configuration files are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/etc/pkg_comp/default.conf&lt;/code&gt;: This is pkg_comp’s own configuration file and the defaults configured by the installer should be good to go for macOS.&lt;/p&gt;

    &lt;p&gt;In particular, packages are configured to go into &lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/pkg/&lt;/code&gt; instead of the traditional &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/pkg/&lt;/code&gt;.  This is a necessity because the latter is not writable starting with OS X El Capitan thanks to System Integrity Protection (SIP).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/etc/pkg_comp/sandbox.conf&lt;/code&gt;: This is the configuration file for sandboxctl, which is the support tool that pkg_comp uses to manage the compilation sandbox.  The default settings configured by the installer should be good.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/etc/pkg_comp/extra.mk.conf&lt;/code&gt;: This is pkgsrc’s own configuration file.  In here, you should configure things like the licenses that are acceptable to you and the package-specific options you’d like to set.  You should &lt;em&gt;not&lt;/em&gt; configure the layout of the installed files (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;LOCALBASE&lt;/code&gt;) because that’s handled internally by pkg_comp as specified in &lt;code class=&quot;highlighter-rouge&quot;&gt;default.conf&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/etc/pkg_comp/list.txt&lt;/code&gt;: This determines the set of packages you want to build automatically (either via the &lt;code class=&quot;highlighter-rouge&quot;&gt;auto&lt;/code&gt; command or your periodic cron job).  The automated builds will fail unless you list at least one package.&lt;/p&gt;

    &lt;p&gt;Make sure to list &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgin&lt;/code&gt; here to install a better binary package management tool.  You’ll find this very handy to keep your installation up-to-date.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that these configuration files use the &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/&lt;/code&gt; directory as the dumping ground for: the pkgsrc tree, the downloaded distribution files, and the built binary packages.  We will see references to this location later on.&lt;/p&gt;

&lt;h2 id=&quot;the-cron-job&quot;&gt;The cron job&lt;/h2&gt;

&lt;p&gt;The installer configures a cron job that runs as root to invoke pkg_comp daily.  The goal of this cron job is to keep your local packages repository up-to-date so that you can do binary upgrades at any time.  You can edit the cron job configuration interactively by running &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo crontab -e&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This cron job won’t have an effect until you have populated the &lt;code class=&quot;highlighter-rouge&quot;&gt;list.txt&lt;/code&gt; file as described above, so it’s safe to let it enabled until you have configured pkg_comp.&lt;/p&gt;

&lt;p&gt;If you want to disable the periodic builds, just remove the pkg_comp entry from the crontab.&lt;/p&gt;

&lt;p&gt;On slow machines, or if you are building a lot of packages, you may want to consider decreasing the build frequency from &lt;code class=&quot;highlighter-rouge&quot;&gt;@daily&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;@weekly&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;sample-configuration&quot;&gt;Sample configuration&lt;/h2&gt;

&lt;p&gt;Here is what the configuration looks like on my Mac Mini as dumped by the &lt;code class=&quot;highlighter-rouge&quot;&gt;config&lt;/code&gt; subcommand.  Use this output to get an idea of what to expect.  I’ll be using the values shown here in the rest of the tutorial:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ pkg_comp config
AUTO_PACKAGES = autoconf automake bash colordiff dash emacs24-nox11 emacs25-nox11 fuse-bindfs fuse-sshfs fuse-unionfs gdb git-base git-docs glib2 gmake gnuls libtool-base lua52 mercurial mozilla-rootcerts mysql56-server pdksh pkg_developer pkgconf pkgin ruby-jekyll ruby-jekyll-archives ruby-jekyll-paginate scmcvs smartmontools sqlite3 tmux vim
CVS_ROOT = :ext:anoncvs@anoncvs.NetBSD.org:/cvsroot
CVS_TAG is undefined
DISTDIR = /var/pkg_comp/distfiles
EXTRA_MKCONF = /usr/local/etc/pkg_comp/extra.mk.conf
FETCH_VCS = git
GIT_BRANCH = trunk
GIT_URL = https://github.com/jsonn/pkgsrc.git
LOCALBASE = /opt/pkg
NJOBS = 4
PACKAGES = /var/pkg_comp/packages
PBULK_PACKAGES = /var/pkg_comp/pbulk-packages
PKG_DBDIR = /opt/pkg/libdata/pkgdb
PKGSRCDIR = /var/pkg_comp/pkgsrc
SANDBOX_CONFFILE = /usr/local/etc/pkg_comp/sandbox.conf
SYSCONFDIR = /opt/pkg/etc
UPDATE_SOURCES = true
VARBASE = /opt/pkg/var

DARWIN_NATIVE_WITH_XCODE = true
SANDBOX_ROOT = /var/pkg_comp/sandbox
SANDBOX_TYPE = darwin-native
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;building-your-own-packages-by-hand&quot;&gt;Building your own packages by hand&lt;/h1&gt;

&lt;p&gt;Now that you are fully installed and configured, you’ll build some stuff by hand to ensure the setup works before the cron job comes in.&lt;/p&gt;

&lt;p&gt;The simplest usage form, which involves full automation and assumes you have listed at least one package in &lt;code class=&quot;highlighter-rouge&quot;&gt;list.txt&lt;/code&gt;, is something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo pkg_comp auto
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This trivially-looking command will:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;clone or update your copy of pkgsrc;&lt;/li&gt;
  &lt;li&gt;create the sandbox;&lt;/li&gt;
  &lt;li&gt;bootstrap pkgsrc and pbulk;&lt;/li&gt;
  &lt;li&gt;use pbulk to build the given packages; and&lt;/li&gt;
  &lt;li&gt;destroy the sandbox.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After a successful invocation, you’ll be left with a collection of packages in the &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/packages/&lt;/code&gt; directory.&lt;/p&gt;

&lt;p&gt;If you’d like to restrict the set of packages to build during a manually-triggered build, provide those as arguments to &lt;code class=&quot;highlighter-rouge&quot;&gt;auto&lt;/code&gt;.  This will override the contents of &lt;code class=&quot;highlighter-rouge&quot;&gt;AUTO_PACKAGES&lt;/code&gt; (which was derived from your &lt;code class=&quot;highlighter-rouge&quot;&gt;list.txt&lt;/code&gt; file).&lt;/p&gt;

&lt;p&gt;But what if you wanted to invoke all stages separately, bypassing &lt;code class=&quot;highlighter-rouge&quot;&gt;auto&lt;/code&gt;?  The command above would be equivalent to:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo pkg_comp fetch
$ sudo pkg_comp sandbox-create
$ sudo pkg_comp bootstrap
$ sudo pkg_comp build &amp;lt;package names here&amp;gt;
$ sudo pkg_comp sandbox-destroy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Go ahead and play with these.  You can also use the &lt;code class=&quot;highlighter-rouge&quot;&gt;sandbox-shell&lt;/code&gt; command to interactively enter the sandbox.  See &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp(8)&lt;/code&gt; for more details.&lt;/p&gt;

&lt;p&gt;Lastly note that the root user will receive email messages if the periodic pkg_comp cron job fails, but only if it fails.  That said, you can find the full logs for all builds, successful or not, under &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/log/&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;installing-the-resulting-packages&quot;&gt;Installing the resulting packages&lt;/h1&gt;

&lt;p&gt;Now that you have built your first set of packages, you will want to install them.  This is easy on macOS because you did &lt;em&gt;not&lt;/em&gt; use pkgsrc itself to install pkg_comp.&lt;/p&gt;

&lt;p&gt;First, unpack the pkgsrc installation.  &lt;strong&gt;You only have to do this once:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd /
$ sudo tar xzvpf /var/pkg_comp/packages/bootstrap.tgz
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;That’s it.  You can now install any packages you like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ PKG_PATH=file:///var/pkg_comp/packages/All sudo pkg_add pkgin &amp;lt;other package names&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The command above assume you have restarted your shell to pick up the correct path to the pkgsrc installation.  If the call to &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_add&lt;/code&gt; fails because of a missing binary, try restarting your shell or explicitly running the binary as &lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/pkg/sbin/pkg_add&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;keeping-your-system-up-to-date&quot;&gt;Keeping your system up-to-date&lt;/h1&gt;

&lt;p&gt;Thanks to the cron job that builds your packages, your local repository under &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/packages/&lt;/code&gt; will always be up-to-date; you can use that to quickly upgrade your system with minimal downtime.&lt;/p&gt;

&lt;p&gt;Assuming you are going to use &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgtools/pkgin&lt;/code&gt; as recommended above (and why not?), configure your local repository:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo /bin/sh -c &quot;echo file:///var/pkg_comp/packages/All &amp;gt;&amp;gt;/opt/pkg/etc/pkgin/repositories.conf&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And, from now on, all it takes to upgrade your system is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo pkgin update
$ sudo pkgin upgrade
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;
</description>
        
        <pubDate>Thu, 23 Feb 2017 22:36:54 +0000</pubDate>
        <link>http://julio.meroh.net/2017/02/pkg_comp-2.0-tutorial-macos.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2017/02/pkg_comp-2.0-tutorial-macos.html</guid>
        
        
        <category>macos</category>
        
        <category>software</category>
        
        <category>tutorial</category>
        
      </item>
    
      <item>
        <title>Keeping NetBSD up-to-date with pkg_comp 2.0</title>
        
          <description>
            This is a tutorial to guide you through the shiny new pkg_comp 2.0 on NetBSD.

Goals: to use pkg_comp 2.0 to build a binary repository of all the packages you are interested in; to keep the repository fresh on a daily basis; and to use that repository with pkgin to maintain your NetBSD system up-to-date and secure.



This tutorial is specifically targeted at NetBSD but should work on other platforms with some small changes.  Expect, at the very least, a macOS-specific tutorial as soon as I create a pkg_comp standalone installer for that platform.

Getting started

First install the sysutils/sysbuild-user package and trigger a full build of NetBSD so that you get usable release sets for pkg_comp.  See sysbuild(1) and pkg_info sysbuild-user for details on how to do so.  Alternatively, download release sets from the FTP site and later tell pkg_comp where they are.

Then install the pkgtools/pkg_comp-cron package.  The rest of this tutorial assumes you have done so.

Adjusting the configuration

To use pkg_comp for periodic builds, you’ll need to do some minimal edits to the default configuration files.  The files can be found directly under /var/pkg_comp/, which is pkg_comp-cron’s “home”:


  
    /var/pkg_comp/pkg_comp.conf: This is pkg_comp’s own configuration file and the defaults installed by pkg_comp-cron should be good to go.

    The contents here are divided in three major sections: declaration on how to download pkgsrc, definition of the file system layout on the host machine, and definition of the file system layout for the built packages.

    You may want to customize the target system paths, such as LOCALBASE or SYSCONFDIR, but you should not have to customize the host system paths.
  
  
    /var/pkg_comp/sandbox.conf: This is the configuration file for sandboxctl.  The default settings installed by pkg_comp-cron should suffice if you used the sysutils/sysbuild-user package as recommended; otherwise tweak the NETBSD_NATIVE_RELEASEDIR and NETBSD_SETS_RELEASEDIR variables to point to where the downloaded release sets are.
  
  
    /var/pkg_comp/extra.mk.conf: This is pkgsrc’s own configuration file.  In here, you should configure things like the licenses that are acceptable to you and the package-specific options you’d like to set.  You should not configure the layout of the installed files (e.g. LOCALBASE) because that’s handled internally by pkg_comp as specified in pkg_comp.conf.
  
  
    /var/pkg_comp/list.txt: This determines the set of packages you want to build in your periodic cron job.  The builds will fail unless you list at least one package.

    WARNING: Make sure to include pkg_comp-cron and pkgin in this list so that your binary kit includes these essential package management tools.  Otherwise you’ll have to deal with some minor annoyances after rebootstrapping your system.
  


Lastly, review root’s crontab to ensure the job specification for pkg_comp is sane.  On slow machines, or if you are building many packages, you will probably want to decrease the build frequency from @daily to @weekly.

Sample configuration

Here is what the configuration looks like on my NetBSD development machine as dumped by the config subcommand.  Use this output to get an idea of what to expect.  I’ll be using the values shown here in the rest of the tutorial:

# pkg_comp -c /var/pkg_comp/pkg_comp.conf config
AUTO_PACKAGES = autoconf automake bash colordiff dash emacs-nox11 git-base git-docs gmake gnuls lua52 mozilla-rootcerts pdksh pkg_comp-cron pkg_developer pkgin sqlite3 sudo sysbuild sysbuild-user sysupgrade tmux vim zsh
CVS_ROOT = :ext:anoncvs@anoncvs.NetBSD.org:/cvsroot
CVS_TAG is undefined
DISTDIR = /var/pkg_comp/distfiles
EXTRA_MKCONF = /var/pkg_comp/extra.mk.conf
FETCH_VCS = cvs
GIT_BRANCH = trunk
GIT_URL = https://github.com/jsonn/pkgsrc.git
LOCALBASE = /usr/pkg
NJOBS = 2
PACKAGES = /var/pkg_comp/packages
PBULK_PACKAGES = /var/pkg_comp/pbulk-packages
PKG_DBDIR = /usr/pkg/libdata/pkgdb
PKGSRCDIR = /var/pkg_comp/pkgsrc
SANDBOX_CONFFILE = /var/pkg_comp/sandbox.conf
SYSCONFDIR = /etc
UPDATE_SOURCES = true
VARBASE = /var

NETBSD_NATIVE_RELEASEDIR = /home/sysbuild/release/amd64
NETBSD_RELEASE_RELEASEDIR = /home/sysbuild/release/amd64
NETBSD_RELEASE_SETS is undefined
SANDBOX_ROOT = /var/pkg_comp/sandbox
SANDBOX_TYPE = netbsd-release



Building your own packages by hand

Now that you are fully installed and configured, you’ll build some stuff by hand to ensure the setup works before the cron job comes in.

The simplest usage form, which involves full automation, is something like this:

# pkg_comp -c /var/pkg_comp/pkg_comp.conf auto



This trivially-looking command will:


  checkout or update your copy of pkgsrc;
  create the sandbox;
  bootstrap pkgsrc and pbulk;
  use pbulk to build the given packages; and
  destroy the sandbox.


After a successful invocation, you’ll be left with a collection of packages in the directory you set in PACKAGES, which in the default pkg_comp-cron installation is /var/pkg_comp/packages/.

If you’d like to restrict the set of packages to build during a manually-triggered build, provide those as arguments to auto.  This will override the contents of AUTO_PACKAGES (which was derived from your list.txt file).

But what if you wanted to invoke all stages separately, bypassing auto?  The command above would be equivalent to:

# pkg_comp -c /var/pkg_comp/pkg_comp.conf fetch
# pkg_comp -c /var/pkg_comp/pkg_comp.conf sandbox-create
# pkg_comp -c /var/pkg_comp/pkg_comp.conf bootstrap
# pkg_comp -c /var/pkg_comp/pkg_comp.conf build &lt;package names here&gt;
# pkg_comp -c /var/pkg_comp/pkg_comp.conf sandbox-destroy



Go ahead and play with these.  You can also use the sandbox-shell command to interactively enter the sandbox.  See pkg_comp(8) for more details.

Lastly note that the root user will receive email messages if the periodic pkg_comp cron job fails, but only if it fails.  That said, you can find the full logs for all builds, successful or not, under /var/pkg_comp/log/.

Installing the resulting packages

Now that you have built your first set of packages, you will want to install them.  On NetBSD, the default pkg_comp-cron configuration produces a set of packages for /usr/pkg so you have to wipe your existing packages first to avoid build mismatches.

WARNING: Yes, you really have to wipe your packages.  pkg_comp currently does not recognize the package tools that ship with the NetBSD base system (i.e. it bootstraps pkgsrc unconditionally, including bmake), which means that the newly-built packages won’t be compatible with the ones you already have.  Avoid any trouble by starting afresh.

To clean your system, do something like this:

# ... ensure your login shell lives in /bin! ...
# pkg_delete -r -R "*"
# mv /usr/pkg/etc /root/etc.old  # Backup any modified files.
# rm -rf /usr/pkg /var/db/pkg*



Now, rebootstrap pkgsrc and reinstall any packages you previously had:

# cd /
# tar xzvpf /var/pkg_comp/packages/bootstrap.tgz
# echo "pkg_admin=/usr/pkg/sbin/pkg_admin" &gt;&gt;/etc/pkgpath.conf
# echo "pkg_info=/usr/pkg/sbin/pkg_info" &gt;&gt;/etc/pkgpath.conf
# export PATH=/usr/pkg/bin:/usr/pkg/sbin:${PATH}
# export PKG_PATH=file:///var/pkg_comp/packages/All
# pkg_add pkgin pkg_comp-cron &lt;other package names&gt;



Finally, reconfigure any packages where you had have previously made custom edits.  Use the backup in /root/etc.old to properly update the corresponding files in /etc.  I doubt you made a ton of edits so this should be easy.

IMPORTANT: Note that the last command in this example includes pkgin and pkg_comp-cron.  You should install these first to ensure you can continue with the next steps in this tutorial.

Keeping your system up-to-date

If you paid attention when you installed the pkg_comp-cron package, you should have noticed that this configured a cron job to run pkg_comp daily.  This means that your packages repository under /var/pkg_comp/packages/ will always be up-to-date so you can use that to quickly upgrade your system with minimal downtime.

Assuming you are going to use pkgtools/pkgin (and why not?), configure your local repository:

# echo 'file:///var/pkg_comp/packages/All' &gt;&gt;/etc/pkgin/repositories.conf



And, from now on, all it takes to upgrade your system is:

# pkgin update
# pkgin upgrade



Enjoy!

            &lt;a href=&quot;http://julio.meroh.net/2017/02/pkg_comp-2.0-tutorial-netbsd.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;This is a tutorial to guide you through the &lt;a href=&quot;/2017/02/introducing-pkg_comp-2.0.html&quot;&gt;shiny new pkg_comp 2.0&lt;/a&gt; on NetBSD.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goals:&lt;/strong&gt; to use pkg_comp 2.0 to build a binary repository of all the packages you are interested in; to keep the repository fresh on a daily basis; and to use that repository with pkgin to maintain your NetBSD system up-to-date and secure.&lt;/p&gt;

&lt;!--end-of-excerpt--&gt;

&lt;p&gt;This tutorial is specifically targeted at NetBSD but should work on other platforms with some small changes.  Expect, at the very least, a macOS-specific tutorial as soon as I create a pkg_comp standalone installer for that platform.&lt;/p&gt;

&lt;h1 id=&quot;getting-started&quot;&gt;Getting started&lt;/h1&gt;

&lt;p&gt;First install the &lt;code class=&quot;highlighter-rouge&quot;&gt;sysutils/sysbuild-user&lt;/code&gt; package and trigger a full build of NetBSD so that you get usable release sets for pkg_comp.  See &lt;code class=&quot;highlighter-rouge&quot;&gt;sysbuild(1)&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_info sysbuild-user&lt;/code&gt; for details on how to do so.  Alternatively, &lt;a href=&quot;https://ftp.netbsd.org/pub/NetBSD/&quot;&gt;download release sets from the FTP site&lt;/a&gt; and later tell pkg_comp where they are.&lt;/p&gt;

&lt;p&gt;Then install the &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgtools/pkg_comp-cron&lt;/code&gt; package.  The rest of this tutorial assumes you have done so.&lt;/p&gt;

&lt;h1 id=&quot;adjusting-the-configuration&quot;&gt;Adjusting the configuration&lt;/h1&gt;

&lt;p&gt;To use pkg_comp for periodic builds, you’ll need to do some minimal edits to the default configuration files.  The files can be found directly under &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/&lt;/code&gt;, which is &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-cron&lt;/code&gt;’s “home”:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/pkg_comp.conf&lt;/code&gt;: This is pkg_comp’s own configuration file and the defaults installed by &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-cron&lt;/code&gt; should be good to go.&lt;/p&gt;

    &lt;p&gt;The contents here are divided in three major sections: declaration on how to download pkgsrc, definition of the file system layout on the host machine, and definition of the file system layout for the built packages.&lt;/p&gt;

    &lt;p&gt;You may want to customize the target system paths, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;LOCALBASE&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;SYSCONFDIR&lt;/code&gt;, but you should not have to customize the host system paths.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/sandbox.conf&lt;/code&gt;: This is the configuration file for sandboxctl.  The default settings installed by &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-cron&lt;/code&gt; &lt;em&gt;should&lt;/em&gt; suffice if you used the &lt;code class=&quot;highlighter-rouge&quot;&gt;sysutils/sysbuild-user&lt;/code&gt; package as recommended; otherwise tweak the &lt;code class=&quot;highlighter-rouge&quot;&gt;NETBSD_NATIVE_RELEASEDIR&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;NETBSD_SETS_RELEASEDIR&lt;/code&gt; variables to point to where the downloaded release sets are.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/extra.mk.conf&lt;/code&gt;: This is pkgsrc’s own configuration file.  In here, you should configure things like the licenses that are acceptable to you and the package-specific options you’d like to set.  You should &lt;em&gt;not&lt;/em&gt; configure the layout of the installed files (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;LOCALBASE&lt;/code&gt;) because that’s handled internally by pkg_comp as specified in &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp.conf&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/list.txt&lt;/code&gt;: This determines the set of packages you want to build in your periodic cron job.  The builds will fail unless you list at least one package.&lt;/p&gt;

    &lt;p&gt;WARNING: Make sure to include &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-cron&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgin&lt;/code&gt; in this list so that your binary kit includes these essential package management tools.  Otherwise you’ll have to deal with some minor annoyances after rebootstrapping your system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lastly, review root’s &lt;code class=&quot;highlighter-rouge&quot;&gt;crontab&lt;/code&gt; to ensure the job specification for pkg_comp is sane.  On slow machines, or if you are building many packages, you will probably want to decrease the build frequency from &lt;code class=&quot;highlighter-rouge&quot;&gt;@daily&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;@weekly&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;sample-configuration&quot;&gt;Sample configuration&lt;/h1&gt;

&lt;p&gt;Here is what the configuration looks like on my NetBSD development machine as dumped by the &lt;code class=&quot;highlighter-rouge&quot;&gt;config&lt;/code&gt; subcommand.  Use this output to get an idea of what to expect.  I’ll be using the values shown here in the rest of the tutorial:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# pkg_comp -c /var/pkg_comp/pkg_comp.conf config
AUTO_PACKAGES = autoconf automake bash colordiff dash emacs-nox11 git-base git-docs gmake gnuls lua52 mozilla-rootcerts pdksh pkg_comp-cron pkg_developer pkgin sqlite3 sudo sysbuild sysbuild-user sysupgrade tmux vim zsh
CVS_ROOT = :ext:anoncvs@anoncvs.NetBSD.org:/cvsroot
CVS_TAG is undefined
DISTDIR = /var/pkg_comp/distfiles
EXTRA_MKCONF = /var/pkg_comp/extra.mk.conf
FETCH_VCS = cvs
GIT_BRANCH = trunk
GIT_URL = https://github.com/jsonn/pkgsrc.git
LOCALBASE = /usr/pkg
NJOBS = 2
PACKAGES = /var/pkg_comp/packages
PBULK_PACKAGES = /var/pkg_comp/pbulk-packages
PKG_DBDIR = /usr/pkg/libdata/pkgdb
PKGSRCDIR = /var/pkg_comp/pkgsrc
SANDBOX_CONFFILE = /var/pkg_comp/sandbox.conf
SYSCONFDIR = /etc
UPDATE_SOURCES = true
VARBASE = /var

NETBSD_NATIVE_RELEASEDIR = /home/sysbuild/release/amd64
NETBSD_RELEASE_RELEASEDIR = /home/sysbuild/release/amd64
NETBSD_RELEASE_SETS is undefined
SANDBOX_ROOT = /var/pkg_comp/sandbox
SANDBOX_TYPE = netbsd-release
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;building-your-own-packages-by-hand&quot;&gt;Building your own packages by hand&lt;/h1&gt;

&lt;p&gt;Now that you are fully installed and configured, you’ll build some stuff by hand to ensure the setup works before the cron job comes in.&lt;/p&gt;

&lt;p&gt;The simplest usage form, which involves full automation, is something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# pkg_comp -c /var/pkg_comp/pkg_comp.conf auto
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This trivially-looking command will:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;checkout or update your copy of pkgsrc;&lt;/li&gt;
  &lt;li&gt;create the sandbox;&lt;/li&gt;
  &lt;li&gt;bootstrap pkgsrc and pbulk;&lt;/li&gt;
  &lt;li&gt;use pbulk to build the given packages; and&lt;/li&gt;
  &lt;li&gt;destroy the sandbox.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After a successful invocation, you’ll be left with a collection of packages in the directory you set in &lt;code class=&quot;highlighter-rouge&quot;&gt;PACKAGES&lt;/code&gt;, which in the default &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-cron&lt;/code&gt; installation is &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/packages/&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you’d like to restrict the set of packages to build during a manually-triggered build, provide those as arguments to &lt;code class=&quot;highlighter-rouge&quot;&gt;auto&lt;/code&gt;.  This will override the contents of &lt;code class=&quot;highlighter-rouge&quot;&gt;AUTO_PACKAGES&lt;/code&gt; (which was derived from your &lt;code class=&quot;highlighter-rouge&quot;&gt;list.txt&lt;/code&gt; file).&lt;/p&gt;

&lt;p&gt;But what if you wanted to invoke all stages separately, bypassing &lt;code class=&quot;highlighter-rouge&quot;&gt;auto&lt;/code&gt;?  The command above would be equivalent to:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# pkg_comp -c /var/pkg_comp/pkg_comp.conf fetch
# pkg_comp -c /var/pkg_comp/pkg_comp.conf sandbox-create
# pkg_comp -c /var/pkg_comp/pkg_comp.conf bootstrap
# pkg_comp -c /var/pkg_comp/pkg_comp.conf build &amp;lt;package names here&amp;gt;
# pkg_comp -c /var/pkg_comp/pkg_comp.conf sandbox-destroy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Go ahead and play with these.  You can also use the &lt;code class=&quot;highlighter-rouge&quot;&gt;sandbox-shell&lt;/code&gt; command to interactively enter the sandbox.  See &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp(8)&lt;/code&gt; for more details.&lt;/p&gt;

&lt;p&gt;Lastly note that the root user will receive email messages if the periodic pkg_comp cron job fails, but only if it fails.  That said, you can find the full logs for all builds, successful or not, under &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/log/&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;installing-the-resulting-packages&quot;&gt;Installing the resulting packages&lt;/h1&gt;

&lt;p&gt;Now that you have built your first set of packages, you will want to install them.  On NetBSD, the default &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-cron&lt;/code&gt; configuration produces a set of packages for &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/pkg&lt;/code&gt; so you have to wipe your existing packages first to avoid build mismatches.&lt;/p&gt;

&lt;p&gt;WARNING: Yes, you really have to wipe your packages.  pkg_comp currently does not recognize the package tools that ship with the NetBSD base system (i.e. it bootstraps pkgsrc unconditionally, including &lt;code class=&quot;highlighter-rouge&quot;&gt;bmake&lt;/code&gt;), which means that the newly-built packages won’t be compatible with the ones you already have.  Avoid any trouble by starting afresh.&lt;/p&gt;

&lt;p&gt;To clean your system, do something like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# ... ensure your login shell lives in /bin! ...
# pkg_delete -r -R &quot;*&quot;
# mv /usr/pkg/etc /root/etc.old  # Backup any modified files.
# rm -rf /usr/pkg /var/db/pkg*
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now, rebootstrap pkgsrc and reinstall any packages you previously had:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# cd /
# tar xzvpf /var/pkg_comp/packages/bootstrap.tgz
# echo &quot;pkg_admin=/usr/pkg/sbin/pkg_admin&quot; &amp;gt;&amp;gt;/etc/pkgpath.conf
# echo &quot;pkg_info=/usr/pkg/sbin/pkg_info&quot; &amp;gt;&amp;gt;/etc/pkgpath.conf
# export PATH=/usr/pkg/bin:/usr/pkg/sbin:${PATH}
# export PKG_PATH=file:///var/pkg_comp/packages/All
# pkg_add pkgin pkg_comp-cron &amp;lt;other package names&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Finally, reconfigure any packages where you had have previously made custom edits.  Use the backup in &lt;code class=&quot;highlighter-rouge&quot;&gt;/root/etc.old&lt;/code&gt; to properly update the corresponding files in &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc&lt;/code&gt;.  I doubt you made a ton of edits so this should be easy.&lt;/p&gt;

&lt;p&gt;IMPORTANT: Note that the last command in this example includes &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgin&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-cron&lt;/code&gt;.  You should install these first to ensure you can continue with the next steps in this tutorial.&lt;/p&gt;

&lt;h1 id=&quot;keeping-your-system-up-to-date&quot;&gt;Keeping your system up-to-date&lt;/h1&gt;

&lt;p&gt;If you paid attention when you installed the &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp-cron&lt;/code&gt; package, you should have noticed that this configured a cron job to run pkg_comp daily.  This means that your packages repository under &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/pkg_comp/packages/&lt;/code&gt; will always be up-to-date so you can use that to quickly upgrade your system with minimal downtime.&lt;/p&gt;

&lt;p&gt;Assuming you are going to use &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgtools/pkgin&lt;/code&gt; (and why not?), configure your local repository:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# echo 'file:///var/pkg_comp/packages/All' &amp;gt;&amp;gt;/etc/pkgin/repositories.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And, from now on, all it takes to upgrade your system is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# pkgin update
# pkgin upgrade
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;
</description>
        
        <pubDate>Sat, 18 Feb 2017 16:51:55 +0000</pubDate>
        <link>http://julio.meroh.net/2017/02/pkg_comp-2.0-tutorial-netbsd.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2017/02/pkg_comp-2.0-tutorial-netbsd.html</guid>
        
        
        <category>netbsd</category>
        
        <category>software</category>
        
        <category>tutorial</category>
        
      </item>
    
      <item>
        <title>Introducing pkg_comp 2.0 (and sandboxctl 1.0)</title>
        
          <description>
            After many (many) years in the making, pkg_comp 2.0 and its companion sandboxctl 1.0 are finally here!

Read below for more details on this launch.  I will publish detailed step-by-step tutorials on setting up periodic package rebuilds in separate posts.

What are these tools?

pkg_comp is an automation tool to build pkgsrc binary packages inside a chroot-based sandbox.  The main goal is to fully automate the process and to produce clean and reproducible packages.  A secondary goal is to support building binary packages for a different system than the one doing the builds: e.g. building packages for NetBSD/i386 6.0 from a NetBSD/amd64 7.0 host.

The highlights of pkg_comp 2.0, compared to the 1.x series, are: multi-platform support, including NetBSD, FreeBSD, Linux, and macOS; use of pbulk for efficient builds; management of the pkgsrc tree itself via CVS or Git; and a more robust and modern codebase.

sandboxctl is an automation tool to create and manage chroot-based sandboxes on a variety of operating systems.  sandboxctl is the backing tool behind pk_comp.  sandboxctl hides the details of creating a functional chroot sandbox on all supported operating systems; in some cases, like building a NetBSD sandbox using release sets, things are easy; but in others, like on macOS, they are horrifyingly difficult and brittle.

Storytelling time

pkg_comp’s history is a long one.  pkg_comp 1.0 first appeared in pkgsrc on September 6th, 2002 as the pkgtools/pkg_comp package in pkgsrc.  As of this writing, the 1.x series are at version 1.38 and have received contributions from a bunch of pkgsrc developers and external users; even more, the tool was featured in the BSD Hacks book back in 2004.

This is a long time for a shell script to survive in its rudimentary original form: pkg_comp 1.x is now a teenager at its 14 years of age and is possibly one of my longest-living pieces of software still in use.

Motivation for the 2.x rewrite

For many of these years, I have been wanting to rewrite pkg_comp to support other operating systems.  This all started when I first got a Mac in 2005, at which time pkgsrc already supported Darwin but there was no easy mechanism to manage package updates.  What would happen—and still happens to this day!—is that, once in a while, I’d realize that my packages were out of date (read: insecure) so I’d wipe the whole pkgsrc installation and start from scratch.  Very inconvenient; I had to automate that properly.

Thus the main motivation behind the rewrite was primarily to support macOS because this was, and still is, my primary development platform.  The secondary motivation came after writing sysbuild in 2012, which trivially configured daily builds of the NetBSD base system from cron; I wanted the exact same thing for my packages.

One, two… no, three rewrites

The first rewrite attempt was sometime in 2006, soon after I learned Haskell in school.  Why Haskell?  Just because that was the new hotness in my mind and it seemed like a robust language to drive a pretty tricky automation process.  That rewrite did not go very far, and that’s possibly for the better: relying on Haskell would have decreased the portability of the tool, made it hard to install it, and guaranteed to alienate contributors.

The second rewrite attempt started sometime in 2010, about a year after I joined Google as an SRE.  This was after I became quite familiar with Python at work, wanting to use the language to rewrite this tool.  That experiment didn’t go very far though, but I can’t remember why… probably because I was busy enough at work and creating Kyua.

The third and final rewrite attempt started in 2013 while I had a summer intern and I had a little existential crisis.  The year before I had written sysbuild and shtk, so I figured recreating pkg_comp using the foundations laid out by these tools would be easy.  And it was… to some extent.

Getting the barebones of a functional tool took only a few weeks, but that code was far from being stable, portable, and publishable.  Life and work happened, so this fell through the cracks… until late last year, when I decided it was time to close this chapter so I could move on to some other project ideas.  To create the focus and free time required to complete this project, I had to shift my schedule to start the day at 5am instead of 7am—and, many weeks later, the code is finally here and I’m still keeping up with this schedule.

Granted: this third rewrite is not a fancy one, but it wasn’t meant to be.  pkg_comp 2.0 is still written in shell, just as 1.x was, but this is a good thing because bootstrapping on all supported platforms is easy.  I have to confess that I also considered Go recently after playing with it last year but I quickly let go of that thought: at some point I had to ship the 2.0 release, and 10 years since the inception of this rewrite was about time.

The launch of 2.0

On February 12th, 2017, the authoritative sources of pkg_comp 1.x were moved from pkgtools/pkg_comp to pkgtools/pkg_comp1 to make room for the import of 2.0.  Yes, the 1.x series only existed in pkgsrc and the 2.x series exist as a standalone project on GitHub.

And here we are.  Today, February 17th, 2017, pkg_comp 2.0 saw the light!

Why sandboxctl as a separate tool?

sandboxctl is the supporting tool behind pkg_comp, taking care of all the logic involved in creating chroot-based sandboxes on a variety of operating systems.  Some are easy, like building a NetBSD sandbox using release sets, and others are horrifyingly difficult like macOS.

In pkg_comp 1.x, this logic used to be bundled right into the pkg_comp code, which made it pretty much impossible to generalize for portability.  With pkg_comp 2.x, I decided to split this out into a separate tool to keep responsibilities isolated.  Yes, the integration between the two tools is a bit tricky, but allows for better testability and understandability.  Lastly, having sandboxctl as a standalone tool, instead of just a separate code module, gives you the option of using it for your own sandboxing needs.

I know, I know; the world has moved onto containerization and virtual machines, leaving chroot-based sandboxes as a very rudimentary thing… but that’s all we’ve got in NetBSD, and pkg_comp targets primarily NetBSD.  Note, though, that because pkg_comp is separate from sandboxctl, there is nothing preventing adding different sandboxing backends to pkg_comp.

Installation

Installation is still a bit convoluted unless you are on one of the tier 1 NetBSD platforms or you already have pkgsrc up and running.  For macOS in particular, I plan on creating and shipping a installer image that includes all of pkg_comp dependencies—but I did not want to block the first launch on this.

For now though, you need to download and install the latest source releases of shtk, sandboxctl, and pkg_comp—in this order; pass the --with-atf=no flag to the configure scripts to cut down the required dependencies.  On macOS, you will also need OSXFUSE and the bindfs file system.

If you are already using pkgsrc, you can install the pkgtools/pkg_comp package to get the basic tool and its dependencies in place, or you can install the wrapper pkgtools/pkg_comp-cron package to create a pre-configured environment with a daily cron job to run your builds.  See the package’s MESSAGE (with pkg_info pkg_comp-cron) for more details.

Documentation

Both pkg_comp and sandboxctl are fully documented in manual pages.  See pkg_comp(8), sandboxctl(8), pkg_comp.conf(5) and sandbox.conf(5) for plenty of additional details.

As mentioned at the beginning of the post, I plan on publishing one or more tutorials explaining how to bootstrap your pkgsrc installation using pkg_comp on, at least, NetBSD and macOS.  Stay tuned.

And, if you need support or find anything wrong, please let me know by filing bugs in the corresponding GitHub projects: jmmv/pkg_comp and jmmv/sandboxctl.

            &lt;a href=&quot;http://julio.meroh.net/2017/02/introducing-pkg_comp-2.0.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;After many (many) years in the making, &lt;strong&gt;&lt;a href=&quot;https://github.com/jmmv/pkg_comp/&quot;&gt;pkg_comp&lt;/a&gt; 2.0 and its companion &lt;a href=&quot;https://github.com/jmmv/sandboxctl/&quot;&gt;sandboxctl&lt;/a&gt; 1.0 are finally here!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Read below for more details on this launch.  I will publish detailed step-by-step tutorials on setting up periodic package rebuilds in separate posts.&lt;/p&gt;

&lt;h1 id=&quot;what-are-these-tools&quot;&gt;What are these tools?&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;pkg_comp is an automation tool to build &lt;a href=&quot;http://pkgsrc.org/&quot;&gt;pkgsrc&lt;/a&gt; binary packages inside a chroot-based sandbox.&lt;/strong&gt;  The main goal is to fully automate the process and to produce clean and reproducible packages.  A secondary goal is to support building binary packages for a different system than the one doing the builds: e.g. building packages for NetBSD/i386 6.0 from a NetBSD/amd64 7.0 host.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;highlights of pkg_comp 2.0&lt;/strong&gt;, compared to the 1.x series, are: &lt;strong&gt;multi-platform support&lt;/strong&gt;, including NetBSD, FreeBSD, Linux, and macOS; &lt;strong&gt;use of &lt;a href=&quot;https://www.netbsd.org/docs/pkgsrc/bulk.html&quot;&gt;pbulk&lt;/a&gt;&lt;/strong&gt; for efficient builds; &lt;strong&gt;management of the pkgsrc tree&lt;/strong&gt; itself via CVS or Git; and a more &lt;strong&gt;robust and modern codebase&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;sandboxctl is an automation tool to create and manage chroot-based sandboxes on a variety of operating systems&lt;/strong&gt;.  sandboxctl is the backing tool behind pk_comp.  sandboxctl hides the details of creating a functional chroot sandbox on all supported operating systems; in some cases, like building a NetBSD sandbox using release sets, things are easy; but in others, like on macOS, they are horrifyingly difficult and brittle.&lt;/p&gt;

&lt;h1 id=&quot;storytelling-time&quot;&gt;Storytelling time&lt;/h1&gt;

&lt;p&gt;pkg_comp’s history is a long one.  pkg_comp 1.0 first appeared in pkgsrc on September 6th, 2002 as the &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgtools/pkg_comp&lt;/code&gt; package in pkgsrc.  As of this writing, the 1.x series are at version 1.38 and have received contributions from a bunch of pkgsrc developers and external users; even more, the tool was featured in the &lt;a href=&quot;http://shop.oreilly.com/product/9780596006792.do&quot;&gt;BSD Hacks book&lt;/a&gt; back in 2004.&lt;/p&gt;

&lt;p&gt;This is a &lt;em&gt;long&lt;/em&gt; time for a shell script to survive in its rudimentary original form: pkg_comp 1.x is now a teenager at its 14 years of age and is possibly one of my longest-living pieces of software still in use.&lt;/p&gt;

&lt;h2 id=&quot;motivation-for-the-2x-rewrite&quot;&gt;Motivation for the 2.x rewrite&lt;/h2&gt;

&lt;p&gt;For many of these years, I have been wanting to rewrite pkg_comp to support other operating systems.  This all started when I first got a Mac in 2005, at which time pkgsrc already supported Darwin but there was no easy mechanism to manage package updates.  What would happen—and still happens to this day!—is that, once in a while, I’d realize that my packages were out of date (read: insecure) so I’d wipe the whole pkgsrc installation and start from scratch.  Very inconvenient; I had to automate that properly.&lt;/p&gt;

&lt;p&gt;Thus the main motivation behind the rewrite was primarily to support macOS because this was, and still is, my primary development platform.  The secondary motivation came after writing sysbuild in 2012, which trivially configured daily builds of the NetBSD base system from cron; I wanted the exact same thing for my packages.&lt;/p&gt;

&lt;h2 id=&quot;one-two-no-three-rewrites&quot;&gt;One, two… no, three rewrites&lt;/h2&gt;

&lt;p&gt;The first rewrite attempt was sometime in 2006, soon after I learned Haskell in school.  Why Haskell?  Just because that was the new hotness in my mind and it seemed like a robust language to drive a pretty tricky automation process.  That rewrite did not go very far, and that’s possibly for the better: relying on Haskell would have decreased the portability of the tool, made it hard to install it, and guaranteed to alienate contributors.&lt;/p&gt;

&lt;p&gt;The second rewrite attempt started sometime in 2010, about a year after I joined Google as an SRE.  This was after I became quite familiar with Python at work, wanting to use the language to rewrite this tool.  That experiment didn’t go very far though, but I can’t remember why… probably because I was busy enough at work and creating Kyua.&lt;/p&gt;

&lt;p&gt;The third and final rewrite attempt started in 2013 while I had a summer intern and I had a little existential crisis.  The year before I had written &lt;a href=&quot;/2012/07/introducing-sysbuild-for-netbsd.html&quot;&gt;sysbuild&lt;/a&gt; and &lt;a href=&quot;/2012/08/introducing-shtk.html&quot;&gt;shtk&lt;/a&gt;, so I figured recreating pkg_comp using the foundations laid out by these tools would be easy.  And it was… to some extent.&lt;/p&gt;

&lt;p&gt;Getting the barebones of a functional tool took only a few weeks, but that code was far from being stable, portable, and publishable.  Life and work happened, so this fell through the cracks… until late last year, when I decided it was time to close this chapter so I could move on to some other project ideas.  To create the focus and free time required to complete this project, I had to shift my schedule to start the day at 5am instead of 7am—and, many weeks later, the code is finally here and I’m still keeping up with this schedule.&lt;/p&gt;

&lt;p&gt;Granted: this third rewrite is not a fancy one, but it wasn’t meant to be.  pkg_comp 2.0 is still written in shell, just as 1.x was, but this is a good thing because bootstrapping on all supported platforms is easy.  I have to confess that I also considered Go recently &lt;a href=&quot;/2016/03/golang-review.html&quot;&gt;after playing with it last year&lt;/a&gt; but I quickly let go of that thought: at some point I had to ship the 2.0 release, and 10 years since the inception of this rewrite was about time.&lt;/p&gt;

&lt;h2 id=&quot;the-launch-of-20&quot;&gt;The launch of 2.0&lt;/h2&gt;

&lt;p&gt;On February 12th, 2017, the authoritative sources of pkg_comp 1.x were moved from &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgtools/pkg_comp&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgtools/pkg_comp1&lt;/code&gt; to make room for the import of 2.0.  Yes, the 1.x series only existed in pkgsrc and the 2.x series exist as a &lt;a href=&quot;https://github.com/jmmv/pkg_comp&quot;&gt;standalone project on GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And here we are.  Today, February 17th, 2017, pkg_comp 2.0 saw the light!&lt;/p&gt;

&lt;h1 id=&quot;why-sandboxctl-as-a-separate-tool&quot;&gt;Why sandboxctl as a separate tool?&lt;/h1&gt;

&lt;p&gt;sandboxctl is the supporting tool behind pkg_comp, taking care of all the logic involved in creating chroot-based sandboxes on a variety of operating systems.  Some are easy, like building a NetBSD sandbox using release sets, and others are horrifyingly difficult like macOS.&lt;/p&gt;

&lt;p&gt;In pkg_comp 1.x, this logic used to be bundled right into the pkg_comp code, which made it pretty much impossible to generalize for portability.  With pkg_comp 2.x, I decided to split this out into a separate tool to keep responsibilities isolated.  Yes, the integration between the two tools is a bit tricky, but allows for better testability and understandability.  Lastly, having sandboxctl as a standalone tool, instead of just a separate code module, gives you the option of using it for your own sandboxing needs.&lt;/p&gt;

&lt;p&gt;I know, I know; the world has moved onto containerization and virtual machines, leaving chroot-based sandboxes as a very rudimentary thing… but that’s all we’ve got in NetBSD, and pkg_comp targets primarily NetBSD.  Note, though, that because pkg_comp is separate from sandboxctl, there is nothing preventing adding different sandboxing backends to pkg_comp.&lt;/p&gt;

&lt;h1 id=&quot;installation&quot;&gt;Installation&lt;/h1&gt;

&lt;p&gt;Installation is still a bit convoluted unless you are on one of the tier 1 NetBSD platforms or you already have pkgsrc up and running.  For macOS in particular, I plan on creating and shipping a installer image that includes all of pkg_comp dependencies—but I did not want to block the first launch on this.&lt;/p&gt;

&lt;p&gt;For now though, you need to download and install the latest source releases of &lt;a href=&quot;https://github.com/jmmv/shtk/&quot;&gt;shtk&lt;/a&gt;, &lt;a href=&quot;https://github.com/jmmv/sandboxctl/&quot;&gt;sandboxctl&lt;/a&gt;, and &lt;a href=&quot;https://github.com/jmmv/pkg_comp/&quot;&gt;pkg_comp&lt;/a&gt;—in this order; pass the &lt;code class=&quot;highlighter-rouge&quot;&gt;--with-atf=no&lt;/code&gt; flag to the &lt;code class=&quot;highlighter-rouge&quot;&gt;configure&lt;/code&gt; scripts to cut down the required dependencies.  On macOS, you will also need &lt;a href=&quot;https://osxfuse.github.io/&quot;&gt;OSXFUSE&lt;/a&gt; and the &lt;a href=&quot;http://bindfs.org&quot;&gt;bindfs&lt;/a&gt; file system.&lt;/p&gt;

&lt;p&gt;If you are already using pkgsrc, you can install the &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgtools/pkg_comp&lt;/code&gt; package to get the basic tool and its dependencies in place, or you can install the wrapper &lt;code class=&quot;highlighter-rouge&quot;&gt;pkgtools/pkg_comp-cron&lt;/code&gt; package to create a pre-configured environment with a daily cron job to run your builds.  See the package’s &lt;code class=&quot;highlighter-rouge&quot;&gt;MESSAGE&lt;/code&gt; (with &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_info pkg_comp-cron&lt;/code&gt;) for more details.&lt;/p&gt;

&lt;h1 id=&quot;documentation&quot;&gt;Documentation&lt;/h1&gt;

&lt;p&gt;Both pkg_comp and sandboxctl are fully documented in manual pages.  See &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp(8)&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;sandboxctl(8)&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;pkg_comp.conf(5)&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;sandbox.conf(5)&lt;/code&gt; for plenty of additional details.&lt;/p&gt;

&lt;p&gt;As mentioned at the beginning of the post, I plan on publishing one or more tutorials explaining how to bootstrap your pkgsrc installation using pkg_comp on, at least, NetBSD and macOS.  Stay tuned.&lt;/p&gt;

&lt;p&gt;And, if you need support or find anything wrong, please let me know by filing bugs in the corresponding GitHub projects: &lt;a href=&quot;https://github.com/jmmv/pkg_comp/issues&quot;&gt;jmmv/pkg_comp&lt;/a&gt; and &lt;a href=&quot;https://github.com/jmmv/sandboxctl/issues&quot;&gt;jmmv/sandboxctl&lt;/a&gt;.&lt;/p&gt;
</description>
        
        <pubDate>Fri, 17 Feb 2017 21:37:06 +0000</pubDate>
        <link>http://julio.meroh.net/2017/02/introducing-pkg_comp-2.0.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2017/02/introducing-pkg_comp-2.0.html</guid>
        
        
        <category>software</category>
        
      </item>
    
      <item>
        <title>#! /usr/bin/env considered harmful</title>
        
          <description>
            Many programming guides recommend to begin scripts with the #! /usr/bin/env shebang in order to to automatically locate the necessary interpreter. For example, for a Python script you would use #! /usr/bin/env python, and then the saying goes, the script would “just work” on any machine with Python installed.

The reason for this recommendation is that /usr/bin/env python will search the PATH for a program called python and execute the first one found… and that usually works fine on one’s own machine.

Unfortunately, this advice is plagued with problems and assuming it will work is wishful thinking. Let me elaborate. I’ll use Python below for illustration purposes but the following applies equally to any other interpreted language.

Problems

i) The first problem is that using #! /usr/bin/env lets you find an interpreter but not necessarily the correct interpreter. In our example above, we told the system to look for an interpreter called python… but we did not say anything about the compatible versions. Did you want Python 2.x or 3.x? Or maybe “exactly 2.7”? Or “at least 3.2”? You can’t tell right? So the the computer can’t tell either; regardless, the script will probably run with whichever version happens to be called python–which could be any thanks to the alternatives system. The danger is that, if the version is mismatched, the script will fail and the failure can manifest itself at a much later stage (e.g. a syntax error in an infrequent code path) under obscure circumstances.

ii) The second problem, assuming you ignore the version problem above because your script is compatible with all possible versions (hah), is that you may pick up an interpreter that does not have all prerequisite dependencies installed. Say your script decides to import a bunch of third-party modules: where are those modules located? Typically, the modules exist in a centralized repository that is specific to the interpreter installation (e.g. a .../lib/python2.7/site-packages/ directory that lives alongside the interpreter binary). So maybe your program found a Python 2.7 under /usr/local/bin/ but in reality you needed it to find the one in /usr/bin/ because that’s where all your Python modules are. If that happens, you’ll receive an obscure error that doesn’t properly describe the exact cause of the problem you got.

iii) The third problem, assuming your script is portable to all versions (hah again) and that you don’t need any modules (really?), is that you are assuming that the interpreter is available via a specific name. Unfortunately, the name of the interpreter can vary. For example: pkgsrc installs all python binaries with explicitly-versioned names (e.g. python2.7 and python3.0) to avoid ambiguity, and no python symlink is created by default… which means your script won’t run at all even when Python is seemingly installed.

iv) The fourth problem is that you cannot pass flags to the interpreter. The shebang line is intended to contain the name of the interpreter plus a single argument to it. Using /usr/bin/env as the interpreter name consumes the first slot and the name of the interpreter consumes the second, so there is no room to pass additional flags to the program. What happens with the rest of the arguments is platform-dependent: they may be all passed as a single string to env or they may be tokenized as individual arguments. This is not a huge deal though: one argument for flags is too restricted anyway and you can usually set up the interpreter later from within the script.

v) The fifth and worst problem is that your script is at the mercy of the user’s environment configuration. If the user has a “misconfigured” PATH, your script will mysteriously fail at run time in ways that you cannot expect and in ways that may be very difficult to troubleshoot later on. I quote “misconfigured” because the problem here is very subtle. For example: I do have a shell configuration that I carry across many different machines and various operating systems; such configuration has complex logic to determine a sane PATH regardless of the system I’m in… but this, in turn, means that the PATH can end up containing more than one version of the same program. This is fine for interactive shell use, but it’s not OK for any program to assume that my PATH will match their expectations.

vi) The sixth and last problem is that a script prefixed with #! /usr/bin/env is not suitable to being installed. This is justified by all the other points illustrated above: once a program is installed on the system, it must behave deterministically no matter how it is invoked. More importantly, when you install a program, you do so under a set of assumptions gathered by a configure-like script or prespecified by a package manager. To ensure things work, the installed script must see the exact same environment that was specified at installation time. In particular, the script must point at the correct interpreter version and at the interpreter that has access to all package dependencies.

So what to do?

All this considered, you may still use #! /usr/bin/env for the convenience of your own throwaway scripts (those that don’t leave your machine) and also for documentation purposes and as a placeholder for a better default.

For anything else, here are some possible alternatives to using this harmful shebang:


  
    Patch up the scripts during the “build” of your software to point to the specific chosen interpreter based on a setting the user provided at configure time or one that you detected automatically. Yes, this means you need make or similar for a simple script, but these are the realities of the environment they’ll run under…
  
  
    Rely on the packaging system do the patching, which is pretty much what pkgsrc does automatically (and I suppose pretty much any other packaging system out there).
  


Just don’t assume that the magic #! /usr/bin/env foo is sufficient or even correct for the final installed program.

Bonus chatter: There is a myth that the original shebang prefix was #! / so that the kernel could look for it as a 32-bit magic cookie at the beginning of an executable file. I actually believed this myth for a long time… until today, as a couple of readers pointed me at The #! magic, details about the shebang/hash-bang mechanism on various Unix flavours with interesting background that contradicts this.

            &lt;a href=&quot;http://julio.meroh.net/2016/09/env-considered-harmful.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;Many programming guides recommend to begin scripts with the &lt;code class=&quot;highlighter-rouge&quot;&gt;#! /usr/bin/env&lt;/code&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Shebang_(Unix)&quot;&gt;shebang&lt;/a&gt; in order to to automatically locate the necessary interpreter. For example, for a Python script you would use &lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;c&quot;&gt;#! /usr/bin/env python&lt;/span&gt;&lt;/code&gt;, and then the saying goes, the script would “just work” on any machine with Python installed.&lt;/p&gt;

&lt;p&gt;The reason for this recommendation is that &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin/env python&lt;/code&gt; will search the &lt;code class=&quot;highlighter-rouge&quot;&gt;PATH&lt;/code&gt; for a program called &lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt; and execute the first one found… and that usually works fine &lt;em&gt;on one’s own machine&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Unfortunately, this advice is plagued with problems and assuming it will work is wishful thinking. Let me elaborate. I’ll use Python below for illustration purposes but &lt;strong&gt;the following applies equally to any other interpreted language.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;problems&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;i)&lt;/em&gt; The first problem is that &lt;strong&gt;using &lt;code class=&quot;highlighter-rouge&quot;&gt;#! /usr/bin/env&lt;/code&gt; lets you find &lt;em&gt;an&lt;/em&gt; interpreter but not necessarily the &lt;em&gt;correct&lt;/em&gt; interpreter&lt;/strong&gt;. In our example above, we told the system to look for an interpreter called &lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt;… but we did not say &lt;em&gt;anything&lt;/em&gt; about the compatible versions. Did you want Python 2.x or 3.x? Or maybe “exactly 2.7”? Or “at least 3.2”? You can’t tell right? So the the computer can’t tell either; regardless, the script &lt;em&gt;will&lt;/em&gt; probably run with whichever version happens to be called &lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt;–which could be &lt;em&gt;any&lt;/em&gt; thanks to the &lt;a href=&quot;https://wiki.debian.org/DebianAlternatives&quot;&gt;alternatives system&lt;/a&gt;. The danger is that, if the version is mismatched, the script will fail and the failure can manifest itself at a much later stage (e.g. a syntax error in an infrequent code path) under obscure circumstances.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;ii)&lt;/em&gt; The second problem, assuming you ignore the version problem above because your script is compatible with all possible versions (hah), is that &lt;strong&gt;you may pick up an interpreter that does not have all prerequisite dependencies installed&lt;/strong&gt;. Say your script decides to import a bunch of third-party modules: where are those modules located? Typically, the modules exist in a centralized repository that is specific to the interpreter installation (e.g. a &lt;code class=&quot;highlighter-rouge&quot;&gt;.../lib/python2.7/site-packages/&lt;/code&gt; directory that lives alongside the interpreter binary). So maybe your program found a Python 2.7 under &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/bin/&lt;/code&gt; but in reality you needed it to find the one in &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin/&lt;/code&gt; because that’s where all your Python modules are. If that happens, you’ll receive an obscure error that doesn’t properly describe the exact cause of the problem you got.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;iii)&lt;/em&gt; The third problem, assuming your script is portable to all versions (hah again) and that you don’t need any modules (really?), is that &lt;strong&gt;you are assuming that the interpreter is available via a specific name&lt;/strong&gt;. Unfortunately, the name of the interpreter can vary. For example: pkgsrc installs all &lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt; binaries with explicitly-versioned names (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;python2.7&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;python3.0&lt;/code&gt;) to avoid ambiguity, and no &lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt; symlink is created by default… which means your script won’t run at all even when Python is seemingly installed.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;iv)&lt;/em&gt; The fourth problem is that &lt;strong&gt;you cannot pass flags to the interpreter&lt;/strong&gt;. The shebang line is intended to contain the name of the interpreter plus a single argument to it. Using &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin/env&lt;/code&gt; as the interpreter name consumes the first slot and the name of the interpreter consumes the second, so there is no room to pass additional flags to the program. What happens with the rest of the arguments is platform-dependent: they may be all passed as a single string to &lt;code class=&quot;highlighter-rouge&quot;&gt;env&lt;/code&gt; or they may be tokenized as individual arguments. This is not a huge deal though: one argument for flags is too restricted anyway and you can usually set up the interpreter later from within the script.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;v)&lt;/em&gt; The fifth and worst problem is that &lt;strong&gt;your script is at the mercy of the user’s &lt;em&gt;environment&lt;/em&gt; configuration&lt;/strong&gt;. If the user has a “misconfigured” &lt;code class=&quot;highlighter-rouge&quot;&gt;PATH&lt;/code&gt;, your script will mysteriously fail at run time in ways that you cannot expect and in ways that may be very difficult to troubleshoot later on. I quote “misconfigured” because the problem here is very subtle. For example: I do have a shell configuration that I carry across many different machines and various operating systems; such configuration has complex logic to determine a sane &lt;code class=&quot;highlighter-rouge&quot;&gt;PATH&lt;/code&gt; regardless of the system I’m in… but this, in turn, means that the &lt;code class=&quot;highlighter-rouge&quot;&gt;PATH&lt;/code&gt; can end up containing more than one version of the same program. This is fine for interactive shell use, but it’s not OK for any program to assume that my &lt;code class=&quot;highlighter-rouge&quot;&gt;PATH&lt;/code&gt; will match their expectations.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;vi)&lt;/em&gt; The sixth and last problem is that &lt;strong&gt;a script prefixed with &lt;code class=&quot;highlighter-rouge&quot;&gt;#! /usr/bin/env&lt;/code&gt; is not suitable to being installed&lt;/strong&gt;. This is justified by all the other points illustrated above: once a program is installed on the system, it must behave deterministically no matter how it is invoked. More importantly, when you install a program, you do so under a set of assumptions gathered by a &lt;code class=&quot;highlighter-rouge&quot;&gt;configure&lt;/code&gt;-like script or prespecified by a package manager. To ensure things work, the installed script must see the exact same environment that was specified at installation time. In particular, the script must point at the correct interpreter version and at the interpreter that has access to all package dependencies.&lt;/p&gt;

&lt;h2 id=&quot;so-what-to-do&quot;&gt;So what to do?&lt;/h2&gt;

&lt;p&gt;All this considered, you may still use &lt;code class=&quot;highlighter-rouge&quot;&gt;#! /usr/bin/env&lt;/code&gt; for the &lt;strong&gt;convenience of your own throwaway scripts&lt;/strong&gt; (those that don’t leave your machine) and also &lt;strong&gt;for documentation purposes and as a placeholder for a better default&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For anything else, here are some possible alternatives to using this harmful shebang:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Patch up the scripts during the “build” of your software to point to the specific chosen interpreter based on a setting the user provided at &lt;code class=&quot;highlighter-rouge&quot;&gt;configure&lt;/code&gt; time or one that you detected automatically. Yes, this means you need &lt;code class=&quot;highlighter-rouge&quot;&gt;make&lt;/code&gt; or similar for a simple script, but these are the realities of the environment they’ll run under…&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rely on the packaging system do the patching, which is pretty much what pkgsrc does automatically (and I suppose pretty much any other packaging system out there).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Just don’t assume that the magic &lt;code class=&quot;highlighter-rouge&quot;&gt;#! /usr/bin/env foo&lt;/code&gt; is sufficient or even correct&lt;/strong&gt; for the final installed program.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Bonus chatter:&lt;/em&gt; There is a myth that the original shebang prefix was &lt;code class=&quot;highlighter-rouge&quot;&gt;#! /&lt;/code&gt; so that the kernel could look for it as a 32-bit magic cookie at the beginning of an executable file. I actually believed this myth for a long time… until today, as a couple of readers pointed me at &lt;a href=&quot;http://www.in-ulm.de/~mascheck/various/shebang/&quot;&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;#!&lt;/code&gt; magic, details about the shebang/hash-bang mechanism on various Unix flavours&lt;/a&gt; with interesting background that contradicts this.&lt;/p&gt;
</description>
        
        <pubDate>Wed, 14 Sep 2016 11:07:46 +0000</pubDate>
        <link>http://julio.meroh.net/2016/09/env-considered-harmful.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2016/09/env-considered-harmful.html</guid>
        
        
        <category>portability</category>
        
        <category>programming</category>
        
        <category>scripts</category>
        
        <category>unix</category>
        
      </item>
    
      <item>
        <title>Welcome to my homepage, version 3.0</title>
        
          <description>
            Welcome to my updated homepage, this time at its third major version!

First iteration — a year ago

The first iteration of this site, which went live exactly a year ago, was a single static page that leveraged Twitter’s Bootstrap. I created that page for two reasons: to serve as the target for all the “homepage” links that online profiles require, and to learn Bootstrap. Due to the site’s simplicity and goals, its contents were not very interesting nor dynamic.

Second iteration — seven months ago

On the second iteration, I adopted Jekyll to manage the split of the monolithic page into various chunks with the goal of extending their content. Among the new content, I added a “news” section to leverage Jekyll’s functionality in managing blog-like sites and announced the site’s 2.0 release.

Not much later, I concluded that Medium was not the right place to host the original versions of my content—nor was Blogger or any other managed site, for that matter. The main reason was that using a third-party site to host the originals meant that I was at the mercy of that site for the fate of my content (Duh—I know). More importantly, which was the trigger, was that I could not retrieve my content in a format that permitted reuse, which I desired recently.

The realization above made me import all of my Medium posts into this site and, now, the time has come to take the leap and actually turn this site into my full-blown blog.

Third iteration — today

On this third iteration, this site has become my full-blown blog, deprecating the old Blogger-based The Julipedia.

What you will find in this third iteration is a revamped look that surfaces recent content more clearly with fewer highlights on my persona. Additionally, I have imported all the content from The Julipedia verbatim, respecting the URL schema, and will be redirecting the old Blogger address to this site at some point in the near future. (The plan is for existing links in the wild to not break, and for anyone subscribed to the old RSS feed to start picking up new entries automatically… we’ll see how well that works in practice.)

Thoughts on Markdown

Importing the content from Blogger and Medium was extremely painful. It is true that these sites provide an export feature, but the output they yield is incredibly user-hostile. I’m glad to have realized this now and not after several more years or blogging!



With my cynical hat on, the export feature exists primarily for backup purposes and secondarily for peace of mind; otherwise, it’s mostly useless for taking the content elsewhere. It took many contortions to extract the content and reconvert it into a version that I can consider canonical and reusable—i.e. either a Markdown document or a simplified HTML page.

Aside from the import, which I will cover on a separate post later on, authoring original content in Markdown has been rewarding so far. This platform makes me feel in full control (for better or worse) of the whole authoring, editing, and publication process, and I get a good feeling that the master copies of my articles will stand the test of time.

Unfortunately, I have to confess that the barrier to publication via Jekyll is higher, which has had the effect of me publishing less frequently and focusing on longer and more elaborate posts. Or… maybe that’s just correlation: I have been posting more infrequent but longer posts since before the birth of this site.



IMPORTANT: So has this site turned into my authoritative blog? I guess that’s the case!
Make sure to update your feed subscription now to not miss a beat.

Hope you enjoy the new site! There is new stuff coming up soon, including a detailed explanation on how I extracted my old Blogger contents and reposted them in Jekyll (which was far from trivial) and a formal “deactivation” of The Julipedia address.

            &lt;a href=&quot;http://julio.meroh.net/2016/05/homepage-v3.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;Welcome to my updated homepage, this time at its third major version!&lt;/p&gt;

&lt;h1 id=&quot;first-iteration--a-year-ago&quot;&gt;First iteration — a year ago&lt;/h1&gt;

&lt;p&gt;The first iteration of this site, which went live exactly a year ago, was a single static page that leveraged Twitter’s Bootstrap. I created that page for two reasons: to serve as the target for all the “homepage” links that online profiles require, and to learn Bootstrap. Due to the site’s simplicity and goals, its contents were not very interesting nor &lt;em&gt;dynamic&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;second-iteration--seven-months-ago&quot;&gt;Second iteration — seven months ago&lt;/h1&gt;

&lt;p&gt;On the second iteration, I adopted &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; to manage the split of the monolithic page into various chunks with the goal of extending their content. Among the new content, I added a “news” section to leverage Jekyll’s functionality in managing blog-like sites and &lt;a href=&quot;/2015/10/new-homepage.html&quot;&gt;announced the site’s 2.0 release&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Not much later, I &lt;a href=&quot;/2016/01/medium-experiment-wrapup.html&quot;&gt;concluded that Medium&lt;/a&gt; was not the right place to host the original versions of my content—nor was Blogger or any other managed site, for that matter. The main reason was that using a third-party site to host the originals meant that I was at the mercy of that site for the fate of my content (&lt;em&gt;Duh&lt;/em&gt;—I know). More importantly, which was the trigger, was that I could not retrieve &lt;em&gt;my&lt;/em&gt; content in a format that permitted reuse, which I desired recently.&lt;/p&gt;

&lt;p&gt;The realization above made me import all of my Medium posts into this site and, now, the time has come to take the leap and actually turn this site into my full-blown blog.&lt;/p&gt;

&lt;h1 id=&quot;third-iteration--today&quot;&gt;Third iteration — today&lt;/h1&gt;

&lt;p&gt;On this third iteration, this site has become my full-blown blog, deprecating the old Blogger-based &lt;a href=&quot;http://julipedia.meroh.net/&quot;&gt;The Julipedia&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What you will find in this third iteration is a revamped look that surfaces recent content more clearly with fewer highlights on my persona. Additionally, I have imported all the content from The Julipedia verbatim, respecting the URL schema, and will be redirecting the old Blogger address to this site at some point in the near future. (The plan is for existing links in the wild to not break, and for anyone subscribed to the old RSS feed to start picking up new entries automatically… we’ll see how well that works in practice.)&lt;/p&gt;

&lt;h1 id=&quot;thoughts-on-markdown&quot;&gt;Thoughts on Markdown&lt;/h1&gt;

&lt;p&gt;Importing the content from Blogger and Medium was &lt;em&gt;extremely painful&lt;/em&gt;. It is true that these sites provide an export feature, but the output they yield is incredibly user-hostile. I’m glad to have realized this now and not after several more years or blogging!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://knowyourmeme.com/photos/976824-reaction-images&quot;&gt;&lt;img src=&quot;/images/2016-05-29-but-why.gif&quot; class=&quot;float-right&quot; alt=&quot;But why?&quot; width=&quot;30%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With my cynical hat on, the export feature exists primarily for backup purposes and secondarily for peace of mind; otherwise, it’s mostly useless for taking the content elsewhere. It took many contortions to extract the content and reconvert it into a version that I can consider canonical and reusable—i.e. either a Markdown document or a simplified HTML page.&lt;/p&gt;

&lt;p&gt;Aside from the import, which I will cover on a separate post later on, authoring original content in Markdown has been rewarding so far. This platform makes me feel in full control (for better or worse) of the whole authoring, editing, and publication process, and I get a good feeling that the master copies of my articles will stand the test of time.&lt;/p&gt;

&lt;p&gt;Unfortunately, I have to confess that the barrier to publication via Jekyll is higher, which has had the effect of me publishing less frequently and focusing on longer and more elaborate posts. Or… maybe that’s just correlation: I have been posting more infrequent but longer posts since before the birth of this site.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;IMPORTANT: &lt;strong&gt;&lt;em&gt;So has this site turned into my authoritative blog? I guess that’s the case!&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
Make sure to &lt;a href=&quot;/feed.xml&quot;&gt;update your feed subscription&lt;/a&gt; now to not miss a beat.&lt;/p&gt;

&lt;p&gt;Hope you enjoy the new site! There is new stuff coming up soon, including a detailed explanation on how I extracted my old Blogger contents and reposted them in Jekyll (which was far from trivial) and a formal “deactivation” of The Julipedia address.&lt;/p&gt;
</description>
        
        <pubDate>Sun, 29 May 2016 21:00:00 +0000</pubDate>
        <link>http://julio.meroh.net/2016/05/homepage-v3.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2016/05/homepage-v3.html</guid>
        
        
        <category>website</category>
        
      </item>
    
      <item>
        <title>Visual Studio Code: A modern editor</title>
        
          <description>
            On April 14th, 2016, Microsoft announced the 1.0 release of their open-source Visual Studio Code (VSCode) editor. I’ve been drive-testing it for a few months and have been quite pleased with it, so here go my impressions.

How did I get here?

Let’s backtrack a bit first. I’ve been a Vim and Emacs user for many years. Yes, I use both regularly depending on what I have to achieve. For me, Vim shines in doing quick single-file changes and repetitive edits through many files, while Emacs shines in long-lived coding sessions that involve numerous open buffers. These editors are well-suited to my remote-based coding workflow because they run just fine in the terminal. However, sometimes I just would like to take advantage of the desktop environment and the GUI of these two editors on OS X… err.. sucks… so I’ve been wanting to find something else.

As a result, with all the hype around GitHub’s Atom last year, I decided to jump on the bandwagon and gave it a try. My first encounters with Atom were not great: I found the environment bloated—just look at the endless settings and extensions panel—and clunky: the UI seemed convoluted, and Atom would routinely spit internal errors when performing “usual” editing tasks. Granted, these were probably caused by the extensions I had installed… but I don’t think I installed anything out of the ordinary!

No matter what, I carried on and used Atom to write the first version of this site. The experience didn’t “hook me” so I couldn’t be bothered to use Atom for anything else. But, soon after, VSCode entered the scene with a similar premise to Atom, and in fact built on the same platform. You may or may not like Microsoft, but they are pretty damn good at building IDEs… ergo VSCode deserved a try.

Enter VSCode

VSCode’s interface is pretty barebones. There is nothing particularly shiny about the editor when you open it the first time: a sidebar with four spartan sections and an empty content panel to display files. If you dare to launch the settings editor, you are met with two editing panels: one on the left to display the default settings in JSON format, and one on the right with an empty file waiting for your overrides. Uh huh?

That’s right. At first sight, VSCode can give the impression of being a joke—I certainly thought so when I encountered the, literally, settings editor—, but that’s very far from the truth.

Start using the command palette to navigate files and commands; it’s like Emacs’ M-x, but magical. Notice how non-intrusive but powerful the Git integration is—and I heard that it may be even better than Atom’s, which would be ironic… Navigate the editor with reasonable keyboard bindings, just as you would expect. Appreciate the utter simplicity of the configuration mechanism. And, the icing, install extensions: as I understand it, VSCode’s initial target was to support web-based languages, but production-quality extensions now exist for native languages—including C#, C++, and Go.

I’ve personally come to enjoy using VSCode. Part of this is because the editor made me so much more productive writing code in a new language, Go. As I hinted in a previous post, having semantic auto-completion, assisted code navigation, and automated code reformatting plus validation are invaluable features when navigating a new, unknown language. I’d have certainly written Go code in Emacs (and that’s exactly how I started), but it’d have been a much more painful experience.

There are also teeny tiny touches that help everyday tasks. One example is the preview of colors when entering color codes in CSS: the editor displays a little box next to the code filled with the color you provided. Another example is how hovering on CSS properties or JSON configuration properties pops up a bubble with more information about the symbol underneath and all possible values.

Open source project

Can Microsoft deliver an open source project without strange licenses and with development happening in the open? It indeed seems so, which is an 180-degree turn from what the company did a few years ago.

VSCode is an open source project in all regards: the code is released under the very liberal MIT license (but see these FAQs), the project is hosted on GitHub, and Microsoft has been able to create a community around the editor. Make no mistake: VSCode is not a code dump. (I’ve personally filed a few feature requests with them and, even though my requests were rejected, the rationales they provided were convincing and the timeliness of the responses was much appreciated.)

Workspaces

Back to the settings chatter, one particular detail I have come to like is the ability to trivially define and customize workspaces (aka projects): create a .vscode/settings.json file at the top of your tree with any project-specific configuration overrides (e.g. indentation tweaks) and VSCode will apply those transparently. I appreciate the fact that this has to be done explicitly and that the contents of the file are super-simple and human readable. Contrast this to, say, Android Studio, which creates an .idea directory with almost-unreadable contents.

With the right command-line alias, all you have to do is type code . at the top-level directory of your project to launch a new instance of VSCode ready to work on your project—and the editor remembers where you left off last time. In case you are curious, an alias for OS X would be:

alias code="open -a 'Visual Studio Code'"



Some bad parts

So what about the bad? I don’t have much to complain about, really.

Obviously, the fact that VSCode is a graphical editor means that it is not usable on the terminal, which is a blocker for doing remote work. Similarly, VSCode currently only supports Windows, OS X, and Linux; I don’t think there is any technical blocking the support of other systems, but the port needs to be done and is probably not super-easy.

Lastly, while the authors praise VSCode’s small size, I don’t really buy it: the fact that the editor is based on Electron, weights several megabytes, and takes a few seconds to start counters this. But, really, none of these have bothered me.

Parting words

If for any reason you would like to try a different editor, I’d certainly encourage you to give VSCode a chance. Make sure to go over the editor basics document to familiarize yourself with the environment and delve into the more advanced topics later on.

I, for one, will continue to use Vim and Emacs for a lot of my work, but VSCode has gained a spot for certain types of projects. How this split will balance in one direction or the other, only time will tell: there are things I like and dislike in both camps. But remember: it’s all about choosing the right tool for the job, so restricting yourself to just one editor is most likely counterproductive.

If you let me… I’ll conclude by saying that Atom is to Emacs what VSCode is to Vim. “What do you mean by this?” you say? Ah, I don’t know, you’ll have to figure it out yourself!

            &lt;a href=&quot;http://julio.meroh.net/2016/04/vscode.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;On April 14th, 2016, Microsoft announced the &lt;a href=&quot;http://code.visualstudio.com/blogs/2016/04/14/vscode-1.0&quot;&gt;1.0 release&lt;/a&gt; of their open-source &lt;a href=&quot;http://code.visualstudio.com&quot;&gt;Visual Studio Code (VSCode)&lt;/a&gt; editor. I’ve been drive-testing it for a few months and have been quite pleased with it, so here go my impressions.&lt;/p&gt;

&lt;h1 id=&quot;how-did-i-get-here&quot;&gt;How did I get here?&lt;/h1&gt;

&lt;p&gt;Let’s backtrack a bit first. I’ve been a Vim and Emacs user for many years. Yes, I use both &lt;em&gt;regularly&lt;/em&gt; depending on what I have to achieve. For me, Vim shines in doing quick single-file changes and repetitive edits through many files, while Emacs shines in long-lived coding sessions that involve numerous open buffers. These editors are well-suited to &lt;a href=&quot;/2015/09/my-coding-workflow.html&quot;&gt;my remote-based coding workflow&lt;/a&gt; because they run just fine in the terminal. However, sometimes I just &lt;em&gt;would like to&lt;/em&gt; take advantage of the desktop environment and the GUI of these two editors on OS X… err.. sucks… so I’ve been wanting to find something else.&lt;/p&gt;

&lt;p&gt;As a result, with all the hype around GitHub’s &lt;a href=&quot;http://atom.io/&quot;&gt;Atom&lt;/a&gt; last year, I decided to jump on the bandwagon and gave it a try. My first encounters with Atom were not great: I found the environment bloated—just look at the endless settings and extensions panel—and clunky: the UI seemed convoluted, and Atom would routinely spit internal errors when performing “usual” editing tasks. Granted, these were probably caused by the extensions I had installed… but I don’t think I installed anything out of the ordinary!&lt;/p&gt;

&lt;p&gt;No matter what, I carried on and used Atom to write the first version of this site. The experience didn’t “hook me” so I couldn’t be bothered to use Atom for anything else. But, soon after, VSCode entered the scene with a similar premise to Atom, and in fact built on the same platform. You may or may not like Microsoft, but they are pretty damn good at building IDEs… ergo VSCode deserved a try.&lt;/p&gt;

&lt;h1 id=&quot;enter-vscode&quot;&gt;Enter VSCode&lt;/h1&gt;

&lt;p&gt;VSCode’s interface is pretty barebones. There is nothing particularly shiny about the editor when you open it the first time: a sidebar with four spartan sections and an empty content panel to display files. If you dare to launch the settings editor, you are met with two editing panels: one on the left to display the default settings in JSON format, and one on the right with an empty file waiting for your overrides. Uh huh?&lt;/p&gt;

&lt;p&gt;That’s right. At first sight, VSCode can give the impression of being a joke—I certainly thought so when I encountered the, literally, settings editor—, but that’s very far from the truth.&lt;/p&gt;

&lt;p&gt;Start using the &lt;strong&gt;command palette&lt;/strong&gt; to navigate files and commands; it’s like Emacs’ &lt;code class=&quot;highlighter-rouge&quot;&gt;M-x&lt;/code&gt;, but magical. Notice how non-intrusive but powerful the &lt;strong&gt;Git integration&lt;/strong&gt; is—and I heard that it may be even better than Atom’s, which would be ironic… Navigate the editor with &lt;strong&gt;reasonable keyboard bindings&lt;/strong&gt;, just as you would expect. Appreciate the utter &lt;strong&gt;simplicity of the configuration mechanism&lt;/strong&gt;. And, the icing, install &lt;strong&gt;extensions&lt;/strong&gt;: as I understand it, VSCode’s initial target was to support web-based languages, but production-quality extensions now exist for native languages—including C#, C++, and Go.&lt;/p&gt;

&lt;p&gt;I’ve personally come to enjoy using VSCode. Part of this is because the editor made me so much more productive writing code in a new language, Go. As I hinted &lt;a href=&quot;/2016/03/golang-review.html#a-note-on-visual-studio-code&quot;&gt;in a previous post&lt;/a&gt;, having &lt;strong&gt;semantic auto-completion&lt;/strong&gt;, &lt;strong&gt;assisted code navigation&lt;/strong&gt;, and &lt;strong&gt;automated code reformatting plus validation&lt;/strong&gt; are invaluable features when navigating a new, unknown language. I’d have certainly written Go code in Emacs (and that’s exactly how I started), but it’d have been a much more painful experience.&lt;/p&gt;

&lt;p&gt;There are also teeny tiny touches that help everyday tasks. One example is the preview of colors when entering color codes in CSS: the editor displays a little box next to the code filled with the color you provided. Another example is how hovering on CSS properties or JSON configuration properties pops up a bubble with more information about the symbol underneath and all possible values.&lt;/p&gt;

&lt;h2 id=&quot;open-source-project&quot;&gt;Open source project&lt;/h2&gt;

&lt;p&gt;Can Microsoft deliver an open source project without strange licenses and with development happening in the open? It indeed seems so, which is an 180-degree turn from what the company did a few years ago.&lt;/p&gt;

&lt;p&gt;VSCode is an open source project in all regards: the code is released under the very liberal MIT license (but &lt;a href=&quot;http://code.visualstudio.com/Docs/supporting/faq#_licensing&quot;&gt;see these FAQs&lt;/a&gt;), the project is &lt;a href=&quot;http://github.com/microsoft/vscode&quot;&gt;hosted on GitHub&lt;/a&gt;, and Microsoft has been able to create a community around the editor. Make no mistake: VSCode is not a code dump. (I’ve personally filed a few feature requests with them and, even though my requests were rejected, the rationales they provided were convincing and the timeliness of the responses was much appreciated.)&lt;/p&gt;

&lt;h2 id=&quot;workspaces&quot;&gt;Workspaces&lt;/h2&gt;

&lt;p&gt;Back to the settings chatter, one particular detail I have come to like is the ability to trivially define and customize workspaces (aka projects): create a &lt;code class=&quot;highlighter-rouge&quot;&gt;.vscode/settings.json&lt;/code&gt; file at the top of your tree with any project-specific configuration overrides (e.g. indentation tweaks) and VSCode will apply those transparently. I appreciate the fact that this has to be done explicitly and that the contents of the file are super-simple and human readable. Contrast this to, say, Android Studio, which creates an &lt;code class=&quot;highlighter-rouge&quot;&gt;.idea&lt;/code&gt; directory with almost-unreadable contents.&lt;/p&gt;

&lt;p&gt;With the right command-line alias, all you have to do is type &lt;code class=&quot;highlighter-rouge&quot;&gt;code .&lt;/code&gt; at the top-level directory of your project to launch a new instance of VSCode ready to work on your project—and the editor remembers where you left off last time. In case you are curious, an alias for OS X would be:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;alias code=&quot;open -a 'Visual Studio Code'&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;some-bad-parts&quot;&gt;Some bad parts&lt;/h2&gt;

&lt;p&gt;So what about the bad? I don’t have much to complain about, really.&lt;/p&gt;

&lt;p&gt;Obviously, the fact that VSCode is a graphical editor means that it is not usable on the terminal, which is a blocker for doing remote work. Similarly, VSCode currently only supports Windows, OS X, and Linux; I don’t think there is any technical blocking the support of other systems, but the port needs to be done and is probably not super-easy.&lt;/p&gt;

&lt;p&gt;Lastly, while the authors praise VSCode’s small size, I don’t really buy it: the fact that the editor is based on &lt;a href=&quot;http://electron.atom.io/&quot;&gt;Electron&lt;/a&gt;, weights several megabytes, and takes a few seconds to start counters this. But, really, none of these have bothered me.&lt;/p&gt;

&lt;h1 id=&quot;parting-words&quot;&gt;Parting words&lt;/h1&gt;

&lt;p&gt;If for any reason you would like to try a different editor, I’d certainly encourage you to give VSCode a chance. Make sure to go over the &lt;a href=&quot;https://code.visualstudio.com/docs/editor/codebasics&quot;&gt;editor basics document&lt;/a&gt; to familiarize yourself with the environment and delve into the more advanced topics later on.&lt;/p&gt;

&lt;p&gt;I, for one, will continue to use Vim and Emacs for a lot of my work, but VSCode has gained a spot for certain types of projects. How this split will balance in one direction or the other, only time will tell: there are things I like and dislike in both camps. But remember: it’s all about choosing the right tool for the job, so restricting yourself to just one editor is most likely counterproductive.&lt;/p&gt;

&lt;p&gt;If you let me… I’ll conclude by saying that Atom is to Emacs what VSCode is to Vim. “&lt;em&gt;What do you mean by this?&lt;/em&gt;” you say? Ah, I don’t know, you’ll have to figure it out yourself!&lt;/p&gt;
</description>
        
        <pubDate>Tue, 19 Apr 2016 15:30:00 +0000</pubDate>
        <link>http://julio.meroh.net/2016/04/vscode.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2016/04/vscode.html</guid>
        
        
        <category>development</category>
        
        <category>software</category>
        
        <category>workflow</category>
        
      </item>
    
      <item>
        <title>A look at Go from a newbie's perspective</title>
        
          <description>
            I confess I am late to the game: the Go programming language came out in 2009 and I had not had the chance to go all in for a real project until two weeks ago. Here is a summary of my experience. Spoiler alert: I’m truly pleased.

The project

What I set out to build is a read-only caching file system to try to solve the problems I presented in my previous analysis of large builds on SSHFS. The reasons I chose Go are simple: I had to write a low-level system component and, in theory, Go excels at this; I did not want to use plain C; Go had the necessary bindings (for FUSE and SQLite); and, heck, I just wanted to try it out!

It only took me a little over two days to get a fully-functional implementation of my file system, and this is without having ever written a FUSE-based file system before nor any non-toy Go code. That said, I had previously written a kernel file system in 2005 and that helped navigating the whole endeavor and tuning the resulting performance.

Where is the code you ask? Nowhere yet unfortunately; I hope to make it available once it is ready.

The Go language review

The summary of what follows is simple: after two weeks, I’m in love with Go.

I remember languages people dismiss Go back in 2009 on the grounds that it was a simplistic language without novel concepts… but there lies its beauty: Go lets you get things done quickly and safely with few performance penalties. I feel I can write much more robust code with Go than with any other comparable language.

Disclaimer: Keep in mind that I’m still a newbie. I’m sure that some of the items below are naïve, incomplete or plain incorrect. If so, please let me know!

The good


  
    Enforced coding style: You may or may not like Go’s coding style (I personally would change a few things), but the fact that it is chosen for you and that gofmt exists beats any personal preferences you may have. No more thinking about tabs vs. spaces; no more thinking about brace placement; no more thinking about the look of your code. It. Just. Does. Not. Matter. Focus on your code’s logic and let the machine format it in a consistent manner across the whole Go ecosystem.
  
  
    Explicit error handling: Yes, the if err != nil { return err } pattern gets old very quickly, but having to explicitly handle errors shows you how hard it is to write robust code.

    You’d say that C is similar in this regard, but not really. There are two key differences: the first is that Go functions can return more than one result, which makes it more difficult to ignore the error code if you want to use the actual result; and the second is the defer keyword, which helps avoid leaks in error paths. I personally prefer C++’s ability to implement the RAII pattern, but defer is a good compromise.

    Do I miss exceptions? Not really. While exceptions let you keep your main code path clean of error handling, they also relegate error handling to a secondary place. This is OKish if the language has provisions to ensure that exceptions are handled at some point like Java’s throws definition, but that’s not how the majority of languages work. Just think of how easy is for exceptions leak all the way through the program’s entry point in C++ or Python.
  
  
    Strong typing, even for native types: The fact that the equivalent of C’s typedef generates types that can’t be implicitly mixed, or the fact that numeric types are not automatically promoted to other types, is a good way to prevent subtle bugs. Having to explicitly convert among types forces you to think about the consequences of doing so: Integer overflow? Check. Sign issues? Check. Precision issues? Check.
  
  
    Builtin profiling with pprof: The pprof profiling tool was invaluable to diagnose performance and memory consumption deficiencies in my caching file system and the libraries I used. What’s best is that pprof was also incredibly easy to plug into my program.

    Honestly, I’m in awe to see that this tooling is open-source because I’ve gotten very accustomed to this particular way of debugging at Google.
  
  
    The standard library: While Go by itself is a decent language, it becomes excellent because of its extensive standard library. The existence of tooling to implement pretty much anything you want wins over any deficiencies you may think the language has. The same applies to Java, by the way.
  
  
    Simple package management: Getting started with a piece of code is easy, and pulling in additional dependencies is trivial. I actually do not like the underpinnings of this “modern” trend of language-specific package management systems, but I must admit that it has been a pleasure to just add new imports, call go get, and have everything up and running in a matter of seconds.
  
  
    Build speed: This item is not tied to my recent experience with Go but is worth mentioning. About two years ago, I built the Go language on my underpowered NetBSD VM. I was expecting horrendously long build times like those of GCC or Clang, but was pleasantly surprised to see that the Go compiler, along with its extensive standard library, compiled in a few minutes.
  


The bad


  
    GOPATH: Sorry but having to modify the environment to get a program to run in the default case is ridiculous in this day and age: things should “just work” and Go is different. I have mitigated my concerns about this by writing a Makefile that sets things up automatically so I do not have to worry about customizing my environment on a project basis.
  
  
    Package management: As we saw in the good parts, Go’s package management gets the job done but its views on the world are too simplistic. On the one hand, the development environment assumes that you will always want to use the HEAD versions of your dependencies; and on the other hand, the default is to merge the dependencies with your personal code in the same workspace.

    I’ve managed to work around the latter by splitting my dependencies in a separate directory and using GOPATH to find them. The excellent article So you want to write a packaging system is a great resource on this topic, and as you read through it, you will realize that it’s geared towards revamping Go’s package manager.
  
  
    The simplistic log module: It is good that the standard library provides a logging module, but it seems way too simple for anything other than trivial logging. Fortunately, the external glog module provides increased functionality such as level-based logging and persistent logging. Therefore, you may say that being able to choose between something simple and something more complex is good, right?!

    Unfortunately, that’s not the case and this is the reason I pinpoint this particular module from the standard library. The real benefits of using glog come when the whole stack uses the same logging infrastructure and principles. Because glog is not the standard module, some Go packages will use it and others will not. As a result, you do not get the full benefits of using glog in your project because most libraries you depend on will not do the same.
  
  
    Mutability by default (aka no const keyword): Before you say “hey, const is currently not present, but if we find the need for it later on, we’ll add it!”, that’s… not great. The problem with adding a const keyword post-facto is that this is the wrong keyword to add: state should be immutable by default, and what the language should have is a mutable keyword to clearly mark those variables that hold multiple values throughout their lifetime.

    The reason I point out this specific language feature is because I’ve mentally gotten very accustomed to separating constant vs. mutable variables in code. Reading code that adheres to this strict separation is easy to follow: you can quickly map the immutable state in your mind and then focus on the mutable pieces of the code to understand what’s happening. Without these clues, you need extra effort.
  


The ugly


  
    Short identifiers: A lot of Go code, including the snippets in the documentation, is plagued by extremely short identifiers. Those are used as local variables, function arguments, method receivers, and even structure members. Combined with the heavy use of interfaces, it is hard to read the code: What’s a c? And an r? And an s.wg? Yes, there is bad code all around, but when the official language documentation seems to recommend this approach, you can expect that others will follow the apparent recommendation in all cases.

    Remember that code is written to be read many more times than it is written. Fully spelled-out identifiers help significantly.
  
  
    Maximum line length: Even though Go has a predefined coding style, there is no pre-specified maximum line length. While it is great to want to abolish a custom that remains from the punch-card days, keeping code narrow has its benefits: think of two or three side-by-side editors in a small laptop screen.

    This would be a non-issue if editors were able to wrap long code lines in a way that made sense—just like word processors do with text—but current editors are terrible at this. So, personally, I will stick to the 80 character limit; I have given 100 a try per more modern recommendations… but when I opened the code in my laptop and couldn’t see two files at once without wrapping, I knew I had to go back to 80.
  


A note on Visual Studio Code

To conclude, let’s talk about editors. I’ve been a long-time Vim and Emacs user and now decided to give another editor a try. Heresy!

In fact, some time last year, I started playing with Atom after all the hype around this editor. It really is full-featured, but it also is overwhelming and fragile: overwhelming because there are knobs everywhere and fragile because I had never had an editor throw errors at me routinely during “normal” operation. (I’m sure I can blame some plugin I installed, but still.)

Soon after, Visual Studio Code (VSCode) came out: an open source editor from Microsoft no less. I installed VSCode just to take a look and I found much sought simplicity: just look at the way to configure the editor, which is by adding personal or project-specific overrides to an empty JSON file. I felt I’d enjoy this editor more than Atom though I did not have a use for it at the time: most of my development still happens over SSH so VSCode would not fit the bill.

This experiment with Go gave me the chance to use VSCode, particularly because I was writing the project primarily for OS X (my desktop) and because I wanted an IDE-like experience. As it turns out, VSCode has a pretty good Go plugin. The reason I wanted IDE integration is because writing code in an unknown language with an unknown standard library is a task that truly benefits from autocompletion and inline syntax validation. I wouldn’t have been able to prototype my file system as quickly as I did with a bare editor.

I know, I know, I could have set up Emacs to do the same thing, but I just wanted to give VSCode a ride. And, mind you, I just took a look at what’s involved to set Emacs up for Go and didn’t find it pleasing; way too much manual work involved.

Do you Go already?

            &lt;a href=&quot;http://julio.meroh.net/2016/03/golang-review.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;I confess I am late to the game: the &lt;a href=&quot;https://golang.org/&quot;&gt;Go programming language&lt;/a&gt; came out in 2009 and I had not had the chance to go &lt;em&gt;all in&lt;/em&gt; for a real project until two weeks ago. Here is a summary of my experience. Spoiler alert: I’m truly pleased.&lt;/p&gt;

&lt;h1 id=&quot;the-project&quot;&gt;The project&lt;/h1&gt;

&lt;p&gt;What I set out to build is a read-only caching file system to try to solve the problems I presented in my previous &lt;a href=&quot;/2016/02/sshfs-performance-analysis-for-builds.html&quot;&gt;analysis of large builds on SSHFS&lt;/a&gt;. The reasons I chose Go are simple: I had to write a low-level system component and, in theory, Go excels at this; I did not want to use plain C; Go had the necessary bindings (for &lt;a href=&quot;https://bazil.org/fuse/&quot;&gt;FUSE&lt;/a&gt; and &lt;a href=&quot;https://github.com/mattn/go-sqlite3&quot;&gt;SQLite&lt;/a&gt;); and, heck, I &lt;em&gt;just&lt;/em&gt; wanted to try it out!&lt;/p&gt;

&lt;p&gt;It only took me a little over two days to get a fully-functional implementation of my file system, and this is without having ever written a FUSE-based file system before nor any non-toy Go code. That said, I had previously &lt;a href=&quot;http://netbsd-soc.sourceforge.net/projects/tmpfs/&quot;&gt;written a kernel file system in 2005&lt;/a&gt; and &lt;em&gt;that&lt;/em&gt; helped navigating the whole endeavor and tuning the resulting performance.&lt;/p&gt;

&lt;p&gt;Where is the code you ask? Nowhere yet unfortunately; I hope to make it available once it is ready.&lt;/p&gt;

&lt;h1 id=&quot;the-go-language-review&quot;&gt;The Go language review&lt;/h1&gt;

&lt;p&gt;The summary of what follows is simple: after two weeks, I’m in love with Go.&lt;/p&gt;

&lt;p&gt;I remember languages people dismiss Go back in 2009 on the grounds that it was a simplistic language without novel concepts… but there lies its beauty: Go lets you get things done quickly and safely with few performance penalties. I feel I can write much more robust code with Go than with any other comparable language.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; Keep in mind that I’m still a newbie. I’m sure that some of the items below are naïve, incomplete or plain incorrect. If so, please let me know!&lt;/p&gt;

&lt;h2 id=&quot;the-good&quot;&gt;The good&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Enforced coding style:&lt;/strong&gt; You may or may not like Go’s coding style (I personally would change a few things), but the fact that it is chosen for you and that &lt;code class=&quot;highlighter-rouge&quot;&gt;gofmt&lt;/code&gt; exists beats any personal preferences you may have. No more thinking about tabs vs. spaces; no more thinking about brace placement; no more thinking about the look of your code. &lt;em&gt;It. Just. Does. Not. Matter.&lt;/em&gt; Focus on your code’s logic and let the machine format it in a consistent manner across the whole Go ecosystem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Explicit error handling:&lt;/strong&gt; Yes, the &lt;code class=&quot;highlighter-rouge&quot;&gt;if err != nil { return err }&lt;/code&gt; pattern gets old very quickly, but having to explicitly handle errors shows you how hard it is to write robust code.&lt;/p&gt;

    &lt;p&gt;You’d say that C is similar in this regard, but not really. There are two key differences: the first is that Go functions can return more than one result, which makes it more difficult to ignore the error code if you want to use the actual result; and the second is the &lt;code class=&quot;highlighter-rouge&quot;&gt;defer&lt;/code&gt; keyword, which helps avoid leaks in error paths. I personally prefer C++’s ability to implement &lt;a href=&quot;https://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization&quot;&gt;the RAII pattern&lt;/a&gt;, but &lt;code class=&quot;highlighter-rouge&quot;&gt;defer&lt;/code&gt; is a good compromise.&lt;/p&gt;

    &lt;p&gt;Do I miss exceptions? Not really. While exceptions let you keep your main code path clean of error handling, they also relegate error handling to a secondary place. This is OKish if the language has provisions to ensure that exceptions are handled at some point like Java’s &lt;code class=&quot;highlighter-rouge&quot;&gt;throws&lt;/code&gt; definition, but that’s not how the majority of languages work. Just think of how easy is for exceptions leak all the way through the program’s entry point in C++ or Python.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Strong typing, even for native types:&lt;/strong&gt; The fact that the equivalent of C’s &lt;code class=&quot;highlighter-rouge&quot;&gt;typedef&lt;/code&gt; generates types that can’t be implicitly mixed, or the fact that numeric types are not automatically promoted to other types, is a good way to prevent subtle bugs. Having to explicitly convert among types forces you to think about the consequences of doing so: Integer overflow? Check. Sign issues? Check. Precision issues? Check.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Builtin profiling with pprof:&lt;/strong&gt; The &lt;a href=&quot;http://blog.golang.org/profiling-go-programs&quot;&gt;pprof profiling tool&lt;/a&gt; was invaluable to diagnose performance and memory consumption deficiencies in my caching file system &lt;em&gt;and&lt;/em&gt; the libraries I used. What’s best is that pprof was also incredibly easy to plug into my program.&lt;/p&gt;

    &lt;p&gt;Honestly, I’m in awe to see that this tooling is open-source because I’ve gotten very accustomed to this particular way of debugging at Google.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The standard library:&lt;/strong&gt; While Go by itself is a decent language, it becomes excellent because of its &lt;a href=&quot;https://golang.org/pkg/&quot;&gt;extensive standard library&lt;/a&gt;. The existence of tooling to implement pretty much anything you want wins over any deficiencies you may think the language has. The same applies to Java, by the way.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Simple package management:&lt;/strong&gt; Getting started with a piece of code is easy, and pulling in additional dependencies is &lt;em&gt;trivial&lt;/em&gt;. I actually do not like the underpinnings of this “modern” trend of language-specific package management systems, but I must admit that it has been a pleasure to just add new imports, call &lt;code class=&quot;highlighter-rouge&quot;&gt;go get&lt;/code&gt;, and have everything up and running in a matter of seconds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Build speed:&lt;/strong&gt; This item is not tied to my recent experience with Go but is worth mentioning. About two years ago, I built the Go language on my underpowered NetBSD VM. I was expecting horrendously long build times like those of GCC or Clang, but was pleasantly surprised to see that the Go compiler, along with its extensive standard library, compiled in a few minutes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-bad&quot;&gt;The bad&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GOPATH&lt;/code&gt;:&lt;/strong&gt; Sorry but having to modify the environment to get a program to run in the default case is ridiculous in this day and age: things should “just work” and Go is different. I have mitigated my concerns about this by writing a &lt;code class=&quot;highlighter-rouge&quot;&gt;Makefile&lt;/code&gt; that sets things up automatically so I do not have to worry about customizing my environment on a project basis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Package management:&lt;/strong&gt; As we saw in the good parts, Go’s package management gets the job done but its views on the world are too simplistic. On the one hand, the development environment assumes that you will always want to use the &lt;code class=&quot;highlighter-rouge&quot;&gt;HEAD&lt;/code&gt; versions of your dependencies; and on the other hand, the default is to merge the dependencies with your personal code in the same workspace.&lt;/p&gt;

    &lt;p&gt;I’ve managed to work around the latter by splitting my dependencies in a separate directory and using &lt;code class=&quot;highlighter-rouge&quot;&gt;GOPATH&lt;/code&gt; to find them. The excellent article &lt;a href=&quot;https://medium.com/@sdboyer/so-you-want-to-write-a-package-manager-4ae9c17d9527#.e666sl8yr&quot;&gt;So you want to write a packaging system&lt;/a&gt; is a great resource on this topic, and as you read through it, you will realize that it’s geared towards revamping Go’s package manager.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The simplistic &lt;code class=&quot;highlighter-rouge&quot;&gt;log&lt;/code&gt; module:&lt;/strong&gt; It is good that the standard library provides a logging module, but it seems way too simple for anything other than trivial logging. Fortunately, the external &lt;a href=&quot;https://github.com/golang/glog&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;glog&lt;/code&gt;&lt;/a&gt; module provides increased functionality such as level-based logging and persistent logging. Therefore, you may say that being able to choose between something simple and something more complex is good, right?!&lt;/p&gt;

    &lt;p&gt;Unfortunately, that’s not the case and this is the reason I pinpoint this particular module from the standard library. The real benefits of using &lt;code class=&quot;highlighter-rouge&quot;&gt;glog&lt;/code&gt; come when the &lt;em&gt;whole stack&lt;/em&gt; uses the same logging infrastructure and principles. Because &lt;code class=&quot;highlighter-rouge&quot;&gt;glog&lt;/code&gt; is not the standard module, some Go packages will use it and others will not. As a result, you do not get the full benefits of using &lt;code class=&quot;highlighter-rouge&quot;&gt;glog&lt;/code&gt; in your project because most libraries you depend on will not do the same.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mutability by default (aka no &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt; keyword):&lt;/strong&gt; Before you say “hey, &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt; is currently not present, but if we find the need for it later on, we’ll add it!”, that’s… not great. The problem with adding a &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt; keyword post-facto is that this is the wrong keyword to add: &lt;em&gt;state should be immutable by default&lt;/em&gt;, and what the language should have is a &lt;code class=&quot;highlighter-rouge&quot;&gt;mutable&lt;/code&gt; keyword to clearly mark those variables that hold multiple values throughout their lifetime.&lt;/p&gt;

    &lt;p&gt;The reason I point out this specific language feature is because I’ve mentally gotten very accustomed to separating constant vs. mutable variables in code. Reading code that adheres to this strict separation is easy to follow: you can quickly map the immutable state in your mind and then focus on the mutable pieces of the code to understand what’s happening. Without these clues, you need extra effort.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-ugly&quot;&gt;The ugly&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Short identifiers:&lt;/strong&gt; A lot of Go code, including the snippets in the documentation, is plagued by extremely short identifiers. Those are used as local variables, function arguments, method receivers, and even structure members. Combined with the heavy use of interfaces, it is hard to read the code: What’s a &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt;? And an &lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt;? And an &lt;code class=&quot;highlighter-rouge&quot;&gt;s.wg&lt;/code&gt;? Yes, there is bad code all around, but when the official language documentation seems to recommend this approach, you can expect that others will follow the apparent recommendation in all cases.&lt;/p&gt;

    &lt;p&gt;Remember that &lt;em&gt;code is written to be read many more times than it is written&lt;/em&gt;. Fully spelled-out identifiers help significantly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Maximum line length:&lt;/strong&gt; Even though Go has a predefined coding style, there is no pre-specified maximum line length. While it is great to want to abolish a custom that remains from the punch-card days, keeping code narrow has its benefits: think of two or three side-by-side editors in a small laptop screen.&lt;/p&gt;

    &lt;p&gt;This would be a non-issue if editors were able to wrap long code lines in a way that made sense—just like word processors do with text—but current editors are terrible at this. So, personally, I will stick to the 80 character limit; I have given 100 a try per more modern recommendations… but when I opened the code in my laptop and couldn’t see two files at once without wrapping, I knew I had to go back to 80.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;a-note-on-visual-studio-code&quot;&gt;A note on Visual Studio Code&lt;/h1&gt;

&lt;p&gt;To conclude, let’s talk about editors. I’ve been a long-time Vim and Emacs user and now decided to give another editor a try. Heresy!&lt;/p&gt;

&lt;p&gt;In fact, some time last year, I started playing with &lt;a href=&quot;https://atom.io/&quot;&gt;Atom&lt;/a&gt; after all the hype around this editor. It really is full-featured, but it also is overwhelming and fragile: overwhelming because there are knobs everywhere and fragile because I had never had an editor throw errors at me routinely during “normal” operation. (I’m sure I can blame some plugin I installed, but still.)&lt;/p&gt;

&lt;p&gt;Soon after, &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;Visual Studio Code (VSCode)&lt;/a&gt; came out: an open source editor from Microsoft no less. I installed VSCode just to take a look and I found much sought simplicity: just look at the way to configure the editor, which is by adding personal or project-specific overrides to an empty JSON file. I felt I’d enjoy this editor more than Atom though I did not have a use for it at the time: &lt;a href=&quot;https://medium.com/@jmmv/my-coding-workflow-f26f81235752&quot;&gt;most of my development still happens over SSH&lt;/a&gt; so VSCode would not fit the bill.&lt;/p&gt;

&lt;p&gt;This experiment with Go gave me the chance to use VSCode, particularly because I was writing the project primarily for OS X (my desktop) and because I wanted an IDE-like experience. As it turns out, VSCode has a pretty good &lt;a href=&quot;https://github.com/Microsoft/vscode-go&quot;&gt;Go plugin&lt;/a&gt;. The reason I wanted IDE integration is because writing code in an unknown language with an unknown standard library is a task that truly benefits from autocompletion and inline syntax validation. I wouldn’t have been able to prototype my file system as quickly as I did with a bare editor.&lt;/p&gt;

&lt;p&gt;I know, I know, I could have set up Emacs to do the same thing, but I just wanted to give VSCode a ride. And, mind you, I just took a look at what’s involved to set Emacs up for Go and &lt;a href=&quot;http://tleyden.github.io/blog/2014/05/22/configure-emacs-as-a-go-editor-from-scratch/&quot;&gt;didn’t find it pleasing&lt;/a&gt;; way too much manual work involved.&lt;/p&gt;

&lt;p&gt;Do you Go already?&lt;/p&gt;
</description>
        
        <pubDate>Tue, 22 Mar 2016 21:00:00 +0000</pubDate>
        <link>http://julio.meroh.net/2016/03/golang-review.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2016/03/golang-review.html</guid>
        
        
        <category>software</category>
        
      </item>
    
      <item>
        <title>Those pesky Makefiles</title>
        
          <description>
            As a software developer, you have probably disregarded the build system of your project—those pesky Makefiles—as unimportant. You have probably “chosen” to use the de-facto build tool make(1). And you have probably hacked your way around until things “seemingly worked”.

But hang on a second. Those build files are way more important than you may think and deserve a wee bit more attention.



What if… you lost them?

Let’s start with an exercise, one that I heard many years ago when parts of the Windows 2000 source code were leaked. What if you received massive amounts of source code without the build files? Could you be able use that code again? “Of course” you could, but it would take an extraordinary effort to make the pieces work again as originally intended. It would be even harder to recover the intended state if the original build system allowed any kind of customization—think building for different production or test environments—or had, say, cross-building functionality. And you could even be introducing new run-time bugs by getting the build wrong!

The reason this came to mind is because I spent all of last month fighting build rules to make Bazel build itself on a newly-supported environment. A month of fiddling with build rules and small portability fixes; a month without writing any substantial code.

Well, in reality I “only” spent a whole week ploughing hacks to make this happen… but then I spent the other three weeks cleaning up the hacks so that I could check them in. It was one thing to prove that the end goal was attainable, and it was a very different one to actually reach that goal in a reasonable manner. Extending build rules that work under multiple scenarios and that target different platforms is hard, especially while keeping existing functionality stable.

Another reason this came to mind is that, recently, I have been toying with writing a Go Appengine app. Running the trivial “Hello World” sample is easy: type some code in .go files, run goapp serve, and boom: the app (re)compiles in the background and is served on the web. Good? No.

Unfortunately, as soon as you start writing “the real stuff”, you end up requiring features not provided by the simplistic and basic toolchain. You may need to fetch dependencies depending on the developer’s needs; you may need to build auxiliary tools to set up the production environment; you may need to run those tools with specific flags for your project and for the developer machine; you may need to interact with the VCS system to install hooks and get those working; etc. All these require some form of plumbing, which I chose to do via make(1) and a hand-tuned configure script—purely out of routine, not because these are great tools—and they have grown more than originally expected.

Writing build files is… hard

Build systems are complex beasts and the average developer should not have to—or, in fact, does not want to—care about them. Regardless, most developers have to end up writing build rules for the software they write (who else would?) and they do so without understanding the fundamentals of the tools nor the implications of their code.

Take, for example, configure.ac scripts. GNU Autoconf has excellent documentation—and reading through it is a humbling exercise, especially if you are ever so tempted to write your own “simplified” configuration tool. (Spoiler: don’t.)

Anyway: the vast majority of (open source) projects end up requiring a configure.ac script… but the developers, understandably, won’t go through the hassle of learning all the details of Autoconf because writing a configuration script is not their goal: it’s just boilerplate. The end result is that the developers copy/paste other people’s scripts and/or snippets from shabby sources. Repeat this copy/paste process over a handful of projects and you end up with a Frankenstein-like configuration script that can barely stand on its own. Yet, it seems to work on the developer’s machine, and therefore gets shipped to the world… only to cause pain down the road.

The same applies to any script used to maintain the project. Think of the auxiliary scripts that invoke continuous tests under Travis, the scripts to sanitize your source tree, the scripts to package your software, or even your installation script. Because these are not part of the project’s “core functionality”, they are often thought as unimportant and are just hacked away. Just keep in mind that those scripts will fail due to their poor quality, and it’s no fun at all when they do so at, say, release time.

Stick to conventions

So how do you minimize the danger of hurting yourself? Do not reinvent the wheel. Stick to existing tools and conventions.

If you think your project is special enough to warrant a hand-tuned build system, think again. Writing your own build system is a recipe for disaster: first, because you are throwing away the collective wisdom of the people that wrote the existing tools; and, second, because you immediately make your package unbuildable out-of-the-box by any existing packaging system.

Let’s see some examples on the lots of conventions that exist: support for environment variables (CC, CFLAGS, CPPFLAGS, et. al.) and how they are later passed to the compiler, the linker, and the preprocessor; specific build targets that should exist (all, install, check, and the myriad required by the widespread GNU Coding Standards); the tools that you should end up invoking (e.g. install instead of plain cp to put your files in the destination location); the hierarchy of the installed files and the ability to tune the layout depending on the platform; support for build directories separate from the source tree; support for cross-compilation; support for dynamic detection of compiler and system features; and a long etc.

Note that the examples above only scratch the surface: I focused exclusively on what your standard software package for a Unix system would need, and nothing else. Take an interpreted language instead of C or C++ and, suddenly, the conventions are all different.

The problem here is that users of your package expect all these things to “just work” based on established conventions. If you change those conventions intentionally or just out of knowledge, your package will not behave as the user expects. And if that’s the case, can you guess what the user will think? Your software is broken.

Which conventions?

OK, so we need to follow conventions. Which ones? As always, it depends. It depends on the project you are writing, it depends on your target environment, and it depends on who your users will be.

There are dozens of build systems and each has gained a specific niche so you need to stick to your niche’s preferred set of tools. To name a few: if you are writing a low-level system package, or your package uses C/C++, use the GNU Autotools; if you are writing a Python package, stick to distutils or setuptools and PyPI; if you are writing a KDE component, use CMake; if you are writing an Android app, choose Android Studio and Gradle; if you want to go fancier and the Java requirement is not a concern, consider Bazel (really, it’s cool!).

You may or may not like the conventions of the tools used in the area you are working on, but you are better off by following the conventions than going against them: you’ll waste fewer time and your users will be happier. E.g. personally, I do not like how the standard way of using the Autotools is by shipping massive generated scripts, or the “strange” behavior of Python’s setup.py scripts… but I just use these in my software because they are what people—and, more importantly, packagers—expect.

This doesn’t mean the tools we have today are good; in fact, many aren’t. But if you spend a few minutes to learn about the shortcomings of your tools and how to make better use of them, the results will be palatable. Give the same care you give code to the build files of your project.

To conclude, follow onto the excellent-but-extremely-long article titled So you want to write a package manager. It does not talk about build systems per se, but it sheds light on the many complexities of managing a source tree and its dependencies, and how those should be hidden away from the developers.

            &lt;a href=&quot;http://julio.meroh.net/2016/03/those-pesky-makefiles.html&quot;&gt;[Continue reading]&lt;/a&gt;
          </description>
        
          <description>&lt;p&gt;As a software developer, you have probably disregarded the build system of your project—those pesky &lt;code class=&quot;highlighter-rouge&quot;&gt;Makefile&lt;/code&gt;s—as unimportant. You have probably “chosen” to use the de-facto build tool &lt;code class=&quot;highlighter-rouge&quot;&gt;make(1)&lt;/code&gt;. And you have probably hacked your way around until things “seemingly worked”.&lt;/p&gt;

&lt;p&gt;But hang on a second. Those build files are way more important than you may think and deserve a wee bit more attention.&lt;/p&gt;

&lt;!--end-of-excerpt--&gt;

&lt;h1 id=&quot;what-if-you-lost-them&quot;&gt;What if… you lost them?&lt;/h1&gt;

&lt;p&gt;Let’s start with an exercise, one that I heard many years ago when &lt;a href=&quot;http://news.microsoft.com/2004/02/12/statement-from-microsoft-regarding-illegal-posting-of-windows-2000-source-code/&quot;&gt;parts of the Windows 2000 source code were leaked&lt;/a&gt;. &lt;strong&gt;What if you received massive amounts of source code without the build files?&lt;/strong&gt; Could you be able use that code again? “Of course” you &lt;em&gt;could&lt;/em&gt;, but it would take an extraordinary effort to make the pieces work again as originally intended. It would be even harder to recover the intended state if the original build system allowed any kind of customization—think building for different production or test environments—or had, say, cross-building functionality. And you could even be introducing new run-time bugs by getting the build wrong!&lt;/p&gt;

&lt;p&gt;The reason this came to mind is because I spent all of last month fighting build rules to make &lt;a href=&quot;http://bazel.io/&quot;&gt;Bazel&lt;/a&gt; build itself on a newly-supported environment. A month of fiddling with build rules and small portability fixes; a month without writing any substantial code.&lt;/p&gt;

&lt;p&gt;Well, in reality I “only” spent a whole week ploughing hacks to make this happen… but then I spent the other three weeks cleaning up the hacks so that I could check them in. It was one thing to prove that the end goal was attainable, and it was a very different one to actually reach that goal in a reasonable manner. Extending build rules that work under multiple scenarios and that target different platforms is hard, especially while keeping existing functionality stable.&lt;/p&gt;

&lt;p&gt;Another reason this came to mind is that, recently, I have been toying with writing a Go Appengine app. Running the trivial “Hello World” sample is easy: type some code in &lt;code class=&quot;highlighter-rouge&quot;&gt;.go&lt;/code&gt; files, run &lt;code class=&quot;highlighter-rouge&quot;&gt;goapp serve&lt;/code&gt;, and boom: the app (re)compiles in the background and is served on the web. Good? No.&lt;/p&gt;

&lt;p&gt;Unfortunately, as soon as you start writing “the real stuff”, you end up requiring features not provided by the simplistic and basic toolchain. You may need to fetch dependencies depending on the developer’s needs; you may need to build auxiliary tools to set up the production environment; you may need to run those tools with specific flags for your project and for the developer machine; you may need to interact with the VCS system to install hooks and get those working; etc. All these require some form of plumbing, which I chose to do via &lt;code class=&quot;highlighter-rouge&quot;&gt;make(1)&lt;/code&gt; and a hand-tuned &lt;code class=&quot;highlighter-rouge&quot;&gt;configure&lt;/code&gt; script—purely out of routine, not because these are great tools—and they have grown more than originally expected.&lt;/p&gt;

&lt;h1 id=&quot;writing-build-files-is-hard&quot;&gt;Writing build files is… hard&lt;/h1&gt;

&lt;p&gt;Build systems are complex beasts and the average developer should not have to—or, in fact, does not want to—care about them. Regardless, most developers &lt;em&gt;have to&lt;/em&gt; end up writing build rules for the software they write (who else would?) and they do so without understanding the fundamentals of the tools nor the implications of their code.&lt;/p&gt;

&lt;p&gt;Take, for example, &lt;code class=&quot;highlighter-rouge&quot;&gt;configure.ac&lt;/code&gt; scripts. GNU Autoconf has &lt;a href=&quot;http://www.gnu.org/software/autoconf/manual/index.html&quot;&gt;excellent documentation&lt;/a&gt;—and reading through it is a humbling exercise, &lt;em&gt;especially&lt;/em&gt; if you are ever so tempted to write your own “simplified” configuration tool. (Spoiler: &lt;em&gt;don’t&lt;/em&gt;.)&lt;/p&gt;

&lt;p&gt;Anyway: the vast majority of (open source) projects end up requiring a &lt;code class=&quot;highlighter-rouge&quot;&gt;configure.ac&lt;/code&gt; script… but the developers, understandably, won’t go through the hassle of learning all the details of Autoconf because writing a configuration script is not their goal: it’s just boilerplate. The end result is that the developers copy/paste other people’s scripts and/or snippets from shabby sources. Repeat this copy/paste process over a handful of projects and you end up with a Frankenstein-like configuration script that can barely stand on its own. Yet, it &lt;em&gt;seems&lt;/em&gt; to work on the developer’s machine, and therefore gets shipped to the world… only to cause pain down the road.&lt;/p&gt;

&lt;p&gt;The same applies to any script used to maintain the project. Think of the auxiliary scripts that invoke continuous tests under Travis, the scripts to sanitize your source tree, the scripts to package your software, or even your installation script. Because these are not part of the project’s “core functionality”, they are often thought as unimportant and are just hacked away. Just keep in mind that those scripts &lt;em&gt;will&lt;/em&gt; fail due to their poor quality, and it’s no fun at all when they do so at, say, release time.&lt;/p&gt;

&lt;h1 id=&quot;stick-to-conventions&quot;&gt;Stick to conventions&lt;/h1&gt;

&lt;p&gt;So how do you minimize the danger of hurting yourself? &lt;em&gt;Do not reinvent the wheel.&lt;/em&gt; &lt;strong&gt;Stick to existing tools and conventions.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you think your project is special enough to warrant a hand-tuned build system, think again. Writing your own build system is a recipe for disaster: first, because you are throwing away &lt;em&gt;the collective&lt;/em&gt; wisdom of the people that wrote the existing tools; and, second, because you immediately make your package unbuildable out-of-the-box by any existing packaging system.&lt;/p&gt;

&lt;p&gt;Let’s see some examples on the lots of conventions that exist: support for environment variables (&lt;code class=&quot;highlighter-rouge&quot;&gt;CC&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;CFLAGS&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;CPPFLAGS&lt;/code&gt;, et. al.) and how they are later passed to the compiler, the linker, and the preprocessor; specific build targets that should exist (&lt;code class=&quot;highlighter-rouge&quot;&gt;all&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;install&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;check&lt;/code&gt;, and the myriad required by the widespread &lt;a href=&quot;https://www.gnu.org/prep/standards/standards.html#Makefile-Conventions&quot;&gt;GNU Coding Standards&lt;/a&gt;); the tools that you should end up invoking (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;install&lt;/code&gt; instead of plain &lt;code class=&quot;highlighter-rouge&quot;&gt;cp&lt;/code&gt; to put your files in the destination location); the hierarchy of the installed files and the ability to tune the layout depending on the platform; support for build directories separate from the source tree; support for cross-compilation; support for dynamic detection of compiler and system features; and a long etc.&lt;/p&gt;

&lt;p&gt;Note that the examples above only scratch the surface: I focused exclusively on what your standard software package for a Unix system would need, and nothing else. Take an interpreted language instead of C or C++ and, suddenly, the conventions are all different.&lt;/p&gt;

&lt;p&gt;The problem here is that users of your package &lt;em&gt;expect&lt;/em&gt; all these things to “just work” based on established conventions. If you change those conventions intentionally or just out of knowledge, your package will not behave as the user expects. And if that’s the case, can you guess what the user will think? &lt;em&gt;Your software is broken.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;which-conventions&quot;&gt;Which conventions?&lt;/h1&gt;

&lt;p&gt;OK, so we need to follow conventions. Which ones? As always, it depends. It depends on the project you are writing, it depends on your target environment, and it depends on who your users will be.&lt;/p&gt;

&lt;p&gt;There are dozens of build systems and each has gained a specific niche so you need to stick to your niche’s preferred set of tools. To name a few: if you are writing a low-level system package, or your package uses C/C++, use the GNU Autotools; if you are writing a Python package, stick to &lt;code class=&quot;highlighter-rouge&quot;&gt;distutils&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;setuptools&lt;/code&gt; and PyPI; if you are writing a KDE component, use CMake; if you are writing an Android app, choose Android Studio and Gradle; if you want to go fancier and the Java requirement is not a concern, consider Bazel (really, it’s cool!).&lt;/p&gt;

&lt;p&gt;You may or may not like the conventions of the tools used in the area you are working on, but you are better off by following the conventions than going against them: you’ll waste fewer time and your users will be happier. E.g. personally, I do not like how the standard way of using the Autotools is by shipping massive generated scripts, or the “strange” behavior of Python’s &lt;code class=&quot;highlighter-rouge&quot;&gt;setup.py&lt;/code&gt; scripts… but I just use these in my software because they are what people—and, more importantly, packagers—expect.&lt;/p&gt;

&lt;p&gt;This doesn’t mean the tools we have today are good; in fact, many aren’t. But if you spend a few minutes to learn about the shortcomings of your tools and how to make better use of them, the results will be palatable. Give the same care you give code to the build files of &lt;em&gt;your&lt;/em&gt; project.&lt;/p&gt;

&lt;p&gt;To conclude, follow onto the excellent-but-extremely-long article titled &lt;a href=&quot;https://medium.com/@sdboyer/so-you-want-to-write-a-package-manager-4ae9c17d9527&quot;&gt;So you want to write a package manager&lt;/a&gt;. It does not talk about build systems per se, but it sheds light on the many complexities of managing a source tree and its dependencies, and how those should be hidden away from the developers.&lt;/p&gt;
</description>
        
        <pubDate>Wed, 02 Mar 2016 14:45:00 +0000</pubDate>
        <link>http://julio.meroh.net/2016/03/those-pesky-makefiles.html</link>
        <guid isPermaLink="true">http://julio.meroh.net/2016/03/those-pesky-makefiles.html</guid>
        
        
        <category>software</category>
        
      </item>
    
  </channel>
</rss>
