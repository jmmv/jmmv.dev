<!doctype html><html lang=en xmlns=http://www.w3.org/1999/xhtml xmlns:fb=http://ogp.me/ns/fb#><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta property="og:site_name" content="Julio Merino (jmmv.dev)"><meta property="twitter:site" content="@jmmv"><title>The costs of the i386 to x86-64 upgrade - Julio Merino (jmmv.dev)</title>
<meta property="og:title" content="The costs of the i386 to x86-64 upgrade - Julio Merino (jmmv.dev)"><meta property="twitter:title" content="The costs of the i386 to x86-64 upgrade - Julio Merino (jmmv.dev)"><meta name=description content="If you read my previous article on DOS memory models, you may have dismissed everything I wrote as &ldquo;legacy cruft from the 1990s that nobody cares about any longer&rdquo;. After all, computers have evolved from sporting 8-bit processors to 64-bit processors and, on the way, the amount of memory that these computers can leverage has grown orders of magnitude: the 8086, a 16-bit machine with a 20-bit address space, could only use 1MB of memory while today&rsquo;s 64-bit machines can theoretically access 16EB.
All of this growth has been in service of ever-growing programs. But&mldr; even if programs are now more sophisticated than they were before, do they all really require access to a 64-bit address space? Has the growth from 8 to 64 bits been a net positive in performance terms?
Let&rsquo;s try to answer those questions to find some very surprising answers. But first, some theory.
"><meta property="og:description" content="If you read my previous article on DOS memory models, you may have dismissed everything I wrote as &ldquo;legacy cruft from the 1990s that nobody cares about any longer&rdquo;. After all, computers have evolved from sporting 8-bit processors to 64-bit processors and, on the way, the amount of memory that these computers can leverage has grown orders of magnitude: the 8086, a 16-bit machine with a 20-bit address space, could only use 1MB of memory while today&rsquo;s 64-bit machines can theoretically access 16EB.
All of this growth has been in service of ever-growing programs. But&mldr; even if programs are now more sophisticated than they were before, do they all really require access to a 64-bit address space? Has the growth from 8 to 64 bits been a net positive in performance terms?
Let&rsquo;s try to answer those questions to find some very surprising answers. But first, some theory.
"><meta property="twitter:description" content="If you read my previous article on DOS memory models, you may have dismissed everything I wrote as &ldquo;legacy cruft from the 1990s that nobody cares about any longer&rdquo;. After all, computers …"><meta name=author content="Julio Merino"><meta property="twitter:creator" content="@jmmv"><meta name=generator content="Hugo 0.136.5"><meta property="og:url" content="https://jmmv.dev/2024/10/x86-64-programming-models.html"><meta property="og:type" content="blog"><meta property="twitter:card" content="summary"><link rel=canonical href=https://jmmv.dev/2024/10/x86-64-programming-models.html><link rel=alternate type=application/rss+xml title="Julio Merino (jmmv.dev)" href=/feed.xml><link rel=stylesheet href=/sass/main.min.cb91349cd93211a37e7d5dc131c35a170fc795721c03373cd05571327eea206b.css><link rel=stylesheet href=/css/chroma.css><link rel=icon type=image/png href=/images/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/images/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicons/favicon-96x96.png sizes=96x96><meta property="og:image" content="/images/2024-10-07-x86-64-cover-image.jpg"><meta property="twitter:image" content="https://jmmv.dev/images/2024-10-07-x86-64-cover-image.jpg"><script>const SITE_ID="e8da9f62-b7ac-4fe9-bf20-7c527199a376",BASE_URL="https://jmmv.dev/"</script></head><body><img src="https://hugo-dynamic.jmmv.dev/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/pages/aHR0cHM6Ly9qbW12LmRldi8yMDI0LzEwL3g4Ni02NC1wcm9ncmFtbWluZy1tb2RlbHMuaHRtbA==/stamp.gif" style=display:none>
<script>window.location.replace("https://blogsystem5.substack.com/p/x86-64-programming-models")</script><header class=site-header><nav class="navbar navbar-expand-md fixed-top navbar-dark bg-primary"><a class=navbar-brand href=/><img src=/images/favicons/favicon-30x30.png width=30 height=30 class="d-inline-block align-top" alt>
&nbsp;jmmv.dev
</a><button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/about.html>About</a></li><li class=nav-item><a class=nav-link href=/blog.html>Blog</a></li><li class=nav-item><a class=nav-link href=/resume.html>Resume</a></li><li class=nav-item><a class=nav-link href=/software.html>Software</a></li></ul><ul class="navbar-nav mr-4"><li class=nav-item><a class=nav-link href=/archive.html>Archive</a></li><li class=nav-item><a class=nav-link href=/series.html>Series</a></li><li class=nav-item><a class=nav-link href=/tags.html>Tags</a></li></ul><form class=form-inline method=get role=search action=https://www.google.com/search><div class=input-group><input type=search name=query class=form-control placeholder=Search aria-label=Search><div class=input-group-append><button type=submit value=Search class="btn btn-light">
<img src=/octicons/search.svg></button></div></div><input type=hidden name=sitesearch value=https://jmmv.dev/></form></div></nav></header><div class=page-header><div class=container><h1>The costs of the i386 to x86-64 upgrade</h1><p>October 7, 2024 &#183;
About 18 minutes
&#183;
Tags:
<a href=/tags/blogsystem5>blogsystem5</a>, <a href=/tags/hardware>hardware</a>, <a href=/tags/unix>unix</a></p><p class=post-meta-p>This article was
<a href=https://blogsystem5.substack.com/p/x86-64-programming-models>originally published
on Substack</a>
in the <a href=https://blogsystem5.substack.com/>Blog System/5 newsletter</a>
and is replicated here for archival purposes.</p><figure class=cover-image><img src=/images/2024-10-07-x86-64-cover-image.jpg style=max-width:100%></figure></div></div><div class="container post-body"><div class=row><div class=col-md-9><div class=row><div class=col><article><p>If you read my previous article on <a href=/2024/09/dos-memory-models.html>DOS memory models</a>, you may have dismissed everything I wrote as &ldquo;legacy cruft from the 1990s that nobody cares about any longer&rdquo;. After all, computers have evolved from sporting 8-bit processors to 64-bit processors and, on the way, the amount of memory that these computers can leverage has grown orders of magnitude: the 8086, a 16-bit machine with a 20-bit address space, could only use 1MB of memory while today&rsquo;s 64-bit machines can theoretically access 16EB.</p><p>All of this growth has been in service of ever-growing programs. But&mldr; even if programs are now more sophisticated than they were before, do they all <em>really</em> require access to a 64-bit address space? Has the growth from 8 to 64 bits been a net positive in performance terms?</p><p>Let&rsquo;s try to answer those questions to find some very surprising answers. But first, some theory.</p><h1 id=code-density>Code density</h1><p>An often overlooked property of machine code is <em>code density</em>, a loose metric that quantifies how many instructions are required to execute a given &ldquo;action&rdquo;. I double-quote action because an &ldquo;action&rdquo; in this context is a subjective operation that captures a programmer-defined outcome.</p><p>Suppose, for example, that you want to increment the value of a variable that resides in main memory, like so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>static</span> <span class=kt>int</span> <span class=n>a</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>a</span> <span class=o>=</span> <span class=n>a</span> <span class=o>+</span> <span class=mi>1</span><span class=p>;</span>
</span></span></code></pre></div><p>On a processor with an ISA that provides complex instructions and addressing modes, you can potentially do so in just one machine instruction:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-asm data-lang=asm><span class=line><span class=cl><span class=c1>; Take the 32-bit quantity at the address indicated by the
</span></span></span><span class=line><span class=cl><span class=c1>; 00001234h immediate, increment it by one, and store it in the
</span></span></span><span class=line><span class=cl><span class=c1>; same memory location.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nf>add</span> <span class=no>dword</span> <span class=p>[</span><span class=mi>1234</span><span class=no>h</span><span class=p>],</span> <span class=mi>1</span>
</span></span></code></pre></div><p>In contrast, on a processor with an ISA that strictly separates memory accesses from other operations, you&rsquo;d need more steps:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-asm data-lang=asm><span class=line><span class=cl><span class=nf>li</span>  <span class=no>r8</span><span class=p>,</span> <span class=mi>1234</span><span class=no>h</span>   <span class=c1>; Load the address of the variable into r8.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nf>lm</span>  <span class=no>r1</span><span class=p>,</span> <span class=no>r8</span>      <span class=c1>; Load the content of the address into r1.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nf>li</span>  <span class=no>r2</span><span class=p>,</span> <span class=mi>1</span>       <span class=c1>; Load the immediate value 1 into r2.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nf>add</span> <span class=no>r3</span><span class=p>,</span> <span class=no>r1</span><span class=p>,</span> <span class=no>r2</span>  <span class=c1>; r3 = r1 + r2
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nf>sm</span>  <span class=no>r8</span><span class=p>,</span> <span class=no>r3</span>      <span class=c1>; Store the result in r3 into the address in r8.
</span></span></span></code></pre></div><p>Which ISA is better, you ask? Well, as usual, there are pros and cons to each approach:</p><ul><li><p>The former type of ISA offers a denser representation of the operation so it can fit more instructions in less memory. But, because of the semantic complexity of each individual instruction, the processor has to do more work to execute them (possibly requiring more transistors and drawing more power). This type of ISA is usually known as <a href=https://en.wikipedia.org/wiki/Complex_instruction_set_computer>CISC</a>.</p></li><li><p>The latter type of ISA offers a much more verbose representation of the operation but the instructions that the processor executes are simpler and can likely be executed faster. This type of ISA is usually known as <a href=https://en.wikipedia.org/wiki/Reduced_instruction_set_computer>RISC</a>.</p></li></ul><p>This is the eternal debate between CISC and RISC but note that the distinction between the two is not crystal clear: some RISC architectures support <em>more</em> instructions that CISC architectures, and contemporary Intel processors translate their CISC-style instructions into internal RISC-style <a href=https://en.wikipedia.org/wiki/Intel_microcode>micro-operations</a> immediately after decoding the former.</p><h1 id=its-the-bytes-that-matter>It&rsquo;s the bytes that matter</h1><p>Code density is not about instruction <em>count</em> though. Code density is about the <em>aggregate size</em>, in bytes, of the instructions required to accomplish a specific outcome.</p><p>In the example I presented above, the single CISC-style <code>mov</code> x86 instruction is encoded using anywhere from 2 to 8 bytes depending on its arguments, whereas the fictitious RISC-style instructions typically take 4 bytes each. Therefore, the CISC-style snippet would take 8 bytes total whereas the RISC-style snippet would take 20 bytes total, while achieving the same conceptual result.</p><p>Is that increase in encoded bytes bad? Not necessarily because &ldquo;bad&rdquo; is subjective and we are talking about trade-offs here, but it&rsquo;s definitely an important consideration due to its impact in various dimensions:</p><ul><li><p><strong>Memory usage</strong>: Larger programs take more memory. You might say that memory is plentiful these days, and that&rsquo;s true, but it wasn&rsquo;t always the case: when computers could only address 1 MB of RAM or less, the size of a program&rsquo;s binary mattered a lot. Furthermore, even today, memory <em>bandwidth</em> is limited, and this is an often-overlooked property of a system.</p></li><li><p><strong>Disk space</strong>: Larger programs take more disk space. You might say that disk space is plentiful these days too and that program code takes a tiny proportion of the overall disk: most space goes towards storing media anyway. And that&rsquo;s true, but I/O bandwidth is not as unlimited as disk space, and loading these programs from disk isn&rsquo;t cheap. Have you noticed how utterly slow is it to open an Electron-based app?</p></li><li><p><strong>L1 thrashing</strong>: Larger or more instructions mean that you can fit fewer of them in the L1 instruction cache. Proper utilization of this cache is critical to achieve reasonable levels of computing performance, and even though main memory size can now be measured in terabytes, the L1 cache is still measured in kilobytes.</p></li></ul><p>We could beat the dead horse over CISC vs. RISC further but that&rsquo;s not what I want to focus on. What I want to focus on is one very specific dimension of the instruction encoding that affects code density in all cases. And that&rsquo;s the <em>size of the addresses</em> (pointers) required to reference program code or data.</p><p>Simply put, the larger the size of the addresses in the program&rsquo;s code, the lower the code density. The lower the code density, the more memory and disk space we need. And the more memory code takes, the more L1 thrashing we will see.</p><p>So, for good performance, we probably want to optimize the way we represent addresses in the code and take advantage of surrounding context. For example: if we have a tight loop like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>static</span> <span class=kt>int</span> <span class=n>a</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>100</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>a</span> <span class=o>=</span> <span class=n>a</span> <span class=o>+</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The corresponding assembly code may be similar to this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-asm data-lang=asm><span class=line><span class=cl>        <span class=nf>mov</span> <span class=no>ecx</span><span class=p>,</span> <span class=mi>0</span>             <span class=c1>; i = 0.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nl>repeat:</span> <span class=nf>cmp</span> <span class=no>ecx</span><span class=p>,</span> <span class=mi>100</span>           <span class=c1>; i &lt; 100?
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>je</span>  <span class=no>out</span>                <span class=c1>; If i == 100, jump to out.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>add</span> <span class=no>dword</span> <span class=p>[</span><span class=mi>01234</span><span class=no>h</span><span class=p>],</span> <span class=mi>1</span>  <span class=c1>; Increment a.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>add</span> <span class=no>ecx</span><span class=p>,</span> <span class=mi>1</span>             <span class=c1>; i++.
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>jmp</span> <span class=no>repeat</span>             <span class=c1>; Jump back to repeat.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nl>out:</span>
</span></span></code></pre></div><p>Now the questions are:</p><ul><li><p>Given that <code>repeat</code> and <code>out</code> are labels for memory addresses, how do we encode the <code>je</code> (jump if equal) and <code>jmp</code> (unconditional jump) instructions as bytes? On a 32-bit machine, do we encode the full 32-bit addresses (4 bytes each) in each instruction, or do we use 1-byte relative short pointers because the jump targets are within a 127-byte distance on either side of the jump instruction?</p></li><li><p>How do we represent the address of the <code>a</code> variable? Do we express it as an absolute address or as a relative one? Do we store <code>1234h</code> as a 32-bit quantity or do we use fewer bytes because this specific number is small?</p></li><li><p>How big should <code>int</code> be? The <code>dword</code> above implies 32 bits but&mldr; that&rsquo;s just an assumption I made when writing the snippet. <code>int</code> could have been 16 bits, or 64 bits, or anything else you can imagine (which is a design mistake in C, if you ask me).</p></li></ul><p>The answers to the questions above are <em>choices</em>: we can decide whichever encoding we want for code and data addresses, and these choices have consequences on code density. This was true many years ago during the DOS days, which is why we had short, near, and far pointer types, and <a href=https://web.eece.maine.edu/~vweaver/papers/iccd09/iccd09_density.pdf>is still true today</a> as we shall see.</p><h1 id=the-switch-to-32-bits>The switch to 32 bits</h1><p>It is no secret that programming 16-bit machines was limiting, and we can identify at least two reasons to back this claim.</p><p>First, 16-bit machines usually had limited address spaces. It&rsquo;s important to understand that a processor&rsquo;s native register size has no direct connection to the size of the addresses the processor can reference. In theory, these machines could have leveraged larger chunks of memory and, in fact, the 16-bit 8086 proved this to be true with its 20-bit address space. But the point is that, <em>in the era</em> of 16-bit machines, 1 MB of RAM was considered &ldquo;a lot&rdquo; and so machines didn&rsquo;t have much memory.</p><p>Second, 16-bit machines can only operate on 16-bit quantities at a time, and 16 bits only allow representing integers with limited ranges: a signed number between -32K and +32K isn&rsquo;t&mldr; particularly large. Like before, it&rsquo;s important to clarify that the processor&rsquo;s native register size does not impose restrictions on the size of the numbers the processor can manipulate, but it does add limitations on how <em>fast</em> it can do so: operating 32-bit numbers on 16-bit machines requires at least twice the number of instructions for each operation.</p><p>So, when 32-bit processors entered the scene, it was pretty much a no-brainer that programs had to become 32-bit by default: all memory addresses and default integer types grew to 32 bits. This allowed programmers to not worry about memory limits for a while: 32 bits can address 4GB of memory, which was a huge amount back then and should <em>still</em> be huge if it wasn&rsquo;t due to bloated software. Also, this upgrade allowed programmers to efficiently manipulate integers with large-enough ranges for most operations.</p><p>In technical mumbo-jumbo, what happened here was that C adopted the ILP32 programming model: integers (I), longs (L), and pointers (P) all became 32 bits, whereas chars and shorts remained 8-bit and 16-bit respectively.</p><p>It didn&rsquo;t have to be this way though: if we look at the model for 16-bit programming, shorts and integers were 16-bit whereas longs were 32-bit, so why did integers change size but longs remained the same as integers? I do not have an answer for this, but if longs had become 64 bits back then, maybe we wouldn&rsquo;t be in the situation today where <a href=https://en.wikipedia.org/wiki/Year_2038_problem>2038 will bring mayhem to Unix systems</a>.</p><h1 id=evolving-towards-x86-64>Evolving towards x86-64</h1><p>4GB of RAM were a lot when 32-bit processors launched but slowly became insufficient as software kept growing. To support those needs, there were crutches like <a href=https://en.wikipedia.org/wiki/Physical_Address_Extension>Intel&rsquo;s PAE</a>, which allowed manipulating up to 64GB of RAM on 32-bit machines without changing the programming model, but they were just that: hacks.</p><p>The thing is: it&rsquo;s not only software that grew. It&rsquo;s the <em>kinds of things</em> that people wanted to do with software that changed: people wanted to edit high-resolution photos and video as well as play more-realistic games, and writing code to achieve those goals on a limited 4GB address space was possible but not <em>convenient</em>. With 64-bit processors, <code>mmap</code>ing huge files made those programs easier to write, and using native 64-bit integers made them faster too. So 64-bit machines became mainstream sometime around the introduction of the Athlon 64 processor and the Power Mac G5, both in 2003.</p><p>So what happened to the programming model? ILP32 was a no-brainer for 32-bit machines, but were LP64 or ILP64 no-brainers for 64-bit machines? These 64-bit models were definitely tempting because they allowed the programmer to leverage the machine&rsquo;s resources freely. Larger pointers allowed addressing &ldquo;unlimited&rdquo; memory transparently, and a larger long type naturally bumped file offsets (<code>ino_t</code>), timestamps (<code>time_t</code>), array lengths (<code>size_t</code>), and more to 64 bits as well. Without a lot of work from programmers, programs could &ldquo;just do more&rdquo; by simply recompiling them.</p><p>But there was a downside to that choice. According to the theory I presented earlier, LP64 would make programs bigger and would decrease code density when compared to ILP32. And lower code density could lead to L1 instruction cache thrashing, which is an important consideration to this day because a modern Intel Core i7-14700K from 2023 has just 80 KB of L1 cache and an Apple Silicon M3 from 2023 has less than 200KB. (These numbers are&mldr; <em>not big</em> when you stack them up against the multi-GB binaries that comprise modern programs, are they?)</p><p>We now know that LP64 was the preferred choice and that it became the default programming model for 64-bit operating systems, which means we can compare its impact against ILP32. So what are the consequences? Let&rsquo;s take a look.</p><figure><img src=/images/2024-10-07-x86-64-iso-images.png class=with-border alt="Comparison of ISO image sizes for various operating systems"></figure><p>Wait, what? The FreeBSD x86-64 installation image is definitely larger than the i386 one&mldr; but all other images are <em>smaller</em>? What&rsquo;s going on here? This contradicts everything I said above!</p><h1 id=down-the-rabbit-hole>Down the rabbit hole</h1><p>I was genuinely surprised by this and I had to dig a bit. Cracking open the FreeBSD bootonly image revealed some differences in the kernel (slightly bigger binaries, but different sets of modules) which made it difficult to compare the two. But looking into the Debian netinst images, I did find that almost all i386 binaries were larger than their x86-64 counterparts.</p><p>To try to understand why that was, the first thing I did was compile a simple hello-world program on my x86-64 Debian VM, targeting both 64-bit and 32-bit binaries:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plain data-lang=plain><span class=line><span class=cl>$ gcc-14 -o hello64 hello.c
</span></span><span class=line><span class=cl>$ i686-linux-gnu-gcc-14 -o hello32 hello.c
</span></span><span class=line><span class=cl>$ file hello32 hello64
</span></span><span class=line><span class=cl>hello32: ELF 32-bit LSB pie executable, Intel 80386, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux.so.2, for GNU/Linux 3.2.0, not stripped
</span></span><span class=line><span class=cl>hello64: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=f1bf851d7f1d56ae5d50eb136793066f67607e06, for GNU/Linux 3.2.0, not stripped
</span></span><span class=line><span class=cl>$ ls -l hello32 hello64
</span></span><span class=line><span class=cl>-rwxrwxr-x 1 jmmv jmmv 15040 Oct  4 17:45 hello32
</span></span><span class=line><span class=cl>-rwxrwxr-x 1 jmmv jmmv 15952 Oct  4 17:45 hello64
</span></span><span class=line><span class=cl>$ █
</span></span></code></pre></div><p>Based on this, it <em>looks</em> as if 32-bit binaries are indeed smaller than 64-bit binaries. But a &ldquo;hello world&rdquo; program is trivial and not worth 15kb of code: those 15kb shown above are definitely <em>not</em> code. They are probably mostly ELF overhead. Indeed, if we look at just the text portion of the binaries:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plain data-lang=plain><span class=line><span class=cl>$ objdump -h hello32 | grep text
</span></span><span class=line><span class=cl> 14 .text         00000169  00001060  00001060  00001060  2**4
</span></span><span class=line><span class=cl>$ objdump -h hello64 | grep text
</span></span><span class=line><span class=cl> 14 .text         00000103  0000000000001050  0000000000001050  00001050  2**4
</span></span><span class=line><span class=cl>$ █
</span></span></code></pre></div><p>&mldr; we find that <code>hello32</code>&rsquo;s text is 169h bytes whereas <code>hello64</code>&rsquo;s text is 103h bytes. And if we disassemble the two:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plain data-lang=plain><span class=line><span class=cl>$ objdump --disassemble=main hello32
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>00001189 &lt;main&gt;:
</span></span><span class=line><span class=cl>    1189:       8d 4c 24 04             lea    0x4(%esp),%ecx
</span></span><span class=line><span class=cl>    118d:       83 e4 f0                and    $0xfffffff0,%esp
</span></span><span class=line><span class=cl>    1190:       ff 71 fc                push   -0x4(%ecx)
</span></span><span class=line><span class=cl>    1193:       55                      push   %ebp
</span></span><span class=line><span class=cl>    1194:       89 e5                   mov    %esp,%ebp
</span></span><span class=line><span class=cl>    1196:       53                      push   %ebx
</span></span><span class=line><span class=cl>    1197:       51                      push   %ecx
</span></span><span class=line><span class=cl>    1198:       e8 28 00 00 00          call   11c5 &lt;__x86.get_pc_thunk.ax&gt;
</span></span><span class=line><span class=cl>    119d:       05 57 2e 00 00          add    $0x2e57,%eax
</span></span><span class=line><span class=cl>    11a2:       83 ec 0c                sub    $0xc,%esp
</span></span><span class=line><span class=cl>    11a5:       8d 90 14 e0 ff ff       lea    -0x1fec(%eax),%edx
</span></span><span class=line><span class=cl>    11ab:       52                      push   %edx
</span></span><span class=line><span class=cl>    11ac:       89 c3                   mov    %eax,%ebx
</span></span><span class=line><span class=cl>    11ae:       e8 8d fe ff ff          call   1040 &lt;puts@plt&gt;
</span></span><span class=line><span class=cl>    11b3:       83 c4 10                add    $0x10,%esp
</span></span><span class=line><span class=cl>    11b6:       b8 00 00 00 00          mov    $0x0,%eax
</span></span><span class=line><span class=cl>    11bb:       8d 65 f8                lea    -0x8(%ebp),%esp
</span></span><span class=line><span class=cl>    11be:       59                      pop    %ecx
</span></span><span class=line><span class=cl>    11bf:       5b                      pop    %ebx
</span></span><span class=line><span class=cl>    11c0:       5d                      pop    %ebp
</span></span><span class=line><span class=cl>    11c1:       8d 61 fc                lea    -0x4(%ecx),%esp
</span></span><span class=line><span class=cl>    11c4:       c3                      ret
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>$ objdump --disassemble=main hello64
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>0000000000001139 &lt;main&gt;:
</span></span><span class=line><span class=cl>    1139:       55                      push   %rbp
</span></span><span class=line><span class=cl>    113a:       48 89 e5                mov    %rsp,%rbp
</span></span><span class=line><span class=cl>    113d:       48 8d 05 c0 0e 00 00    lea    0xec0(%rip),%rax        # 2004 &lt;_IO_stdin_used+0x4&gt;
</span></span><span class=line><span class=cl>    1144:       48 89 c7                mov    %rax,%rdi
</span></span><span class=line><span class=cl>    1147:       e8 e4 fe ff ff          call   1030 &lt;puts@plt&gt;
</span></span><span class=line><span class=cl>    114c:       b8 00 00 00 00          mov    $0x0,%eax
</span></span><span class=line><span class=cl>    1151:       5d                      pop    %rbp
</span></span><span class=line><span class=cl>    1152:       c3                      ret
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>$ █
</span></span></code></pre></div><p>We observe massive differences in the machine code generated for the trivial <code>main</code> function. The 64-bit code is definitely <em>smaller</em> than the 32-bit code, contrary to my expectations. But the code is also <em>very</em> different; so different, in fact, that ILP32 vs. LP64 doesn&rsquo;t explain it.</p><p>The first difference we can observe is around calling conventions. The i386 architecture has a limited number of registers, favors passing arguments via the stack, and only 3 registers can be clobbered within a function. x86-64, on the other hand, prefers passing arguments through registers as much as possible and defines 7 registers as volatile.</p><p>The second difference is that we don&rsquo;t see 64-bit addresses anywhere in the code above. Jump addresses are encoded using near pointers, and data addresses are specified as offsets over a 64-bit base previously stored in a register. I found it smart that those addresses are relative to the program counter (the RIP register).</p><p>There may be more differences, but these two alone seem to be the reason why 64-bit binaries end up being more compact than 32-bit ones. <em>On Intel x86</em>, that is. You see: Intel x86&rsquo;s instruction set is so versatile that the compiler and the ABI can play tricks to hide the cost of 64-bit pointers.</p><p>Is that true of more RISC-y 64-bit architectures though? I installed the PowerPC 32-bit and 64-bit toolchains and ran the same test. And guess what? The PowerPC 64-bit binary was indeed larger than the 32-bit one, so <em>maybe</em> it&rsquo;s true. Unfortunately, running a broader comparison than this is difficult: there is no full operating system I can find that ships both builds any longer, and ARM images can&rsquo;t easily be compared.</p><h1 id=its-all-about-the-data>It&rsquo;s all about the data</h1><p>OK, fine, we&rsquo;ve settled that 64-bit <em>code</em> isn&rsquo;t necessarily larger than 32-bit code, at least on Intel, and thus any adverse impact on the L1 instruction cache is probably negligible. But&mldr; what about <em>data density</em>?</p><p>Pointers don&rsquo;t only exist in instructions or as jump targets. They also exist within the most modest of data types: lists, trees, graphs&mldr; all contain pointers in them. And in those, unless the programmer explicitly plays complex <a href=https://v8.dev/blog/pointer-compression>tricks to compress pointers</a>, we&rsquo;ll usually end up with larger data structures by simply jumping to LP64. The same applies to the innocent-looking <code>long</code> type by the way, which appears throughout codebases and also grows with this model.</p><p>And <em>this</em>&mdash;a decrease in data density&mdash;is where the real performance penalty comes from: it&rsquo;s not so much about the code size but about the data size.</p><p>Let&rsquo;s take a look. I wrote a simple program that creates a linked list of integers with 10 million nodes and then iterates over them in sequence. Each node is 8 bytes in 32-bit mode (4 bytes for the <code>int</code> and 4 bytes for the <code>next</code> pointer), whereas it is 16 bytes in 64-bit mode (4 bytes for the <code>int</code>, <em>4 bytes of padding</em>, and 8 bytes for the <code>next</code> pointer). I then compiled that program in 32-bit and 64-bit mode, measured it, and ran it:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plain data-lang=plain><span class=line><span class=cl>$ gcc -o list64 list.c
</span></span><span class=line><span class=cl>$ i686-linux-gnu-gcc-14 -o list32 list.c
</span></span><span class=line><span class=cl>$ objdump -h list32 | grep text
</span></span><span class=line><span class=cl> 13 .text         000001f6  00001070  00001070  00001070  2**4
</span></span><span class=line><span class=cl>$ objdump -h list64 | grep text
</span></span><span class=line><span class=cl> 14 .text         000001b0  0000000000001060  0000000000001060  00001060  2**4
</span></span><span class=line><span class=cl>$ hyperfine --warmup 1 ./list32 ./list64
</span></span><span class=line><span class=cl>Benchmark 1: ./list32
</span></span><span class=line><span class=cl>  Time (mean ± σ):     394.2 ms ±   2.1 ms    [User: 311.4 ms, System: 83.1 ms]
</span></span><span class=line><span class=cl>  Range (min … max):   392.1 ms … 398.2 ms    10 runs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Benchmark 2: ./list64
</span></span><span class=line><span class=cl>  Time (mean ± σ):     502.4 ms ±   4.5 ms    [User: 334.9 ms, System: 167.8 ms]
</span></span><span class=line><span class=cl>  Range (min … max):   494.9 ms … 509.5 ms    10 runs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Summary
</span></span><span class=line><span class=cl>  ./list32 ran
</span></span><span class=line><span class=cl>    1.27 ± 0.01 times faster than ./list64
</span></span><span class=line><span class=cl>$ █
</span></span></code></pre></div><p>As before with the hello-world comparison, this simple microbenchmark&rsquo;s 32-bit code continues to be slightly larger than its 64-bit counterpart (1F6h bytes vs 1B0h). However, its runtime is <em>27% faster</em>, and it is no wonder because the doubling of the linked list node size implies doubling the memory usage and thus the halving of cache hits. We can confirm the impact of this using the <code>perf</code> tool:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plain data-lang=plain><span class=line><span class=cl>$ perf stat -B -e cache-misses ./list32 2&gt;&amp;1 | grep cpu_core
</span></span><span class=line><span class=cl>         2,400,339      cpu_core/cache-misses:u/      (99.33%)
</span></span><span class=line><span class=cl>$ perf stat -B -e cache-misses ./list64 2&gt;&amp;1 | grep cpu_core
</span></span><span class=line><span class=cl>         4,687,156      cpu_core/cache-misses:u/      (99.34%)
</span></span><span class=line><span class=cl>$ █
</span></span></code></pre></div><p>The 64-bit build of this microbenchmark incurs almost double the cache misses than the 32-bit build.</p><p>Of course, this is just a microbenchmark, and tweaking it slightly will make it show very different results and make it say whatever we want. I tried to add jitter to the memory allocations so that the nodes didn&rsquo;t end up as consecutive in memory, and then the 64-bit version executed faster. I suspect this is due to the memory allocator having a harder time handling memory when the address space is limited.</p><p>The impact on real world applications is harder to quantify. It is difficult to find the same program built in 32-bit and 64-bit mode and to run it on the same kernel. It is even more difficult to find one such program where the difference matters. But at the end of the day, the differences exist, and I bet they are more meaningful than we might think in terms of bloat&mdash;but I did not intend to write a research paper here, so I&rsquo;ll leave that investigation to someone else&mldr; or another day.</p><h1 id=the-x32-abi>The x32 ABI</h1><p>There is one last thing to discuss before we depart. While we find ourselves using the LP64 programming model on x86-64 processors, remember that this was a choice and there were and are other options on the table.</p><p>Consider this: we could have made the operating system kernel leverage 64 bits to gain access to a humongous address space, but we could have kept the user-space programming model as it was before&mdash;that is, we could have kept ILP32. And we could have gone even further and optimized the calling conventions to reduce the binary code size by leveraging the additional general-purpose registers that x86-64 provides.</p><p>And in fact, this exists and is known as <a href=https://en.wikipedia.org/wiki/X32_ABI>x32</a>.</p><p>We can install the x32 toolchain and see that it effectively works as we&rsquo;d imagine:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plain data-lang=plain><span class=line><span class=cl>$ gcc -o hello64 hello.c
</span></span><span class=line><span class=cl>$ x86_64-linux-gnux32-gcc-14 -o hellox32 hello.c
</span></span><span class=line><span class=cl>$ objdump --disassemble=main hello64
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>0000000000001139 &lt;main&gt;:
</span></span><span class=line><span class=cl>    1139:       55                      push   %rbp
</span></span><span class=line><span class=cl>    113a:       48 89 e5                mov    %rsp,%rbp
</span></span><span class=line><span class=cl>    113d:       48 8d 05 c0 0e 00 00    lea    0xec0(%rip),%rax        # 2004 &lt;_IO_stdin_used+0x4&gt;
</span></span><span class=line><span class=cl>    1144:       48 89 c7                mov    %rax,%rdi
</span></span><span class=line><span class=cl>    1147:       e8 e4 fe ff ff          call   1030 &lt;puts@plt&gt;
</span></span><span class=line><span class=cl>    114c:       b8 00 00 00 00          mov    $0x0,%eax
</span></span><span class=line><span class=cl>    1151:       5d                      pop    %rbp
</span></span><span class=line><span class=cl>    1152:       c3                      ret
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>$ objdump --disassemble=main hellox32
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>00401126 &lt;main&gt;:
</span></span><span class=line><span class=cl>  401126:       55                      push   %rbp
</span></span><span class=line><span class=cl>  401127:       89 e5                   mov    %esp,%ebp
</span></span><span class=line><span class=cl>  401129:       b8 04 20 40 00          mov    $0x402004,%eax
</span></span><span class=line><span class=cl>  40112e:       89 c0                   mov    %eax,%eax
</span></span><span class=line><span class=cl>  401130:       48 89 c7                mov    %rax,%rdi
</span></span><span class=line><span class=cl>  401133:       e8 f8 fe ff ff          call   401030 &lt;puts@plt&gt;
</span></span><span class=line><span class=cl>  401138:       b8 00 00 00 00          mov    $0x0,%eax
</span></span><span class=line><span class=cl>  40113d:       5d                      pop    %rbp
</span></span><span class=line><span class=cl>  40113e:       c3                      ret
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>$ █
</span></span></code></pre></div><p>Now, the <code>main</code> method of our hello-world program is really similar between the 64-bit and 32-bit builds, but pay close attention to the x32 version: it uses the same calling conventions as x86-64, but it contains a mixture of 32-bit and 64-bit registers, delivering a more compact binary size.</p><p>Unfortunately:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plain data-lang=plain><span class=line><span class=cl>$ ./hellox32
</span></span><span class=line><span class=cl>zsh: exec format error: ./hellox32
</span></span><span class=line><span class=cl>$ █
</span></span></code></pre></div><p>We can&rsquo;t run the resulting binary. x32 is an ABI that impacts the kernel interface too, so these binaries cannot be executed on a regular x86-64 kernel. Sadly, and as far as I can tell, x32 is pretty much abandonware today. Gentoo claims to support it but there are no official builds of any distribution I can find that are built in x32 mode.</p><hr><p>In the end, even though LP64 <a href=https://unix.org/version2/whatsnew/lp64_wp.html>wasn&rsquo;t the obvious choice</a> for x86-64, it&rsquo;s the compilation mode that won and stuck.</p></article></div></div><div class="container post-links"><div class=row><div class="col mr-auto text-left"><span><a href=https://jmmv.dev/2024/09/dos-memory-models.html>&#171; Previous</a></span></div><div class="col text-center"><span><a href=/archive.html>All posts</a></span></div><div class="col ml-auto text-right"><span><a href=https://jmmv.dev/2024/10/bazelcon-2024-recap.html>Next &#187;</a></span></div></div></div><div class="container post-votes"><div class=row><div class="col-lg-4 my-2"><div class=row><div class=col><a onclick=voteThumbsUp() class="btn btn-block btn-outline-success" id=thumbs-up-btn>👍
(<span id=thumbs-up-count>0</span>)</a></div><div class=col><a onclick=voteThumbsDown() class="btn btn-block btn-outline-danger" id=thumbs-down-btn>👎
(<span id=thumbs-down-count>0</span>)</a></div></div></div><div class="col-lg-8 my-2 d-none d-sm-block"><div class=row><div class="col-md-4 text-center"><a href="https://www.reddit.com/submit?title=The+costs+of+the+i386+to+x86-64+upgrade&amp;url=https%3A%2F%2Fjmmv.dev%2F2024%2F10%2Fx86-64-programming-models.html" class="btn btn-block btn-outline-primary">Share on
<img src=/images/badges/reddit.png alt=Reddit width=24 height=24></a></div><div class="col-md-4 text-center"><a href="https://news.ycombinator.com/submitlink?t=The+costs+of+the+i386+to+x86-64+upgrade&amp;u=https%3A%2F%2Fjmmv.dev%2F2024%2F10%2Fx86-64-programming-models.html" class="btn btn-block btn-outline-primary">Share on
<img src=/images/badges/ycombinator.png alt="Hacker News" width=24 height=24></a></div><div class="col-md-4 text-center"><a href="https://twitter.com/intent/tweet?status=The+costs+of+the+i386+to+x86-64+upgrade+%E2%80%94+https%3A%2F%2Fjmmv.dev%2F2024%2F10%2Fx86-64-programming-models.html+%E2%80%94+cc+%40jmmv" class="btn btn-block btn-outline-primary">Share on
<img src=/images/badges/Twitter_Social_Icon_Circle_Color.png alt=Twitter width=24 height=24></a></div></div></div></div><div class="row my-4" id=postCommentRow><div class=col><div class=media><img class=mr-3 src=/octicons/pencil.svg width=32px height=32px><div class="media-body container"><form method=post id=newPost><div class=row><div class="col my-1"><textarea class=form-control rows=1 id=postCommentContent onclick=showPostComment() placeholder="Leave a comment"></textarea></div></div><div class="row newPostControls" style=display:none><div class="col my-1 form-group"><label for=postCommentAuthor>Your name (optional):</label>
<input type=text class=form-control id=postCommentAuthor></div></div><div class="row newPostControls" style=display:none><div class="col my-1 form-group"><label for=postCommentEmail>Your email (optional):</label>
<input type=text class=form-control type=email id=postCommentEmail>
<small class="form-text text-muted">Invisible to all readers; if provided, you will receive notifications when replied to (not implemented yet)</small></div></div><div class=row id=postCommentError style=display:none><div class="col mt-1 alert-danger"><p></p></div></div><div class="row newPostControls" style=display:none><button class="col-md-3 my-1 btn btn-primary" type=submit id=submitCommentButton>Post</button>
<small class="col-md-9 my-1 text-muted">Comments are subject to moderation. This feature is experimental and is powered by <a href=/software/endtracker.html>EndTRACKER</a>. If you experience any issues, please <a href=/about.html#contact>contact me off-band</a>.</small></div></form></div></div></div></div><script>function hidePostComment(){var t,e=document.getElementById("postCommentContent");e.rows=1,e.value="",t=document.getElementsByClassName("newPostControls"),Array.prototype.forEach.call(t,function(e){e.style.display="none"})}function showPostComment(){var e,t=document.getElementById("postCommentContent");t.rows=5,e=document.getElementsByClassName("newPostControls"),Array.prototype.forEach.call(e,function(e){e.style.display=""})}const form=document.querySelector("#newPost");form.onsubmit=function(e){e.preventDefault();let t=document.getElementById("postCommentAuthor").value,n=document.getElementById("postCommentEmail").value,s=document.getElementById("postCommentContent").value;postComment(t,n,s,hidePostComment)}</script></div></div><div class="col-md-3 sidebar d-none d-md-block"><div class=row><div class="col text-center p-2"><p><a href=/about.html class=clear-link><img src=/images/avatars/20181124-snow.jpg class="rounded-circle shadow my-2" style=width:100px><br><b>Julio Merino</b><br>A blog on operating systems, programming languages, testing, build systems, my own software
projects and even personal productivity. Specifics include FreeBSD, Linux, Rust, Bazel and
EndBASIC.</a></p><div class=row><div class=col><div class=form-group><form action=https://endtracker.azurewebsites.net/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/subscribers/add method=post><input type=text name=email placeholder="Enter your email" class="form-control input-sm text-center my-1">
<button type=submit class="btn btn-primary btn-block my-1">Subscribe</button></form></div></div></div><div class="row px-2"><div class="col-sm-5 text-left"><small><span class=subscriber-count>0</span> subscribers</small></div><div class="col-sm-7 text-right"><p><a rel=me href=https://mastodon.online/@jmmv><img src=/images/badges/mastodon-logo.svg width=32px height=32px alt="Follow @jmmv on Mastodon">
</a><a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fjmmv.dev%2F&amp;screen_name=jmmv"><img src=/images/badges/Twitter_logo_blue.svg width=32px height=32px alt="Follow @jmmv on Twitter">
</a><a href=/feed.xml><img src=/images/badges/feed-icon-28x28.png alt="RSS feed"></a></p></div></div></div></div><div class=row><div class=col><h2>Featured software</h2><ul><li class=overflow-ellipsis><a href=https://www.endbasic.dev/ target=_blank>EndBASIC: Online BASIC+DOS env</a></li><li class=overflow-ellipsis><a href=https://endtracker.azurewebsites.net/ target=_blank>EndTRACKER: Services for static sites</a></li></ul></div></div><div class=row><div class=col><h2>Featured posts</h2><ul><li class=overflow-ellipsis><a href=/2023/06/fast-machines-slow-machines.html>Fast machines, slow machines</a></li><li class=overflow-ellipsis><a href=/2022/12/endbasic-0.10.html>EndBASIC 0.10: Core language, evolved</a></li><li class=overflow-ellipsis><a href=/2022/10/bye-microsoft-hi-snowflake.html>Farewell, Microsoft; hello, Snowflake!</a></li><li class=overflow-ellipsis><a href=/2022/05/rust-is-hard-but-does-it-matter.html>Rust is hard, yes, but does it matter?</a></li><li class=overflow-ellipsis><a href=/2022/04/rust-traits-and-dependency-injection.html>Rust traits and dependency injection</a></li><li class=overflow-ellipsis><a href=/2022/03/a-year-on-windows-intro.html>A year on Windows: Introduction</a></li><li class=overflow-ellipsis><a href=/2021/04/always-be-quitting.html>Always be quitting</a></li><li class=overflow-ellipsis><a href=/2021/02/google-monorepos-and-caching.html>How does Google keep build times low?</a></li><li class=overflow-ellipsis><a href=/2020/12/google-no-clean-builds.html>How does Google avoid clean builds?</a></li><li class=overflow-ellipsis><a href=/2020/12/unit-testing-a-console-app.html>Unit-testing a console app (a text editor)</a></li><li class=overflow-ellipsis><a href=/essays.html#featured>More...</a></li></ul></div></div></div></div></div><footer class=container-fluid><div class=row><div class="col-4 order-2 text-right"><p class=float-right><a href=#>Back to top</a></p></div><div class="col-8 order-1 mr-auto"><p>Copyright 2004&ndash;2023 Julio Merino</p></div></div></footer><script type=module>
  import { addAnchorsToHeaders, addElementClasses, BatchClient } from "\/js\/main.0de73782e9e0fbcb4184fb5793949b3b0d0ada16455df89320415b4dcc052f88.js";

  var batchClient = new BatchClient(SITE_ID);
  batchClient.doAll({
    put_request: true, get_comments: true, get_subscriber_count: true, get_votes: true });

  window.voteThumbsUp = function() { batchClient.voteThumbsUp(); }
  window.voteThumbsDown = function() { batchClient.voteThumbsDown(); }

  window.postComment = function(...args) {
    batchClient.postComment(...args);
  };

  addAnchorsToHeaders();
  addElementClasses();
</script></body></html>