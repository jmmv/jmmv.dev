<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Julio Merino (jmmv.dev)</title><link>https://jmmv.dev/</link><description>Recent content on Julio Merino (jmmv.dev)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor>julio@meroh.net (Julio Merino)</managingEditor><webMaster>julio@meroh.net (Julio Merino)</webMaster><copyright>Copyright 2004&#150;2025 Julio Merino</copyright><lastBuildDate>Fri, 17 Jan 2025 09:45:00 -0800</lastBuildDate><atom:link href="https://jmmv.dev/feed.xml" rel="self" type="application/rss+xml"/><item><title>Hands-on graphics without X11</title><link>https://jmmv.dev/2025/01/netbsd-graphics-wo-x11.html</link><pubDate>Fri, 17 Jan 2025 09:45:00 -0800</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2025/01/netbsd-graphics-wo-x11.html</guid><description>&lt;p>&lt;em>A crash course on direct framebuffer and keyboard access via NetBSD&amp;rsquo;s wscons&lt;/em>&lt;/p>
&lt;p>Look at these two consoles:&lt;/p>
&lt;figure>
&lt;img src="/images/2025-01-17-netbsd-vs-endbasic-console.png" />
&lt;figcaption>Side-by-side comparison of the NetBSD console right after boot vs. the EndBASIC console.&lt;/figcaption>
&lt;/figure>
&lt;p>Same colors, (almost) same font, same&amp;hellip; everything? Other than for the actual text they display, they look identical, don&amp;rsquo;t they? But the one on the right can do things that the one on the left cannot. Witness this:&lt;/p>
&lt;figure>
&lt;video width="100%" controls>
&lt;source src="/images/2025-01-17-endbasic-console.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>Demonstration of the EndBASIC hybrid console by issuing a couple of graphics primitives.&lt;/figcaption>
&lt;/figure>
&lt;p>A square? OK, meh, we had those in the DOS days with &lt;a href="https://en.wikipedia.org/wiki/Box-drawing_characters">box-drawing characters&lt;/a>. But a circle?! That&amp;rsquo;s only possible because the console on the right is a hybrid console that supports mixing the usual textual grid of a terminal with overlapping graphics.&lt;/p>
&lt;p>Now, if you have been following the development of &lt;a href="https://www.endbasic.dev/">EndBASIC&lt;/a>, this is not surprising. The defining characteristic of the EndBASIC console is that it&amp;rsquo;s hybrid as the video shows. What&amp;rsquo;s newsworthy, however, is that the EndBASIC console can now run directly on a framebuffer exposed by the kernel. No X11 nor Wayland in the picture (pun intended).&lt;/p>
&lt;p>But how? The answer lies in NetBSD&amp;rsquo;s flexible wscons framework, and this article dives into what it takes to render graphics on a standard Unix system. I&amp;rsquo;ve found this exercise exciting because, in the old days, graphics were trivial (&lt;a href="https://en.wikipedia.org/wiki/Mode_13h">mode 13h&lt;/a>, anyone?) and, for many years now, computers use framebuffer-backed textual consoles. The kernel is obviously rendering &amp;ldquo;graphics&amp;rdquo; by drawing individual letters; so why can&amp;rsquo;t you, a user of the system, do so too?&lt;/p>
&lt;div class="container action-highlight p-4 my-4 d-md-none">
&lt;div class="row text-center">
&lt;p>A blog on operating systems, programming languages, testing, build systems, my own software
projects and even personal productivity. Specifics include FreeBSD, Linux, Rust, Bazel and
EndBASIC.&lt;/p>
&lt;/div>
&lt;div class="row">
&lt;div class="col">
&lt;div class="form-group">
&lt;form action="https://endtracker.azurewebsites.net/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/subscribers/add" method="post">
&lt;input type="text" name="email"
placeholder="Enter your email"
class="form-control input-sm text-center my-1"/>
&lt;button type="submit" class="btn btn-primary btn-block my-1">Subscribe&lt;/button>
&lt;/form>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="row px-2">
&lt;div class="col col-sm-5 text-left">
&lt;small>&lt;span class="subscriber-count">0&lt;/span> subscribers&lt;/small>
&lt;/div>
&lt;div class="col col-sm-7 text-right">
&lt;p>
&lt;a rel="me" href="https://mastodon.online/@jmmv">
&lt;img src="/images/badges/mastodon-logo.svg" width="32px" height="32px" alt="Follow @jmmv on Mastodon">
&lt;/a>
&lt;a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fjmmv.dev%2F&amp;amp;screen_name=jmmv">
&lt;img src="/images/badges/Twitter_logo_blue.svg" width="32px" height="32px" alt="Follow @jmmv on Twitter">
&lt;/a>
&lt;a href="/feed.xml">&lt;img src="/images/badges/feed-icon-28x28.png" alt="RSS feed">&lt;/a>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h1 id="wscons-overview">wscons overview&lt;/h1>
&lt;p>&lt;a href="https://man.netbsd.org/wscons.4">wscons(4)&lt;/a>, or Workstation Console in its full form, is NetBSD&amp;rsquo;s framework to access the physical console attached to a computer.&lt;/p>
&lt;p>wscons abstracts the details of the hardware display and input devices so that the kernel and the user-space configuration tools can treat them all uniformly across the tens of platforms that NetBSD supports. If you use &lt;a href="https://man.netbsd.org/wsconsctl.8">wsconsctl(8)&lt;/a> on a modern amd64 laptop to control its display, you use wsconsctl on an ancient vax box to control its display too.&lt;/p>
&lt;figure>
&lt;img src="/images/2025-01-17-wsdisplay-devices.png" />
&lt;figcaption>Layered architecture of wsdisplay and its backing devices.&lt;/figcaption>
&lt;/figure>
&lt;p>The output architecture of wscons is composed of multiple devices, layered like this:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://man.netbsd.org/wsdisplay.4">wsdisplay(4)&lt;/a> sits at the top of the stack and implements the console in hardware-independent terms. The functionality at this level includes handling of VT100-like sequences, cursor positioning logic, text wrapping, scrolling decisions, etc.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Under wsdisplay sit the drivers that know how to access specific hardware devices. These include, among others: &lt;a href="https://man.netbsd.org/vga.4">vga(4)&lt;/a>, which does not do graphics at all; &lt;a href="https://man.netbsd.org/genfb.4">genfb(4)&lt;/a>, which is a generic framebuffer driver that talks to the &amp;ldquo;native&amp;rdquo; framebuffer of the system (e.g. the one configured by the EFI); and &lt;a href="https://man.netbsd.org/radeonfb.4">radeonfb(4)&lt;/a>, which implements an accelerated console on AMD cards. These drivers know how to initialize and interact with the hardware.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Under the graphical drivers sits &lt;a href="https://man.netbsd.org/vcons.4">vcons(4)&lt;/a>, the driver that implements one or more graphical consoles in terms of a grid of pixels. vcons is parameterized on &amp;ldquo;raster operations&amp;rdquo; (rasops), a set of virtual methods to perform low-level operations. An example is the &lt;code>moverows&lt;/code> method, which is used by wsdisplay to implement scrolling in the most efficient way provided by the hardware. vcons provides default (inefficient) implementations of these methods, but the upper drivers like radeonfb can provide hardware-accelerated specializations when instantiating vcons. vcons also interacts with &lt;a href="https://man.netbsd.org/wsfont.4">wsfont(4)&lt;/a> to render text to the console.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;figure>
&lt;img src="/images/2025-01-17-wskbd-devices.png" />
&lt;figcaption>Layered architecture of wskbd and its backing devices, including the optional wsmux wrapper.&lt;/figcaption>
&lt;/figure>
&lt;p>The input architecture of wscons is similar in terms of layering of devices, albeit somewhat simpler:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://man.netbsd.org/wsmux.4">wsmux(4)&lt;/a> is an optional component that multiplexes multiple input devices under a single virtual device for event extraction.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://man.netbsd.org/wsbkd.4">wskbd(4)&lt;/a> sits at the top of the stack (not accounting for wsmux) and implements generic keyboard handling. The functionality at this level includes translating keycodes to layouts, handling key input repetition, and more. wskbd exposes a stream of wsevents to user-space so that user-space can process state changes (e.g. key presses).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Under wskbd sit the device drivers that know how to deal with specific hardware devices. These include, among others: &lt;a href="https://man.netbsd.org/ukbd.4">ukbd(4)&lt;/a> for USB keyboard input and &lt;a href="https://man.netbsd.org/pckbd.4">pckbd(4)&lt;/a> for PC/AT keyboard input. These drivers wait for hardware input, generate events, and provide a map of keycodes to key symbols to the upper layer so that wskbd can operate in generic terms.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The input architecture can handle other types of devices like mice and touch panels (both via &lt;a href="https://man.netbsd.org/wsmouse.4">wsmouse(4)&lt;/a>), but I&amp;rsquo;m not going to cover those here. Just know that they sit under wsmux at the equivalent level of wskbd and produce a set of wsevents in the exact same manner as wskbd.&lt;/p>
&lt;h1 id="querying-framebuffer-properties">Querying framebuffer properties&lt;/h1>
&lt;p>As you can sense from the overview, the whole architecture under wsdisplay is geared towards video devices&amp;hellip; if it wasn&amp;rsquo;t for the vga driver: in the common case, wsdisplay is backed by a graphical framebuffer managed by vcons for text rendering, yet the user only sees a textual console. But if the kernel has direct access to the framebuffer, so should user-space too.&lt;/p>
&lt;p>The details on how to do this click if you read through the operations described in the wsdisplay manual page. In particular, you may notice the &lt;code>WSDISPLAYIO_GET_FBINFO&lt;/code> call which retrieves extended information about, you guessed it, a framebuffer display.&lt;/p>
&lt;p>Let&amp;rsquo;s try it: I wrote a trivial program to open the display device (named &lt;code>/dev/ttyE0&lt;/code> for reasons that escape me), call this function, and store the results in an &lt;code>fbinfo&lt;/code> structure:&lt;/p>
&lt;div class="src">
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/param.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/types.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/ioctl.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;dev/wscons/wsconsio.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;err.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;fcntl.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;stdlib.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;unistd.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">int main(void) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Open the main wsdisplay device.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> int fd = open(&amp;#34;/dev/ttyE0&amp;#34;, O_RDWR | O_NONBLOCK | O_EXCL);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (fd == -1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;open failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Query information about the framebuffer.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> struct wsdisplayio_fbinfo fbinfo;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (ioctl(fd, WSDISPLAYIO_GET_FBINFO, &amp;amp;fbinfo) == -1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;ioctl failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> close(fd);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> exit(EXIT_SUCCESS);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;div class="footer">
&lt;div class="filename">
&lt;a href="/src/netbsd-graphics-wo-x11/wsdisplay-fbinfo.c" type="text/plain">wsdisplay-fbinfo.c&lt;/a>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Hmm, but this program does not have any visible output, right? The code just queries the framebuffer information and does nothing with it. The reason is that the content of the &lt;code>wsdisplayio_fbinfo&lt;/code> structure is large and I didn&amp;rsquo;t want to pretty-print it myself. I thought it&amp;rsquo;d be fun to show you how to use GDB to inspect large data structures and how to script the process. Here, look:&lt;/p>
&lt;div class="src">
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">gdb -q \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -ex &amp;#39;set print pretty on&amp;#39; \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -ex &amp;#39;break exit&amp;#39; \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -ex &amp;#39;run&amp;#39; \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -ex &amp;#39;frame 1&amp;#39; \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -ex &amp;#39;print fbinfo&amp;#39; \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -ex &amp;#39;cont&amp;#39; \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -ex &amp;#39;quit&amp;#39; \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ./wsdisplay-fbinfo
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;div class="footer">
&lt;div class="filename">
&lt;a href="/src/netbsd-graphics-wo-x11/wsdisplay-fbinfo.sh" type="text/plain">wsdisplay-fbinfo.sh&lt;/a>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>This call to GDB starts the sample program shown above and automates various GDB commands to set a breakpoint, step through the program, and pretty-print the &lt;code>fbinfo&lt;/code> structure right before exiting. When we execute this command as root (which is important to get access to the &lt;code>/dev/ttyE0&lt;/code> device), we get this:&lt;/p>
&lt;figure>
&lt;img src="/images/2025-01-17-wsdisplay-fbinfo-gdb.png" />
&lt;figcaption>Content of the &lt;tt>fbinfo&lt;/tt> structure as grabbed by the sample &lt;tt>wsdisplay-fbinfo&lt;/tt> program and printed by GDB.&lt;/figcaption>
&lt;/figure>
&lt;p>Neat. We get sensible stuff from the kernel! &lt;code>fbi_width&lt;/code> is 640 and &lt;code>fbi_height&lt;/code> is 480, which matches the 640x480 resolution I have configured in my test VM.&lt;/p>
&lt;h1 id="drawing-to-the-framebuffer">Drawing to the framebuffer&lt;/h1>
&lt;p>But note these other fields in the structure printed above:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="line">&lt;span class="cl">&lt;span class="k">struct&lt;/span> &lt;span class="n">wsdisplayio_fbinfo&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">uint64_t&lt;/span> &lt;span class="n">fbi_fbsize&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">uint64_t&lt;/span> &lt;span class="n">fbi_fboffset&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// ... more fields ...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>fbi_fbsize&lt;/code> and &lt;code>fbi_fboffset&lt;/code> fields are &lt;em>begging&lt;/em> us to use &lt;code>mmap&lt;/code> to memory-map the area of the device starting at &lt;code>fbi_fboffset&lt;/code> and spanning &lt;code>fbi_fbsize&lt;/code> bytes. Presumably we can write to the framebuffer if we do this, but beforehand, we have to switch the console to &amp;ldquo;framebuffer mode&amp;rdquo; by using the &lt;code>WSDISPLAYIO_SMODE&lt;/code> (&amp;ldquo;set mode&amp;rdquo;) call. This call accepts an integer to indicate which mode to set:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>WSDISPLAYIO_MODE_EMUL&lt;/code>: Set the display to emulating (text) mode. This is the default operation mode of wsdisplay and configures the console to &amp;ldquo;emulate&amp;rdquo; a text terminal.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>WSDISPLAYIO_MODE_MAPPED&lt;/code>: Set the display to mapped (graphics) mode. This allows access to the framebuffer and allows the &lt;code>mmap&lt;/code> operation to succeed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>WSDISPLAYIO_MODE_DUMBFB&lt;/code>: Set the display to mapped (framebuffer) mode. This is similar to &lt;code>WSDISPLAYIO_MODE_MAPPED&lt;/code> and, for our purposes in the demo below, works the same. I haven&amp;rsquo;t found a concise description of how these two differ, but from my reading of the code, the &amp;ldquo;mapped&amp;rdquo; mode offers access to the framebuffer as well as device-specific control registers, whereas &amp;ldquo;dumb framebuffer&amp;rdquo; just exposes the framebuffer memory.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In any case. Once we know that we have to switch the console device to a graphical mode before mapping the framebuffer, and having access to the pixel format described in the &lt;code>fbinfo&lt;/code> structure&amp;hellip; drawing something fun is just a few byte manipulation operations away:&lt;/p>
&lt;div class="src">
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/param.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/types.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/ioctl.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/mman.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;dev/wscons/wsconsio.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;err.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;fcntl.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;stdlib.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;unistd.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">int main(void) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Open the main wsdisplay device.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> int fd = open(&amp;#34;/dev/ttyE0&amp;#34;, O_RDWR | O_NONBLOCK | O_EXCL);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (fd == -1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;open failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Query information about the framebuffer.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> struct wsdisplayio_fbinfo fbinfo;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (ioctl(fd, WSDISPLAYIO_GET_FBINFO, &amp;amp;fbinfo) == -1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;ioctl failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Ensure the framebuffer aligns with the expectations of our demo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // code below.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (fbinfo.fbi_bitsperpixel != 32)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> errx(1, &amp;#34;bitsperpixel not supported by this demo&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (fbinfo.fbi_pixeltype != WSFB_RGB)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> errx(1, &amp;#34;pixeltype not supported by this demo&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Configure the wsdisplay to enter &amp;#34;dumb framebuffer&amp;#34; mode.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> unsigned int mode = WSDISPLAYIO_MODE_DUMBFB;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (ioctl(fd, WSDISPLAYIO_SMODE, &amp;amp;mode) == -1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;ioctl failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Map the framebuffer memory. Must come after the SMODE ioctl.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> uint32_t *ptr = (uint32_t*)mmap(
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0, fbinfo.fbi_fbsize, PROT_READ | PROT_WRITE, MAP_SHARED,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> fd, fbinfo.fbi_fboffset);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (ptr == MAP_FAILED)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;mmap failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Fill the screen multiple times with pixels of different
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // colors to render a simple animation.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> size_t pixels = fbinfo.fbi_fbsize / sizeof(uint32_t);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> int off = 0;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> for (int i = 0; i &amp;lt; 100; i++) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> int r = off; int g = off; int b = off;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> for (size_t i = 0; i &amp;lt; pixels; i++) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> r = (r + 1) % 255; g = (g + 2) % 255; b = (b + 3) % 255;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ptr[i] = 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | (r &amp;lt;&amp;lt; fbinfo.fbi_subtype.fbi_rgbmasks.red_offset)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | (g &amp;lt;&amp;lt; fbinfo.fbi_subtype.fbi_rgbmasks.green_offset)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | (b &amp;lt;&amp;lt; fbinfo.fbi_subtype.fbi_rgbmasks.blue_offset);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> off += 10;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> usleep(1);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Configure the wsdisplay to enter &amp;#34;console emulation&amp;#34; mode.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // In other words: return to the console.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> mode = WSDISPLAYIO_MODE_EMUL;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (ioctl(fd, WSDISPLAYIO_SMODE, &amp;amp;mode) == -1) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;ioctl failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> close(fd);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> return EXIT_SUCCESS;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;div class="footer">
&lt;div class="filename">
&lt;a href="/src/netbsd-graphics-wo-x11/wsdisplay-draw.c" type="text/plain">wsdisplay-draw.c&lt;/a>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>And if we run this:&lt;/p>
&lt;figure>
&lt;video width="100%" controls>
&lt;source src="/images/2025-01-17-wsdisplay-draw.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>The demo &lt;tt>wsdisplay-draw&lt;/tt> program running on the NetBSD console immediately after logging in.&lt;/figcaption>
&lt;/figure>
&lt;p>Voila. We&amp;rsquo;ve got graphics without paying the X11 startup tax. Switching from the console to graphics is instantaneous, like in the good old mode 13h days.&lt;/p>
&lt;h1 id="handling-keyboard-input">Handling keyboard input&lt;/h1>
&lt;p>Rendering graphics is just half of the puzzle when writing an interactive application though. The other half is handling input. And, for that, we have to turn to the wskbd device.&lt;/p>
&lt;p>After we switch the console to mapped mode, keystrokes don&amp;rsquo;t go to &lt;code>stdin&lt;/code> anymore. We have to write code to explicitly read from an attached keyboard, and we can do this via the &lt;code>/dev/wskbd0&lt;/code> device representing the first attached keyboard.&lt;/p>
&lt;p>Once we open the keyboard device for reading, wscons sends us its own representation of events known as wsevents. We can write a trivial program to read one key press:&lt;/p>
&lt;div class="src">
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/param.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/types.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/ioctl.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;dev/wscons/wsconsio.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;err.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;fcntl.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;stdio.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;stdlib.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;unistd.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">int main(int argc, char** argv) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Open the main wskbd device.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> int fd = open(&amp;#34;/dev/wskbd0&amp;#34;, O_RDONLY);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (fd == -1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;open failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Wait for one key down press only.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> for (;;) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> struct wscons_event ev;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> int ret = read(fd, &amp;amp;ev, sizeof(ev));
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (ret == -1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;read failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (ev.type == WSCONS_EVENT_KEY_DOWN) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> printf(&amp;#34;value: %d, char &amp;#39;%c&amp;#39;\n&amp;#34;, ev.value, (char)ev.value);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> break;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> close(fd);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> return EXIT_SUCCESS;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;div class="footer">
&lt;div class="filename">
&lt;a href="/src/netbsd-graphics-wo-x11/wskbd-trivial.c" type="text/plain">wskbd-trivial.c&lt;/a>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>But&amp;hellip; if we try to run it and press a key, say &lt;code>k&lt;/code>, we might get:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl"># ./wskbd-trivial
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">value: 37, char &amp;#39;%&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Huh. We pressed &lt;code>k&lt;/code> but the character we got is &lt;code>%&lt;/code>. Not what we expected! Well, as it turns out, the &amp;ldquo;value&amp;rdquo; that wsevents report for key presses (37 in this case) is the raw keycode of the key. This is hardware-specific and needs to be translated to an actual symbol via a keymap.&lt;/p>
&lt;p>One feature of wskbd is that it exposes the keymap as configured in the kernel so there is a single source of truth for the machine. We can query a portion of it with another program:&lt;/p>
&lt;div class="src">
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/param.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/types.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;sys/ioctl.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;dev/wscons/wsconsio.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;dev/wscons/wsksymdef.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;err.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;fcntl.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;stdio.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;stdlib.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;string.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#include &amp;lt;unistd.h&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">int main(int argc, char** argv) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Open the main wskbd device.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> int fd = open(&amp;#34;/dev/wskbd0&amp;#34;, O_RDONLY);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (fd == -1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;open failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Allocate space for the biggest possible keymap.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> struct wscons_keymap map[WSKBDIO_MAXMAPLEN];
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> memset(map, 0, sizeof(struct wscons_keymap) * WSKBDIO_MAXMAPLEN);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Get the keymap from the device.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> struct wskbd_map_data data;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> data.maplen = WSKBDIO_MAXMAPLEN;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> data.map = map;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (ioctl(fd, WSKBDIO_GETMAP, &amp;amp;data) == -1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> err(1, &amp;#34;ioctl failed&amp;#34;);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Dump keymap entries.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> printf(&amp;#34;Keymap length: %u entries\n&amp;#34;, data.maplen);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> for (size_t i = 0; i &amp;lt; data.maplen; i++) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // Skip printing entries that are not for letters.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (map[i].command != KS_voidSymbol)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> continue;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> char normal = map[i].group1[0];
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> char shifted = map[i].group1[1];
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> if (normal &amp;lt; &amp;#39;a&amp;#39; || normal &amp;gt; &amp;#39;z&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> continue;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> printf(&amp;#34;Keycode %zd: &amp;#39;%c&amp;#39;, &amp;#39;%c&amp;#39;\n&amp;#34;, i, normal, shifted);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> }
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> close(fd);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> return EXIT_SUCCESS;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;div class="footer">
&lt;div class="filename">
&lt;a href="/src/netbsd-graphics-wo-x11/wskbd-map.c" type="text/plain">wskbd-map.c&lt;/a>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>And if we run it, we might get:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl"># ./wskbd-map
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keymap length: 222 entries
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 16: &amp;#39;q&amp;#39;, &amp;#39;Q&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 17: &amp;#39;w&amp;#39;, &amp;#39;W&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 18: &amp;#39;e&amp;#39;, &amp;#39;E&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 19: &amp;#39;r&amp;#39;, &amp;#39;R&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 20: &amp;#39;t&amp;#39;, &amp;#39;T&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 21: &amp;#39;y&amp;#39;, &amp;#39;Y&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 22: &amp;#39;u&amp;#39;, &amp;#39;U&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 23: &amp;#39;i&amp;#39;, &amp;#39;I&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 24: &amp;#39;o&amp;#39;, &amp;#39;O&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 25: &amp;#39;p&amp;#39;, &amp;#39;P&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 30: &amp;#39;a&amp;#39;, &amp;#39;A&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 31: &amp;#39;s&amp;#39;, &amp;#39;S&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 32: &amp;#39;d&amp;#39;, &amp;#39;D&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 33: &amp;#39;f&amp;#39;, &amp;#39;F&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 34: &amp;#39;g&amp;#39;, &amp;#39;G&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 35: &amp;#39;h&amp;#39;, &amp;#39;H&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 36: &amp;#39;j&amp;#39;, &amp;#39;J&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 37: &amp;#39;k&amp;#39;, &amp;#39;K&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 38: &amp;#39;l&amp;#39;, &amp;#39;L&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 44: &amp;#39;z&amp;#39;, &amp;#39;Z&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 45: &amp;#39;x&amp;#39;, &amp;#39;X&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 46: &amp;#39;c&amp;#39;, &amp;#39;C&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 47: &amp;#39;v&amp;#39;, &amp;#39;V&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 48: &amp;#39;b&amp;#39;, &amp;#39;B&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 49: &amp;#39;n&amp;#39;, &amp;#39;N&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Keycode 50: &amp;#39;m&amp;#39;, &amp;#39;M&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This dump is telling us how keycodes map to symbols, both in &amp;ldquo;normal&amp;rdquo; and in shifted form. If we look up keycode 37, we indeed find the letter &lt;code>k&lt;/code>. With this, it&amp;rsquo;s just an &lt;a href="https://en.wikipedia.org/wiki/Small_matter_of_programming">SMOP&lt;/a> to come up with a program that parses the keymap as exposed by wskbd and converts keycodes to something useful.&lt;/p>
&lt;p>This is all good and dandy, but what happens if the keyboard is not connected when you try to open &lt;code>/dev/wskbd0&lt;/code>? (Spoiler: the &lt;code>open&lt;/code> call fails.) Or what happens if your computer has more than one keyboard attached? (Spoiler: you can only read events from one.) This is where wsmux comes to the rescue&amp;mdash;a device driver that multiplexes multiple input devices into one.&lt;/p>
&lt;p>By default, the system reserves &lt;code>/dev/wsmux0&lt;/code> as the multiplexer for all attached mice and &lt;code>/dev/wsmux1&lt;/code> as the multiplexer for all attached keyboards. We can define our own too via the &lt;a href="https://man.netbsd.org/wsmuxctl.8">wsmuxctl(8)&lt;/a> command line utility.&lt;/p>
&lt;p>wsmux then supports &amp;ldquo;hot plugging&amp;rdquo;. You can then open a &lt;code>/dev/wsmuxN&lt;/code> device even when there is no physical hardware attached, and whenever a peripheral is connected, it automatically becomes part of the mux. So, if we modify the program above to open &lt;code>/dev/wsmux1&lt;/code> instead of &lt;code>/dev/wskbd0&lt;/code>, the program will be resilient to missing keyboards and it&amp;rsquo;ll recognize multiple keyboards. Easy peasy!&lt;/p>
&lt;h1 id="what-will-you-build">What will you build?&lt;/h1>
&lt;p>You are now equipped with the basics to write graphical applications on a NetBSD system (and maybe OpenBSD too) without running X11. I know NetBSD may not be your jam, but it is a good choice for embedded projects due to its console architecture and other features like &lt;a href="/2024/12/netbsd-build-system.html">its build system&lt;/a>.&lt;/p>
&lt;p>If the code above still seems mysterious, you can read the source code for the &lt;a href="https://cvsweb.netbsd.org/bsdweb.cgi/xsrc/external/mit/xf86-video-wsfb/">xf86-video-wsfb&lt;/a> and &lt;a href="https://cvsweb.netbsd.org/bsdweb.cgi/xsrc/external/mit/xf86-input-ws/">xf86-input-ws&lt;/a> drivers for X.org. The code is easy enough to read, although it is longer because it has to support all the bells and whistles of wsdisplay and wskbd. (I took shortcuts above by making various assumptions on pixel formats and the like.)&lt;/p>
&lt;p>And, guess what, I am indeed working on an embedded project! A little dev box that can boot straight into EndBASIC with super-fast boot times and for which I couldn&amp;rsquo;t afford the X11 startup penalty.&lt;/p>
&lt;figure>
&lt;video width="100%" controls>
&lt;source src="/images/2025-01-17-endbasic-netbsd-boot.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>&lt;/figcaption>
&lt;/figure>
&lt;p>Stay tuned. In the meantime, what will &lt;em>YOU&lt;/em> build? For those of us in the U.S., there is a 3-day weekend ahead and this can be a good distraction. Have fun!&lt;/p></description><enclosure url="https://jmmv.dev/images/2025-01-17-cover-image.png" length="25978" type="image/jpeg"/></item><item><title>Self-documenting Makefiles</title><link>https://jmmv.dev/2025/01/make-help.html</link><pubDate>Fri, 10 Jan 2025 09:00:00 -0800</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2025/01/make-help.html</guid><description>&lt;p>Make, as arcane as a build tool can be, may still be a good first fit for certain scenarios. &amp;ldquo;Heresy!&amp;rdquo;, you say, as you hear a so-called &amp;ldquo;Bazel expert&amp;rdquo; utter these words.&lt;/p>
&lt;p>The specific problem I&amp;rsquo;m facing is that I need to glue together &lt;a href="/2024/12/netbsd-build-system.html">the NetBSD build system&lt;/a>, &lt;a href="/2013/11/patch-management-with-quilt.html">a quilt patch set&lt;/a>, EndBASIC&amp;rsquo;s Cargo-based Rust build, and a couple of QEMU invocations to produce a Frankenstein disk image for a Raspberry Pi. And the thing is: Make allows doing this sort of stitching with relative ease. Sure, Make is not the best option because the overall build performance is &amp;ldquo;meh&amp;rdquo; and because incremental builds are almost-impossible to get right&amp;hellip; but adopting Bazel for this project would be an almost-infinite time sink.&lt;/p>
&lt;p>Anyway. When using Make in this manner, you often end up with what&amp;rsquo;s essentially a &amp;ldquo;command dispatcher&amp;rdquo; and, over time, the number of commands grows and it&amp;rsquo;s hard to make sense of which one to use for what. Sure, you can write a &lt;code>README.md&lt;/code> with instructions, but I guarantee you that the text will get out of sync faster than you can read this article. There is a better way, though.&lt;/p>
&lt;figure>
&lt;img src="/images/2025-01-10-make-help.png" />
&lt;figcaption>Sample output of the &lt;tt>make help&lt;/tt> command that we will implement in this article.&lt;/figcaption>
&lt;/figure>
&lt;p>What if we could provide a &lt;code>make help&lt;/code> command that showed an overview of the project&amp;rsquo;s &amp;ldquo;build interface&amp;rdquo;? And what if we could embed such information inside the &lt;code>Makefile&lt;/code>s themselves, close to the entities that they document? This idea is neither new nor mine, and it has been written about before by different people. However, I bet that most of you haven&amp;rsquo;t heard about it before so it&amp;rsquo;s worth for me to repeat it. And I think that my solution is a bit more comprehensive than others I&amp;rsquo;ve found. So here you go.&lt;/p>
&lt;div class="container action-highlight p-4 my-4 d-md-none">
&lt;div class="row text-center">
&lt;p>A blog on operating systems, programming languages, testing, build systems, my own software
projects and even personal productivity. Specifics include FreeBSD, Linux, Rust, Bazel and
EndBASIC.&lt;/p>
&lt;/div>
&lt;div class="row">
&lt;div class="col">
&lt;div class="form-group">
&lt;form action="https://endtracker.azurewebsites.net/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/subscribers/add" method="post">
&lt;input type="text" name="email"
placeholder="Enter your email"
class="form-control input-sm text-center my-1"/>
&lt;button type="submit" class="btn btn-primary btn-block my-1">Subscribe&lt;/button>
&lt;/form>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="row px-2">
&lt;div class="col col-sm-5 text-left">
&lt;small>&lt;span class="subscriber-count">0&lt;/span> subscribers&lt;/small>
&lt;/div>
&lt;div class="col col-sm-7 text-right">
&lt;p>
&lt;a rel="me" href="https://mastodon.online/@jmmv">
&lt;img src="/images/badges/mastodon-logo.svg" width="32px" height="32px" alt="Follow @jmmv on Mastodon">
&lt;/a>
&lt;a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fjmmv.dev%2F&amp;amp;screen_name=jmmv">
&lt;img src="/images/badges/Twitter_logo_blue.svg" width="32px" height="32px" alt="Follow @jmmv on Twitter">
&lt;/a>
&lt;a href="/feed.xml">&lt;img src="/images/badges/feed-icon-28x28.png" alt="RSS feed">&lt;/a>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h1 id="targets">Targets&lt;/h1>
&lt;p>As I mentioned in the introduction, Make is often used as a command dispatcher: with very little code, you can write what essentially are multiple shell scripts with automatic chaining, all wrapped in one single interface. It&amp;rsquo;s all pretty terrible, but people are used to this pattern due to Make&amp;rsquo;s ubiquity and somehow expect it when they face a Make-based project.&lt;/p>
&lt;p>To implement this command dispatcher idea, each user-facing action is exposed via a &lt;em>target&lt;/em>. These targets tend to be marked as &amp;ldquo;phony&amp;rdquo;&amp;mdash;i.e. they are targets that produce no outputs of their own. Take a look at this &lt;code>Makefile&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-make" data-lang="make">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">.PHONY&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">build&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">build&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">target&lt;/span>/&lt;span class="n">debug&lt;/span>/&lt;span class="n">program&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">target/debug/program&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">src&lt;/span>/&lt;span class="n">main&lt;/span>.&lt;span class="n">rs&lt;/span> &lt;span class="n">src&lt;/span>/&lt;span class="n">lib&lt;/span>.&lt;span class="n">rs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cargo build
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">.PHONY&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">test&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">test&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">build&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> cargo &lt;span class="nb">test&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the snippet above, the &lt;code>target/debug/program&lt;/code> target represents a built &lt;em>file&lt;/em>. This target depends on a list of sources and specifies what command to run to generate the output when it is missing or out of date (according to file modification times, &lt;a href="/2020/12/google-no-clean-builds.html#file-modification-times">yikes&lt;/a>). When you type &lt;code>make target/debug/program&lt;/code>, you expect the file &lt;code>target/debug/program&lt;/code> to exist on disk after the command completes.&lt;/p>
&lt;p>But the snippet also shows two phony targets: &lt;code>build&lt;/code> and &lt;code>test&lt;/code>. When you type &lt;code>make build&lt;/code> or &lt;code>make test&lt;/code>, you do not expect neither a &lt;code>build&lt;/code> nor a &lt;code>test&lt;/code> file to be created, no. What you expect is that the project is built and tested. And for this, Make evaluates the dependencies of the phony targets (if any are specified, as is the case for &lt;code>build&lt;/code>) and then unconditionally executes any commands in the phony targets (as is the case for &lt;code>test&lt;/code>).&lt;/p>
&lt;p>With this in mind, the first thing we want to do in our &lt;code>make help&lt;/code> command is to document these &amp;ldquo;special&amp;rdquo; targets that represent user-facing actions. To do this, we&amp;rsquo;ll leverage one not-well-known aspect of Make&amp;rsquo;s syntax: the list of dependencies of a target is &lt;em>cumulative&lt;/em> across multiple target definitions of the same name. Basically, these target definitions are equivalent:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-make" data-lang="make">&lt;span class="line">&lt;span class="cl">&lt;span class="c"># All dependencies in one line.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span>&lt;span class="nf">target/debug/program&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">src&lt;/span>/&lt;span class="n">main&lt;/span>.&lt;span class="n">rs&lt;/span> &lt;span class="n">src&lt;/span>/&lt;span class="n">lib&lt;/span>.&lt;span class="n">rs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c"># Dependencies spread over multiple lines.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span>&lt;span class="nf">target/debug/program&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">src&lt;/span>/&lt;span class="n">main&lt;/span>.&lt;span class="n">rs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">target/debug/program&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">src&lt;/span>/&lt;span class="n">lib&lt;/span>.&lt;span class="n">rs&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Knowing this, we can add &amp;ldquo;extra&amp;rdquo; lines for a target and use one of those to document the target so that we do not end up with super-long lines. For example, we can do:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-make" data-lang="make">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">target/debug/program&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="c"># Builds the program.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span>&lt;span class="nf">target/debug/program&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">src&lt;/span>/&lt;span class="n">main&lt;/span>.&lt;span class="n">rs&lt;/span> &lt;span class="n">src&lt;/span>/&lt;span class="n">lib&lt;/span>.&lt;span class="n">rs&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And then we are just one &lt;code>grep&lt;/code> away from extracting the targets and their documentation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">sed -e&lt;span class="s1">&amp;#39;s/^\([^: ]\+\):.*#\(.*\)$/\1 \2/p;d&amp;#39;&lt;/span> Makefile &lt;span class="p">|&lt;/span> column -t -l &lt;span class="m">2&lt;/span> &lt;span class="p">|&lt;/span> sort
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>OK fine. It&amp;rsquo;s a bit more complicated than just &lt;code>grep&lt;/code> because we have to reformat the lines a bit and we need to create a nicely formatted table. Also, I know the &lt;code>sed&lt;/code> syntax is awful, but I really don&amp;rsquo;t want to call into Perl or Python as other guides tell you just for this silly string manipulation. There are native Unix tools that can help us here, and they are much lighter-weight.&lt;/p>
&lt;h1 id="variables">Variables&lt;/h1>
&lt;p>All other &amp;ldquo;self-documenting &lt;code>Makefile&lt;/code>&amp;rdquo; tutorials I found out there focus exclusively on documenting targets. But &lt;code>Makefile&lt;/code>s often expose another dimension of their API, and this is the collection of user-settable configuration variables that they accept.&lt;/p>
&lt;p>Many &lt;code>Makefile&lt;/code>s do things like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-make" data-lang="make">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">CFLAGS&lt;/span> &lt;span class="o">?=&lt;/span> -O2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&amp;hellip; to indicate that &lt;code>CFLAGS&lt;/code> is set to &lt;code>-O2&lt;/code>. But note: the &lt;code>?=&lt;/code> operator invites users to override the variable&amp;rsquo;s value if they choose to. For example, if the user wanted to build the project in debug mode, they could probably do the following and get the code to build without optimizations and with debug symbols:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">$ make &lt;span class="nv">CFLAGS&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;-O0 -g&amp;#34;&lt;/span> build
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Given that these variables are user-facing, we should document them as well as part of the &lt;code>make help&lt;/code> output.&lt;/p>
&lt;p>To document variables, we don&amp;rsquo;t have the luxury of splitting their definition into multiple lines like we did with targets to prevent super-long lines. That said, we can still add comments at the end of the line, like shown below, and those comments won&amp;rsquo;t be part of the variable&amp;rsquo;s default value. It is important, however, to not leave any space between the default value and the comment, or else the spaces become part of the variable&amp;rsquo;s value.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-make" data-lang="make">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">DEVELOPER&lt;/span> &lt;span class="o">?=&lt;/span> 0# Set to &lt;span class="m">1&lt;/span> to &lt;span class="nb">enable&lt;/span> developer builds.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Like with targets, we are also just one &lt;code>grep&lt;/code> away from extracting the variables and their documentation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">sed -e &lt;span class="s1">&amp;#39;s/^\([^ ]\+\)[ ]*?=[^#]\+#\(.*\)$/\1 \2/p;d&amp;#39;&lt;/span> Makefile &lt;span class="p">|&lt;/span> column -t -l &lt;span class="m">2&lt;/span> &lt;span class="p">|&lt;/span> sort
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Again, more complicated than just a &lt;code>grep&lt;/code>, but you get the idea.&lt;/p>
&lt;h1 id="putting-it-all-together">Putting it all together&lt;/h1>
&lt;p>Alright. So now we know how to extract a table documenting targets and a table documenting variables, but these two lists may still be too obscure on their own. Which targets are important? Which variables might the user want to look into first?&lt;/p>
&lt;p>To address this deficiency, we can preface those tables with some prose that explains, at a very high level, what to do when interacting with the project for the first time. To implement this, we can write the instructions in a separate file (like a &lt;code>README.md&lt;/code>) next to the &lt;code>Makefile&lt;/code>, and then have our &lt;code>make help&lt;/code> command print out the text file&amp;rsquo;s contents.&lt;/p>
&lt;p>And so without further ado, here is how we can tie everything together:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-make" data-lang="make">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">.PHONY&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">help&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">help&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="c"># Shows interactive help.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> @cat README.md
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> @echo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> @echo &lt;span class="s2">&amp;#34;make variables:&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> @echo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> @sed -e &lt;span class="s1">&amp;#39;s/^\([^ ]\+\)[ ]*?=[^#]\+#\(.*\)$$/\1 \2/p;d&amp;#39;&lt;/span> Makefile &lt;span class="p">|&lt;/span> column -t -l &lt;span class="m">2&lt;/span> &lt;span class="p">|&lt;/span> sort
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> @echo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> @echo &lt;span class="s2">&amp;#34;make targets:&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> @echo
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> @sed -e&lt;span class="s1">&amp;#39;s/^\([^: ]\+\):.*#\(.*\)$$/\1 \2/p;d&amp;#39;&lt;/span> Makefile &lt;span class="p">|&lt;/span> column -t -l &lt;span class="m">2&lt;/span> &lt;span class="p">|&lt;/span> sort
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you copy/paste this text, beware that there are embedded tabs in it. The ones at the beginning of the line are obvious, but the ones in the &lt;code>[ ]&lt;/code> character classes are not. The latter are supposed to be &lt;code>[ &amp;lt;tab&amp;gt;]&lt;/code>.&lt;/p>
&lt;p>Now, have fun with this, but please don&amp;rsquo;t use Make for new projects if you can avoid it!&lt;/p></description><enclosure url="https://jmmv.dev/images/2025-01-10-make-help.png" length="110230" type="image/jpeg"/></item><item><title>Revisiting the NetBSD build system</title><link>https://jmmv.dev/2024/12/netbsd-build-system.html</link><pubDate>Sat, 28 Dec 2024 08:50:00 +0100</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/12/netbsd-build-system.html</guid><description>&lt;p>I recently picked up an embedded project in which I needed to build a highly customized full system image with minimal boot times. As I explored my options, I came to the conclusion that NetBSD, the often-forgotten BSD variant, was the best viable choice for my project.&lt;/p>
&lt;p>One reason for this choice is NetBSD&amp;rsquo;s build system. Once you look and get past the fact that it feels frozen in time since 2002, you realize it is still one of the most advanced build systems you can find for an OS. And it shows: the NetBSD build system allows you to build the full OS from scratch, on pretty much any host POSIX platform, while targeting any hardware architecture supported by NetBSD. All without root privileges.&lt;/p>
&lt;p>Another reason for this choice is that NetBSD was my daily workhorse for many years and I&amp;rsquo;m quite familiar with its internals, which is useful knowledge to quickly achieve the goals I have in mind. In fact, I was a NetBSD Developer with capital D: I had commit access to the project from about 2002 through 2012 or so, and I have just revived my account in service of this project. &lt;code>jmmv@&lt;/code> is back!&lt;/p>
&lt;p>So, strap onto your seats and let&amp;rsquo;s see how today&amp;rsquo;s NetBSD build system looks like and what makes it special. I&amp;rsquo;ll add my own critique at the end, because it ain&amp;rsquo;t perfect, but overall it continues to deliver on its design goals set in the late 1990s.&lt;/p>
&lt;div class="container action-highlight p-4 my-4 d-md-none">
&lt;div class="row text-center">
&lt;p>A blog on operating systems, programming languages, testing, build systems, my own software
projects and even personal productivity. Specifics include FreeBSD, Linux, Rust, Bazel and
EndBASIC.&lt;/p>
&lt;/div>
&lt;div class="row">
&lt;div class="col">
&lt;div class="form-group">
&lt;form action="https://endtracker.azurewebsites.net/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/subscribers/add" method="post">
&lt;input type="text" name="email"
placeholder="Enter your email"
class="form-control input-sm text-center my-1"/>
&lt;button type="submit" class="btn btn-primary btn-block my-1">Subscribe&lt;/button>
&lt;/form>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="row px-2">
&lt;div class="col col-sm-5 text-left">
&lt;small>&lt;span class="subscriber-count">0&lt;/span> subscribers&lt;/small>
&lt;/div>
&lt;div class="col col-sm-7 text-right">
&lt;p>
&lt;a rel="me" href="https://mastodon.online/@jmmv">
&lt;img src="/images/badges/mastodon-logo.svg" width="32px" height="32px" alt="Follow @jmmv on Mastodon">
&lt;/a>
&lt;a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fjmmv.dev%2F&amp;amp;screen_name=jmmv">
&lt;img src="/images/badges/Twitter_logo_blue.svg" width="32px" height="32px" alt="Follow @jmmv on Twitter">
&lt;/a>
&lt;a href="/feed.xml">&lt;img src="/images/badges/feed-icon-28x28.png" alt="RSS feed">&lt;/a>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h1 id="the-basics">The basics&lt;/h1>
&lt;p>The NetBSD build system is powerful and featureful, but it&amp;rsquo;s also arcane as it&amp;rsquo;s based on a combination of the BSD variant of make and shell scripts. Just peek through the files under &lt;a href="https://cvsweb.netbsd.org/bsdweb.cgi/src/share/mk/">&lt;code>src/share/mk/&lt;/code>&lt;/a>, the directory that contains the bulk of the infrastructure, to see what I mean.&lt;/p>
&lt;p>As a user of the build system, however, you rarely interact with make directly. Instead, you use the &lt;code>build.sh&lt;/code> script located at the top of the source tree. This script provides a user-friendly interface to most operations you may want to do, and abstracts away the intricacies of the targets that coordinate the build of the system and the configuration that controls it.&lt;/p>
&lt;p>The structure of the command is to pass high-level &amp;ldquo;goals&amp;rdquo; to &lt;code>build.sh&lt;/code> as arguments, which indicate the operations to perform. In its most simple form, all you need to do to build a full system distribution targeting the architecture of the host is:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">./build.sh tools release
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But hey, I promised you can trivially cross-build too, right? Sure, let&amp;rsquo;s compile the system for a Raspberry Pi with a 64-bit chip, produce the USB image that we can write to an SD card, and do everything as an unprivileged user:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">./build.sh -U -a aarch64 -m evbarm tools release disk-image
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>That&amp;rsquo;s it, really. That&amp;rsquo;s all it takes.&lt;/p>
&lt;p>We must dig deeper though, so let&amp;rsquo;s look at some of those &amp;ldquo;goals&amp;rdquo; to see what &lt;code>tools&lt;/code> means and understand how a release is put together without root privileges.&lt;/p>
&lt;h1 id="the-toolchain">The toolchain&lt;/h1>
&lt;p>The very first step of any &lt;code>build.sh&lt;/code> invocation is to generate the toolchain used to build the rest of the system. This is true of any build, including those that target the host machine, because this ensures that the build is independent of the host&amp;rsquo;s state. In particular, this avoids the situation where you might have to upgrade certain components before building, which was/is common in other BSDs.&lt;/p>
&lt;p>And you have seen this prerequisite step in the previous section, by the way: all sample &lt;code>build.sh&lt;/code> invocations I showed you included &lt;code>tools&lt;/code> as the first goal. Now, you don&amp;rsquo;t &lt;em>have&lt;/em> to provide &lt;code>tools&lt;/code> to &lt;em>every&lt;/em> invocation of the script: as soon as you have built a toolchain, you can reuse it in subsequent invocations.&lt;/p>
&lt;p>Building a toolchain with &lt;code>build.sh&lt;/code> is incredibly handy on its own, as you can produce cross-build toolchains and use them for other purposes outside of building NetBSD itself. Zig&amp;rsquo;s build system has often been praised for this reason, but NetBSD&amp;rsquo;s has nothing to envy. For example, if we do this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">./build.sh -a aarch64 -m evbarm -T ~/tools tools
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We end up with a cross-build C and C++ toolchain under &lt;code>~/tools/&lt;/code> that targets NetBSD running on ARM 64 bits. And what goes into the toolchain, you ask?&lt;/p>
&lt;figure>
&lt;img src="/images/2024-12-28-netbsd-tools.png" />
&lt;figcaption>Directory listing of &lt;tt>tools&lt;/tt> as produced by the command above and its &lt;tt>bin&lt;/tt> subdirectory.&lt;/figcaption>
&lt;/figure>
&lt;p>In this listing, you can see a bunch of binaries prefixed with &lt;code>aarch64--netbsd-&lt;/code>. These are all part of the C and C++ toolchain. The rest of the tools, prefixed with &lt;code>nb&lt;/code>, are NetBSD-specific tools required during the build. These are programs that are part of a normal NetBSD installation and would be available without the &lt;code>nb&lt;/code> prefix if we were building on a NetBSD host, but remember, the build system supports &lt;em>any&lt;/em> POSIX host OS. Take &lt;code>nbsed&lt;/code> as an example: yes, all POSIX hosts provide a &lt;code>sed&lt;/code> tool, but its syntax varies among systems so the NetBSD build system isolates itself from those differences by compiling &lt;code>sed&lt;/code> as a host tool and using that throughout.&lt;/p>
&lt;p>One special tool from this listing is &lt;code>nbmake-evbarm&lt;/code>. This is not the binary for make (which is itself stored as &lt;code>nbmake&lt;/code>). This is a shell script that captures all settings provided to &lt;code>build.sh&lt;/code> and then invokes &lt;code>nbmake&lt;/code> with those, and this script is useful when you want to manually rebuild portions of the tree. Not something you would want to use as an &amp;ldquo;end user&amp;rdquo; of the build, but something you definitely will want to use as a NetBSD developer.&lt;/p>
&lt;h1 id="build-structure">Build structure&lt;/h1>
&lt;p>Let&amp;rsquo;s explore the source tree a bit, which is the prime example of a monorepo in an open source project:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-12-28-netbsd-src-bin-ls.png" />
&lt;figcaption>Directory listing of the source tree, its &lt;tt>bin&lt;/tt> and &lt;tt>bin/ls&lt;/tt> subdirectories, and the content of &lt;tt>bin/ls/Makefile&lt;/tt>.&lt;/figcaption>
&lt;/figure>
&lt;p>In this picture, you can see first the content of the top-level directory of the source tree. It all looks pretty simple: there are various subdirectories, such as &lt;code>bin&lt;/code>, &lt;code>lib&lt;/code>, or &lt;code>usr.bin&lt;/code>, that roughly track the structure of the installed system; there is the &lt;code>build.sh&lt;/code> script that I previously described; and there is a &lt;code>Makefile&lt;/code> as well. Looking into one subdirectory, like &lt;code>bin&lt;/code>, we see another &lt;code>Makefile&lt;/code> and many more subdirectories, one per tool installed onto &lt;code>/bin/&lt;/code>.&lt;/p>
&lt;p>Knowing this directory-based structure, we can use the &lt;code>nbmake-evbarm&lt;/code> wrapper script I mentioned earlier to operate on just a portion of the monorepo. Focusing on the &lt;code>ls&lt;/code> example shown in the screenshot, we could build and upgrade this piece of the system on its own by doing:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ cd ~/src/bin/ls
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ ~/tools/bin/nbmake-evbarm all
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">... console noise ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ sudo ~/tools/bin/nbmake-evbarm install
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">... console noise ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Another extra detail to highlight from the screenshot is that NetBSD&amp;rsquo;s &lt;code>Makefile&lt;/code>s are mostly declarative. Each &lt;code>Makefile&lt;/code> defines a bunch of variables to specify what is being built and, at the end, includes one of the many &lt;code>bsd.*.mk&lt;/code> files that pull in the build logic. Among these, we have &lt;code>bsd.prog.mk&lt;/code> to build one program, &lt;code>bsd.lib.mk&lt;/code> to build one static/shared library, and &lt;code>bsd.subdir.mk&lt;/code> to recurse into subdirectories. Importantly, the general design is to build just &lt;em>one&lt;/em> item per directory&amp;mdash;although I myself broke this rule when I added &lt;code>bsd.tests.mk&lt;/code> to build tests because splitting them into subdirectories would have added too much noise to the tree.&lt;/p>
&lt;p>This declarative design is interesting because it maps well to the foundations of modern build systems like Bazel. In fact, the design of the NetBSD build system is what fueled my interest in build systems, influenced the design of &lt;a href="/2022/05/remembering-buildtool.html">my own Buildtool&lt;/a>, and made me like &lt;del>Bazel&lt;/del> Blaze as soon as I first saw it in 2008.&lt;/p>
&lt;h1 id="the-destdir">The destdir&lt;/h1>
&lt;p>In order to produce the structure of the final installation, the build system uses the &amp;ldquo;destdir&amp;rdquo; concept. A destdir is a staging location where built files are installed, but paths to this staging location are &lt;em>not&lt;/em> used within the artifacts produced by the build. This idea &lt;a href="https://www.gnu.org/software/automake/manual/html_node/DESTDIR.html">exists in other build systems such as GNU Automake&lt;/a> and is pretty much a necessity to build multiple pieces of software together before installing them or to package software without root privileges.&lt;/p>
&lt;p>Imagine that you want to build a library, say &lt;code>libm&lt;/code> (the math library), and a tool that uses it, say &lt;code>bc&lt;/code> (the calculator). &lt;code>libm&lt;/code> typically goes into &lt;code>/lib/&lt;/code> so we cannot just build and install it in place: for one, we may &amp;ldquo;break&amp;rdquo; the existing system if the new version happens to be backwards-incompatible; for another, we may be targeting a different architecture so we cannot just replace &lt;code>/lib/libm.so&lt;/code> with an incompatible version.&lt;/p>
&lt;p>The destdir comes to the rescue. We first build &lt;code>libm&lt;/code> as if it would be installed into &lt;code>/lib/&lt;/code>. However, during &lt;em>installation&lt;/em>, we prefix all file copy operations with the destdir. In this way, we build a separate &amp;ldquo;system root&amp;rdquo;, say &lt;code>/tmp/destdir/lib/&lt;/code>, that contains the newly-built &lt;code>libm.so&lt;/code>. After that, we build &lt;code>bc&lt;/code> and point it to the &lt;code>libm&lt;/code> that&amp;rsquo;s in &lt;code>/tmp/destdir/lib/&lt;/code>, but&amp;hellip; we have a problem: we can&amp;rsquo;t allow the &lt;code>/tmp/destdir/&lt;/code> path to appear anywhere inside the &lt;code>bc&lt;/code> binary because this directory is transient. To fix this, we must separate build paths from runtime paths during the build: when we build &lt;code>bc&lt;/code>, we tell the linker to look for libraries under &lt;code>/tmp/destdir/lib/&lt;/code> via the &lt;code>-L&lt;/code> flag, and we also tell the linker that, at &lt;em>runtime&lt;/em>, libraries will be available in &lt;code>/lib/&lt;/code> via the &lt;code>--rpath&lt;/code> flag (which stands for runtime path).&lt;/p>
&lt;p>As you can imagine, the NetBSD build system heavily relies on this idea and, after a &lt;code>distribution&lt;/code> build (implied by the &lt;code>release&lt;/code> goal I showed earlier):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">./build.sh -D /tmp/testdir/ distribution
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>we end up with a destdir that contains all system files laid out exactly as they need to be installed.&lt;/p>
&lt;p>In fact, if you run the build as root and target the host system (where the host is NetBSD), the destdir can serve as the target of a chroot. So, if you do:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">chroot /tmp/destdir
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>you essentially can enter the freshly-built system. This may or may not work, however: the newly-built binaries might require new kernel features, which is likely true if you are building a more modern NetBSD release from an older release or if you are tracking NetBSD-current. And this obviously won&amp;rsquo;t work if you are cross-building.&lt;/p>
&lt;h1 id="distribution-media">Distribution media&lt;/h1>
&lt;p>The destdir serves as a staging area but it does not represent the final artifacts of the build. To put the destdir to use, we either have to &amp;ldquo;copy&amp;rdquo; the staging area onto the host to perform an in-place upgrade, or we need to build distribution media.&lt;/p>
&lt;p>The former case of an in-place upgrade is tricky because it requires issuing manual post-installation steps, so I&amp;rsquo;m not going to describe it here. But the latter case of producing distribution media is trivial. For example, we can do:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">./build.sh release
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>to produce the release &amp;ldquo;sets&amp;rdquo; for the system from the contents of the destdir, or we can do:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">./build.sh iso-image install-image live-image
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>to create various types of installation media (a bootable CD, a bootable USB image, a live system image&amp;hellip;) from the contents of the destdir as well.&lt;/p>
&lt;p>The release sets are an interesting thing to discuss because they form the core of a NetBSD distribution. You see: NetBSD ships as a collection of tarballs, and installing NetBSD amounts to simply unpacking those tarballs onto a file system and performing a few post-installation configuration steps.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-12-28-netbsd-amd64-binary-sets.png" class="with-border" />
&lt;figcaption>Content of the &lt;tt>binary/sets&lt;/tt> directory of the NetBSD/amd64 distribution.&lt;/figcaption>
&lt;/figure>
&lt;p>Now, the way these tarballs are produced from the destdir is by leveraging &lt;code>mtree&lt;/code>, a really cool tool that is not known in Linux land. The purpose of this tool is to compare a textual &amp;ldquo;golden&amp;rdquo; representation of a directory against the actual contents of the directory, and highlight where they might differ.&lt;/p>
&lt;p>BSD systems use &lt;code>mtree&lt;/code> to describe how the installed system looks like and, as you can imagine, NetBSD is no exception. The NetBSD build system uses &lt;code>mtree&lt;/code> files to ensure the destdir contents match expectations, and also uses the &lt;code>mtree&lt;/code> &amp;ldquo;manifests&amp;rdquo; to &amp;ldquo;bucketize&amp;rdquo; the files from the destdir into the individual release sets. You can find these golden manifests in the &lt;a href="https://cvsweb.netbsd.org/bsdweb.cgi/src/distrib/sets/lists/">&lt;code>src/distrib/sets/lists/&lt;/code>&lt;/a> directory.&lt;/p>
&lt;h1 id="unprivileged-builds">Unprivileged builds&lt;/h1>
&lt;p>These &lt;code>mtree&lt;/code> files are also critical for another very important feature: namely, the ability to build the whole NetBSD system as an unprivileged user. This, to me, is one of the most impressive features of this build system: you can produce the full build, including disk images, without ever running &lt;code>sudo&lt;/code> or using weird intercept tools like Debian&amp;rsquo;s &lt;code>fakeroot&lt;/code>.&lt;/p>
&lt;p>Here is how this works. When building in unprivileged mode (enabled via the &lt;code>-U&lt;/code> flag to &lt;code>build.sh&lt;/code>), the build system produces a &lt;code>METALOG&lt;/code> file under the destdir. This file looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ grep &amp;#39;^\./sbin&amp;#39; ~/destdir/METALOG | head -n 5
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">./sbin type=dir uname=root gname=wheel mode=0755
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">./sbin/amrctl type=file uname=root gname=wheel mode=0555 size=72120 time=1735119460.0 sha256=d6f474441dc98648a4e8ec633dd76fc3349c85a0af9830ce8eb6566e94291fdb
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">./sbin/apmlabel type=file uname=root gname=wheel mode=0555 size=72360 time=1735119460.0 sha256=731cf433328bd14c6d3f322ef60fa92eb9eaa2725baa0cb52253b50743d9d324
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">./sbin/atactl type=file uname=root gname=wheel mode=0555 size=73512 time=1735119461.0 sha256=74a15335c8715513ac50f615a8e906319df79f76dad601bc688ae214b3e41673
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">./sbin/badsect type=file uname=root gname=wheel mode=0555 size=72328 time=1735119461.0 sha256=76e34649bf100fe490befb2a4ec58455a7710a665dd59bcbbd8c672a6086bde8
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Every line of this file maps a file system entry (a directory, a file, a device&amp;hellip;) stored in the destdir to its properties, including ownership information and permissions. These entries are generated from metadata encoded in the &lt;code>Makefile&lt;/code>s whenever the build system places a new file under the destdir via the &lt;code>install&lt;/code> command (another nice tool often unknown to Linux users). The &lt;code>METALOG&lt;/code> is the key that allows building media images without root privileges.&lt;/p>
&lt;p>If you think about it, media images are simply files with an internal structure that represents disk partitions, file systems, and metadata. Because they are simply files, there is no need to have root access nor to make the host&amp;rsquo;s &lt;code>passwd&lt;/code> file contain all users represented by entries in these file systems. Traditionally, OS builds have needed root because it&amp;rsquo;s easier to leverage the kernel&amp;rsquo;s virtual devices and file system implementations, but there is not inherent reason for that to be the only choice. All the work can be done in user space, and that&amp;rsquo;s precisely what NetBSD does.&lt;/p>
&lt;p>Now, go back and revisit the screenshot above that showed the toolchain contents. You&amp;rsquo;ll notice tools like &lt;code>nbmakefs&lt;/code> (the tool to format a file system) and &lt;code>nbgpt&lt;/code> (the tool to create a GPT partitioning scheme). These tools are part of the toolchain because they are needed to generate installation media, and these tools know how to read the &lt;code>METALOG&lt;/code> in order to embed the right permissions and special file modes into the built images. All without ever becoming root.&lt;/p>
&lt;h1 id="sysbuild">sysbuild&lt;/h1>
&lt;p>Now, as simple and powerful as &lt;code>build.sh&lt;/code> might be, I find it cumbersome for day to day use if you want to customize any of its default settings. It is not uncommon to end up running &lt;code>build.sh&lt;/code> with invocations like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">./build.sh -O ../obj -T ../tools -U -u -V &lt;span class="nv">MKDEBUG&lt;/span>&lt;span class="o">=&lt;/span>no -V &lt;span class="nv">MKGCC&lt;/span>&lt;span class="o">=&lt;/span>no distribution
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>which the official documentation describes as &amp;ldquo;golden invocations&amp;rdquo; and I have no desire to type or even remember.&lt;/p>
&lt;p>This is what drove me to &lt;a href="/software/sysbuild.html">write sysbuild&lt;/a>: a layer of abstraction over &lt;code>build.sh&lt;/code> &lt;em>and&lt;/em> &lt;code>cvs&lt;/code> that coordinates updating the source tree and building it. The tool even integrates with &lt;code>cron&lt;/code> trivially, providing a mechanism to keep NetBSD-current installations up-to-date.&lt;/p>
&lt;p>sysbuild is driven by configuration &amp;ldquo;profiles&amp;rdquo; which allow you to customize the paths and settings of a build in just one place and then puts them to use with a trivial command. For example, with a configuration file like the following stored in &lt;code>~/.sysbuild/rpi.conf&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">BUILD_ROOT&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">HOME&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">/netbsd&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">BUILD_TARGETS&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;tools sets disk-image&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">CVSROOT&lt;/span>&lt;span class="o">=&lt;/span>:ext:anoncvs@cvs.NetBSD.org:/cvsroot
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">MACHINES&lt;/span>&lt;span class="o">=&lt;/span>amd64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">RELEASEDIR&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BUILD_ROOT&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">/release&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">SRCDIR&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BUILD_ROOT&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">/src&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">UPDATE_SOURCES&lt;/span>&lt;span class="o">=&lt;/span>no
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can simply run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">sysbuild -c rpi build
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>to update the NetBSD source tree to the latest version, ensure that the tools are up-to-date, and produce the USB disk images for a Raspberry Pi.&lt;/p>
&lt;h1 id="deficiencies">Deficiencies&lt;/h1>
&lt;p>Not everything about the NetBSD build system is rosy though.&lt;/p>
&lt;p>&lt;em>The&lt;/em> thing that differentiates a good build system from a &amp;ldquo;meh&amp;rdquo; one for me personally is the behavior of incremental builds and, in particular, two aspects of these:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>First, incremental builds need to do minimal work, especially when there is &amp;ldquo;nothing to do&amp;rdquo;. The NetBSD build system is a recursive make one (which comes with its own &lt;a href="https://accu.org/journals/overload/14/71/miller_2004/">set of problems&lt;/a>), so it does &lt;em>not&lt;/em> do minimal work. On my 72-core machine, it takes about 3 minutes to run through a &lt;code>build.sh release&lt;/code> invocation that does nothing. This is OK for end users looking to upgrade their running machine, but it is painful because it makes iterating on system changes difficult. As a developer, you end up needing to know how to surgically rebuild individual subdirectories using the &lt;code>nbmake-&amp;lt;arch&amp;gt;&lt;/code> wrappers I described earlier, and manually track dependencies across those.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Second, incremental builds must &lt;a href="/2020/12/google-no-clean-builds.html">always deliver correct results&lt;/a> &lt;em>without&lt;/em> having to do a &lt;code>make clean&lt;/code> in between. But that&amp;rsquo;s generally not true for make-based build systems&amp;mdash;and NetBSD&amp;rsquo;s is no exception. Generally, an incremental build after a &lt;code>git pull&lt;/code> (sorry, a &lt;code>cvs update&lt;/code>) will work fine, but sometimes it won&amp;rsquo;t. And if you start playing with build-time switches (things like &lt;code>MKDEBUG&lt;/code>), then you are out of luck and must resort to a &lt;code>make clean&lt;/code> to &amp;ldquo;switch configurations&amp;rdquo;.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>And there are other problems. Running a parallel build on a system with many cores sometimes leads to spurious build failures because the interdependencies between components are not always precisely specified (it&amp;rsquo;s &lt;em>really&lt;/em> difficult to be correct with make). And the build is inefficient: of those 3 minutes I mentioned earlier, you can see that &lt;em>most of the time&lt;/em> is wasted by make recursing through directories and discovering there is nothing to do, whereas other times, make &amp;ldquo;chokes&amp;rdquo; on way too many C++ compiles at once that lead to out of memory situations.&lt;/p>
&lt;p>It has been &lt;a href="/2015/04/on-bazel-and-open-source.html">my dream since the publication of Bazel&lt;/a> as open source to have a Bazel-based build of NetBSD. I think Bazel is the perfect build system for such a project because it&amp;rsquo;d deliver correct and efficient incremental builds to NetBSD&amp;rsquo;s monorepo, and it would save tons of resources when running on many-core machines. Except for the fact that it&amp;rsquo;s written in Java, so it&amp;rsquo;d be a really odd choice for such project. Maybe Buck 2 would be suitable. Anyway, one can only dream&amp;hellip;&lt;/p>
&lt;h1 id="but-why">But why?&lt;/h1>
&lt;p>Why I&amp;rsquo;m looking at this at all again, after years of not touching NetBSD? I said it in the opening: I&amp;rsquo;m working on a new embedded project for which NetBSD is the greatest fit. I could tell you what it is about but it&amp;rsquo;s easier to just show you:&lt;/p>
&lt;figure>
&lt;video width="100%" controls>
&lt;source src="/images/2024-12-28-endbasic-rpi-boot.mp4" type="video/mp4">
&lt;/video>
&lt;figcaption>Second iteration of the "boot to EndBASIC" prototype.&lt;/figcaption>
&lt;/figure>
&lt;p>OK, fine, in words: I am building a minimal system that boots straight into EndBASIC with quick build times and low overhead. You&amp;rsquo;ll have to wait a bit more to get your hands on this though, as I&amp;rsquo;m still ironing out various details and want to end up providing a pre-built &amp;ldquo;box&amp;rdquo; with the right hardware and software combination.&lt;/p>
&lt;p>I&amp;rsquo;m also becoming &lt;em>super&lt;/em> tempted to migrate NetBSD&amp;rsquo;s build to Bazel to make my own life easier in this journey. This is a monumental task&amp;hellip; but I&amp;rsquo;m not sure that it&amp;rsquo;d be crazy to tackle the minimum subset of NetBSD that I need for this minimal disk image and port only those portions to Bazel. The results might impress some and then want to help the effort. Right?&lt;/p>
&lt;p>In the meantime, I encourage you to read through the comprehensive &lt;a href="https://www.netbsd.org/docs/guide/en/part-compile.html">Building the system&lt;/a> portion of &lt;a href="https://www.netbsd.org/docs/guide/en/index.html">The NetBSD guide&lt;/a>, and to play with building a NetBSD image straight from your Linux machine. You may like it.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-12-28-cover-image.jpg" length="403124" type="image/jpeg"/></item><item><title>Synology DS923+ vs. FreeBSD w/ZFS</title><link>https://jmmv.dev/2024/12/synology-ds923-vs-freebsd.html</link><pubDate>Fri, 13 Dec 2024 09:10:00 -0800</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/12/synology-ds923-vs-freebsd.html</guid><description>&lt;p>My interest in storage is longstanding&amp;mdash;I loved playing with different file systems in my early Unix days and then I worked on Google&amp;rsquo;s and Microsoft&amp;rsquo;s distributed storage solutions&amp;mdash;and, about four years ago, I started running a home-grown NAS leveraging FreeBSD and its excellent ZFS support. I first hosted the server on a PowerMac G5 and then upgraded it to an overkill 72-core ThinkStation that I snapped second-hand for a great price.&lt;/p>
&lt;p>But as stable and low maintenance as FreeBSD is, running day-to-day services myself is not my idea of &amp;ldquo;fun&amp;rdquo;. This drove me to replace this machine&amp;rsquo;s routing functionality with a dedicated pfSense box a year ago and, for similar reasons, I have been curious about dedicated NAS solutions.&lt;/p>
&lt;p>I was pretty close to buying a second-hand NAS from the work classifieds channel when a Synology marketing person (hi Kyle!) contacted me to offer a partnership: they&amp;rsquo;d ship me &lt;a href="https://sy.to/hekgh">one of their devices&lt;/a> for free in exchange for me publishing a few articles about it. Given my interest to drive-test one of these appliances without committing to buying one (they ain&amp;rsquo;t cheap and I wasn&amp;rsquo;t convinced I wanted to get rid of my FreeBSD-based solution), I was game.&lt;/p>
&lt;p>And you guessed right: this article is one of those I promised to write but, before you stop reading, the answer is no. This post is &lt;em>not&lt;/em> sponsored by Synology and has not been reviewed nor approved by them. The content here, including any opinions, are purely my own. And what I want do do here is compare how the Synology appliance stacks against my home-built FreeBSD server.&lt;/p>
&lt;div class="container action-highlight p-4 my-4 d-md-none">
&lt;div class="row text-center">
&lt;p>A blog on operating systems, programming languages, testing, build systems, my own software
projects and even personal productivity. Specifics include FreeBSD, Linux, Rust, Bazel and
EndBASIC.&lt;/p>
&lt;/div>
&lt;div class="row">
&lt;div class="col">
&lt;div class="form-group">
&lt;form action="https://endtracker.azurewebsites.net/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/subscribers/add" method="post">
&lt;input type="text" name="email"
placeholder="Enter your email"
class="form-control input-sm text-center my-1"/>
&lt;button type="submit" class="btn btn-primary btn-block my-1">Subscribe&lt;/button>
&lt;/form>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="row px-2">
&lt;div class="col col-sm-5 text-left">
&lt;small>&lt;span class="subscriber-count">0&lt;/span> subscribers&lt;/small>
&lt;/div>
&lt;div class="col col-sm-7 text-right">
&lt;p>
&lt;a rel="me" href="https://mastodon.online/@jmmv">
&lt;img src="/images/badges/mastodon-logo.svg" width="32px" height="32px" alt="Follow @jmmv on Mastodon">
&lt;/a>
&lt;a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fjmmv.dev%2F&amp;amp;screen_name=jmmv">
&lt;img src="/images/badges/Twitter_logo_blue.svg" width="32px" height="32px" alt="Follow @jmmv on Twitter">
&lt;/a>
&lt;a href="/feed.xml">&lt;img src="/images/badges/feed-icon-28x28.png" alt="RSS feed">&lt;/a>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h1 id="the-hardware">The hardware&lt;/h1>
&lt;p>Here are the two contenders in my comparison:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-12-13-ds923-and-thinkstation.jpg" />
&lt;figcaption>Synology D923+ next to my ThinkStation P710 on top of a dusty LackRack in the garage.&lt;/figcaption>
&lt;/figure>
&lt;ul>
&lt;li>&lt;strong>My home-built NAS:&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Hardware:&lt;/strong> &lt;a href="https://amzn.to/3DaBQze">ThinkStation P710&lt;/a> equipped with 2x Intel Xeon E5-2697 v4 processors (18c/36t) at 2.30GHz, 64GB of RAM, 2x &lt;a href="https://amzn.to/3ZvimN7">Seagate 4TB Enterprise Capacity 7200 RPM&lt;/a> drives and a &lt;a href="https://amzn.to/49zx3DE">Samsung 970 EVO Plus SSD&lt;/a> 500GB.&lt;/li>
&lt;li>&lt;strong>Operating system:&lt;/strong> FreeBSD 14.&lt;/li>
&lt;li>&lt;strong>Storage configuration:&lt;/strong> ZFS with the two HDD drives in mirror mode and with the SSD set up as the L2ARC plus ZIL log for the drive pool.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>The Synology NAS:&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Hardware:&lt;/strong> &lt;a href="https://amzn.to/3ZL7sUI">Synology DS923+&lt;/a> equipped with 3x &lt;a href="https://amzn.to/4g73s6S">Synology Plus Series 4TB 5400 RPM&lt;/a> drives and the same &lt;a href="https://amzn.to/49zx3DE">Samsung 970 EVO Plus SSD&lt;/a> 500GB that I moved from one machine to the other. The box is equipped with a 4-core AMD Ryzen Embedded R1600 and 4GB of RAM.&lt;/li>
&lt;li>&lt;strong>Operating system:&lt;/strong> Synology&amp;rsquo;s own DiskStation Manager (DSM).&lt;/li>
&lt;li>&lt;strong>Storage configuration:&lt;/strong> btrfs with the three HDD drives set up in RAID 5 mode and with the SSD set up to act as the cache for the drive pool.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The two configurations are quite different&amp;mdash;after all, I am comparing a workstation machine with lots of spare CPU and RAM to a dedicated machine specifically crafted for file sharing&amp;mdash;so it&amp;rsquo;s going to be difficult to be &amp;ldquo;fair&amp;rdquo; in any comparison. In any case, here are a few things to contrast:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>IOPS:&lt;/strong> The P710 runs with 2 7200 RPM drives in mirror mode whereas the DS923+ runs with 3 5400 RPM drives in RAID 5 mode. The number of total IOPS from each is going to differ but&amp;hellip; for all purposes, the 1Gbit NIC that each machine has is the limiting factor in performance so I haven&amp;rsquo;t bothered to run any performance tests. Both can saturate the network, so there is that.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Quality:&lt;/strong> Both the P710 and the DS923+ are impressive machines&amp;mdash;maybe not PowerMac G5 impressive levels, but pretty, pretty close. I love the ThinkStation&amp;rsquo;s outer design and the interior shows great cable management and airflow. As for the NAS, I love the lightweight and small form factor that allows placing it pretty much anywhere. Both are tool-less enclosures.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Noise:&lt;/strong> When idle, both machines are equally quiet. The ThinkStation gets really, really loud under heavy load though, and it is also uncomfortably loud even at idle when the room temperature is warm (around or over 25C). The DS923+, however, seems quiet throughout. The Synology Plus Series HDDs are also quieter than the ones I had in the ThinkStation, and that&amp;rsquo;s partly because they are slower 5400 RPM drives. In any case, these two machines stay in my garage so I don&amp;rsquo;t care about their noise.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Power consumption:&lt;/strong> I unfortunately do not own a tool to measure it, but it&amp;rsquo;d be neat to compare how these two stack up. I&amp;rsquo;m sure I&amp;rsquo;ve thrown money away by keeping the ThinkStation online 24x7 and having those drives never rest (ZFS doesn&amp;rsquo;t let them spin down) but it&amp;rsquo;s hard to care about it because my power bill is dominated by heating almost year-round.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="freebsd-and-zfs">FreeBSD and ZFS&lt;/h1>
&lt;p>FreeBSD is my current favorite system for servers. FreeBSD remains close to its Unix roots and maintains rational and orthogonal tooling (unlike Linux), is quick and trivial to maintain (I could run through the 13 to 14 upgrade in&amp;hellip; 15 minutes?), and bundles modern technology like ZFS and bhyve (unlike NetBSD, sadly, which used to be my BSD of choice).&lt;/p>
&lt;p>It is true that FreeBSD gave me some headaches when I ran it on the PowerMac G5, but that&amp;rsquo;s expected due to the machine being 20 years old and &lt;a href="https://www.freebsd.org/platforms/ppc/">FreeBSD&amp;rsquo;s PowerPC support&lt;/a> being a Tier 2 platform. The thing is that I never intended to run a NAS on the G5; it just so happened to be the only machine I had available for it. In any case, I have had &lt;em>zero&lt;/em> problems on the ThinkStation. Mind you: when I bought this machine, both Windows and Fedora experienced occasional freezes with their default installations (before pulling upgrades from the network) and FreeBSD never has shown any signs of instability.&lt;/p>
&lt;p>As for ZFS, it is hard to convey the feeling of &amp;ldquo;power&amp;rdquo; you experience when you type &lt;code>zpool&lt;/code> and &lt;code>zfs&lt;/code> commands and see the machine coordinate multiple disks to offer you a dependable storage solution. Creating file systems on a pool, creating raw volumes for VMs or iSCSI targets, taking snapshots, replicating snapshots over the network or to backup USB drives, scrubbing the drives to verify data integrity&amp;hellip; all are trivial commands away.&lt;/p>
&lt;p>ZFS feels slow though. I grew up thinking of file systems as contiguous portions of a disk and tinkering with their partitioning scheme to keep groups of data unfragmented for speedy access. Due to the way ZFS operates, however, none of this archaic knowledge applies (and to be honest, I do not know how ZFS works in detail internally). That said, a hard drive&amp;rsquo;s seek time is around 10ms and has been like that for decades, which combined with the fact that we are now spoiled by having SSDs everywhere, exacerbates how slow an HDD-based solution feels no matter the file system.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-12-13-defrag.png" class="with-border" />
&lt;figcaption>Did I just mention disk fragmentation? Yeah I did, and I could not resist including this screenshot of MS-DOS 6.22's &lt;tt>DEFRAG.EXE&lt;/tt> here. Just because.&lt;/figcaption>
&lt;/figure>
&lt;p>Regarding network connectivity, FreeBSD offers all sorts of networked file systems and services. The base system provides NFSv3, NFSv4, FTP, and iSCSI targets. The ports (packages) system offers whatever else you may need, including SMB, DLNA, and even ancient protocols like AppleTalk or distributed protocols like Ceph. All configuration is done by hand over SSH in the traditional Unix way of editing configuration files&amp;mdash;aka messing around with different, inconsistent text formats.&lt;/p>
&lt;h1 id="synologys-dsm-and-btrfs">Synology&amp;rsquo;s DSM and btrfs&lt;/h1>
&lt;p>The DS923+ runs Synology&amp;rsquo;s own operating system: the &lt;a href="https://www.synology.com/en-us/dsm">DiskStation Manager (DSM)&lt;/a>. The DSM is a headless-first system designed to be accessed over the network, which is no surprise. What &lt;em>is&lt;/em> surprising is the choice of the interface: while most networked devices offer some sort of a web-based tabbed UI, the DSM offers a desktop environment&amp;mdash;with overlapping windows no less. This feels like a gimmick to me, and a quite neat one, but overkill nonetheless.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-12-13-synology-dsm-overview.png" />
&lt;figcaption>The DSM desktop with the &lt;i>Control Panel&lt;/i> and the &lt;i>File Station&lt;/i> apps open to demonstrate the appearance of the web-based windowing system.&lt;/figcaption>
&lt;/figure>
&lt;p>If we peek under the covers, which we can do by logging into the machine over SSH, we find that the DSM is a Linux system. No surprises here after seeing that its choice of file system is btrfs. But what kind of Linux is it? A weird one, let me tell you. Luckily, you do not have to interact with it at all if you don&amp;rsquo;t want to, but hey, I&amp;rsquo;m curious so I did look.&lt;/p>
&lt;p>The DSM seems to be some sort of Debian derivative based on the fact that &lt;code>dpkg&lt;/code> is installed, but otherwise I cannot find any other obvious remains of what might have been a Debian installation; even &lt;code>dpkg -l&lt;/code> shows nothing. What I can tell is that the btrfs file systems are mounted under &lt;code>/volume1&lt;/code> and, in there, we can find one directory per shared folder that we create in the UI. We can also find various directories prefixed with &lt;code>@&lt;/code>, which is a&amp;hellip; weird choice for Unix file names, and I understand are managed by the DSM. These include things like &lt;code>@appdata&lt;/code> or &lt;code>@userpreference&lt;/code>, which carry strong Windows vibes.&lt;/p>
&lt;p>Considering that the DS923+ is almost a PC and runs Linux, I did look to see if it was possible to run FreeBSD instead. It turns out some people have tried this, and FreeBSD does run, but it lacks drivers to control power to the drives so it cannot actually leverage the storage devices. From what I understood, the DS923+ seems to have dedicated hardware to control power the drive pool, and this hardware logic is controlled via GPIO.&lt;/p>
&lt;p>Which got me even more curious: if the DSM explicitly controls power to the drive pool, how does it boot and how does it remain responsive even after shutting down the drives? I guessed that the device comes with a tiny drive to hold the DSM and boots off of it, and upon inspecting the &lt;code>dmesg&lt;/code> and looking for non-obvious stuff, I found this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">sd 7:0:0:0: Attached scsi generic sg3 type 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sd 7:0:0:0: [synoboot] 245760 512-byte logical blocks: (126 MB/120 MiB)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sd 7:0:0:0: [synoboot] Write Protect is off
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sd 7:0:0:0: [synoboot] Mode Sense: 23 00 00 00
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sd 7:0:0:0: [synoboot] No Caching mode page found
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sd 7:0:0:0: [synoboot] Assuming drive cache: write through
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sd 7:0:0:0: [synoboot] Attached SCSI disk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ah-ha. A small 120MB (flash?) drive that acts as the system device. However&amp;hellip; this separation of system vs. drive pool comes with a price: the drive pool often shuts off, and powering it back on is &lt;em>slow&lt;/em> (10&amp;ndash;15 seconds are not unusual). When you try to reach the NAS over the network and the pool is off, it can feel as if the NAS is unreachable / down, which has already confused me a few times and caused me to started diagnosing network issues. Quite annoying to be honest, but obviously this can be configured in the &lt;em>Hardware &amp;amp; Power&lt;/em> menu:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-12-13-synology-dsm-hardware-and-power.png" />
&lt;figcaption>The list of available options under the Hardware &amp; Power menu.&lt;/figcaption>
&lt;/figure>
&lt;p>Regarding network connectivity, the DSM offers multiple networked file systems and services out of the box, namely: SMB, NFS v3, NFS v4, AFP, FTP, and rsync. I&amp;rsquo;m actually surprised to see AFP, AppleTalk&amp;rsquo;s &amp;ldquo;replacement&amp;rdquo;, in the default set of supported file systems in this day and age, but there it is. Additionally, the DSM provides its own package management system to install a limited set of additional services and utilities, which I used to add DLNA support.&lt;/p>
&lt;h1 id="seeding-the-nas">Seeding the NAS&lt;/h1>
&lt;p>Anyhow, let&amp;rsquo;s change topics and talk about the DS923+ itself.&lt;/p>
&lt;p>The first thing I had to do after getting the device was to seed it with data. I needed to copy about 1TB of photos and documents from the FreeBSD machine to the DS923+, which is not a lot, but is large enough that different copy mechanisms can make a huge difference in the time the copy takes. In particular, the choice of &lt;em>protocol&lt;/em> matters for a quick copy: both SMB and NFS are OK with large files, but transferring many small files with them is painful. Fortunately, the DSM allows rsync over SSH and I used that to do the initial seeding.&lt;/p>
&lt;p>Now, using rsync came with two &amp;ldquo;problems&amp;rdquo;. The first is that I had to know the &lt;em>path&lt;/em> to the shared folders in the file system to specify the rsync targets. This is not clearly exposed through the UI, so I had to rely on SSH to log into the machine and figure out that the shared folders are at &lt;code>/volume1/&amp;lt;shared_folder&amp;gt;/&lt;/code> as I briefly mentioned in the previous section.&lt;/p>
&lt;p>Which led to the second problem or, rather, surprise: even though I had set up 2FA for the user accounts I created in the DSM UI, 2FA was meaningless when accessing the machine over SSH. I understand why that may be, but then I question what the point of enabling 2FA really is if one can gain access to the machine without it. The same is true of SMB by the way: you just need an account&amp;rsquo;s password, not the 2FA. So, unless you disable all networked file systems and only allow web access to the NAS, all the 2FA does is give you a false sense of security.&lt;/p>
&lt;h1 id="peace-of-mind">Peace of mind&lt;/h1>
&lt;p>When comparing my custom FreeBSD server to the newer DS923+, the main difference I can notice is an increase in &amp;ldquo;peace of mind&amp;rdquo;. Yes, FreeBSD is very stable and ZFS is great&amp;hellip; but I wasn&amp;rsquo;t running the ThinkStation as a dedicated NAS: I was mixing the machine&amp;rsquo;s NAS responsibilities with those of a host for development tasks. I always felt uncomfortable about the health of the system and I wasn&amp;rsquo;t convinced that my maintenance was &amp;ldquo;good enough&amp;rdquo; to safeguard my data.&lt;/p>
&lt;p>As you can imagine, being uncomfortable about your data is not something you want to feel: trust is &lt;em>The Thing&lt;/em> you most want in a storage solution, and there are a few things that stand out during the setup of the system that gave me warm feelings on this topic.&lt;/p>
&lt;p>The first and obvious one is that the DSM offers to encrypt the pool right from the beginning, which is something you should always do in this day and age: encryption is cheap in CPU terms, the data you own is precious, and the devices that store it tend to be small and light so they are at risk of theft. But beware: the key is stored on the device to allow auto-mounting by default, which means a knowledgeable thief could still gain access. Thus, this level of encryption is only useful to facilitate the disposal of drives. That said, you can configure additional encryption on each shared folder if you want per-user password-protected encryption.&lt;/p>
&lt;p>The second thing is that, nowhere in the setup process I was asked to create a Synology account for the NAS to be fully functional. You might think that this is a given, but seeing the state of other hardware or software products these days&amp;hellip; it&amp;rsquo;s not. So this sent a very welcome message. I ended up needing to create an account for certain features that required it, but these were completely optional.&lt;/p>
&lt;p>The third thing, which is actually one of those features that required creating a Synology account, is the ability to send email notifications for system alerts. Email-based alerts are terrible in large organizations (and of course the DSM offers better alternatives, like ActiveInsight), but for my personal use case, emails are perfect and make a huge difference with my previous FreeBSD setup. I can rest assured that I&amp;rsquo;ll be told about anything unusual with the DS932+ in a way that I will notice. With my custom build, I had certain monitoring features in place like weekly disk scrubs and periodic online backups&amp;hellip; but no way to properly notify me of problems with either: if anything went wrong, I would not have known on time, and that was very unsettling.&lt;/p>
&lt;p>The fourth and final thing is that the DSM has been built throughout the years by people that deal with storage all the time, and it&amp;rsquo;s fair to assume that they know a thing or two about running a NAS. I have reasonable confidence that the configuration of the system and the storage pool is going to be &amp;ldquo;correct&amp;rdquo; over time, particularly across upgrades, whereas I was never quite sure of it with my manual FreeBSD setup. One such example is with the NFS configuration: setting up the NFSv4 server in the DSM was pretty much knob-free&amp;mdash;so &lt;a href="/2024/11/demystifying-secure-nfs.html">when things didn&amp;rsquo;t work with the clients&lt;/a>, I could assume that the issue was almost certainly with the clients themselves.&lt;/p>
&lt;h1 id="additional-apps">Additional apps&lt;/h1>
&lt;p>As I mentioned earlier, the DS923+ is pretty much a PC with Linux in a tiny box, so in principle it can host any kind of software you like. The way this works is via DSM&amp;rsquo;s own &amp;ldquo;marketplace&amp;rdquo; where you can find new services to add to the machine.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-12-13-synology-dsm-package-center.png" />
&lt;figcaption>The DSM desktop showing the &lt;i>Package Center&lt;/i> application.&lt;/figcaption>
&lt;/figure>
&lt;p>I did add the optional DLNA service so that I could play videos from my Xbox, and also toyed with with the Domain Server (and later gave up due to the sheer complexity of setting up a domain just for home use). But there are other interesting optional features. For example, you can create virtual machines on the DS923+ which, despite the limited CPU and RAM of the machine, can come in handy from time to time. I suppose with a more powerful box from Synology, the story here would be quite different.&lt;/p>
&lt;p>One thing to highlight is that these extra pieces of software are &lt;em>curated&lt;/em>: you aren&amp;rsquo;t just installing an extra service to the underlying Linux machine. You are installing an extension to the DSM, which comes with new configuration panels in the UI and full integration with the system. You never have to know that Linux is there if you don&amp;rsquo;t want to.&lt;/p>
&lt;h1 id="backup-solutions">Backup solutions&lt;/h1>
&lt;p>Having a dedicated NAS with a storage pool that protects against corruption is great, but a singly-homed box like this is still subject to massive data loss due to ransomware, physical damage caused by fire or flooding, and correlated disk failures. Backups are critically important.&lt;/p>
&lt;p>With the FreeBSD setup, my backup strategy involved using &lt;code>zfs send&lt;/code> and &lt;code>zfs receive&lt;/code> to back up snapshots into two USB drives: one kept in a fire safe and one kept offsite. This worked but I had to figure out the syntax of these commands over and over again, which didn&amp;rsquo;t make me feel confident about these actions. I did also back up irreplaceable data to a OneDrive account using &lt;code>rclone&lt;/code>, which was good but&amp;hellip; manual and ad-hoc as well. Needless to say, while the backups existed, they were often stale and they were a pain to manage.&lt;/p>
&lt;p>Backing up a NAS is difficult though. For one, a NAS is designed to store &lt;em>lots&lt;/em> of data, which means the backup targets have to be large capacity-wise too. And for another, given the DS923+ reduced physical size, it is likely to end up placed in a location that isn&amp;rsquo;t super convenient to access, which will make attaching temporary USB drives and the like an annoying experience.&lt;/p>
&lt;p>For now, what I have tried is the optional &lt;em>Cloud Sync&lt;/em> package that allows replicating to many cloud services, including OneDrive. And it&amp;rsquo;s a joy to use. I could trivially install the package, log into my Microsoft account, and configure the two specific folders I care about backing up. I could even respect the previous file layout of my OneDrive account so I did not have to re-upload anything. And the tool supports encryption too, which you may want if you really don&amp;rsquo;t like those cloud providers from training their AI systems with your data.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-12-13-synology-dsm-cloud-sync.png" />
&lt;figcaption>Sample configuration of cloud syncing to OneDrive.&lt;/figcaption>
&lt;/figure>
&lt;p>There is more though. Synology offers a variety of products for remote and physical backup. The one that does interest me the most is &lt;a href="https://www.synology.com/en-global/dsm/feature/snapshot_replication">Snapshot Replication&lt;/a>, which provides something similar to what I was doing with &lt;code>zfs send&lt;/code> and &lt;code>zfs receive&lt;/code>, but unfortunately requires a second Synology system offsite that I don&amp;rsquo;t have. The other solutions are &lt;a href="https://sy.to/hf5yf">Active Backup for Business&lt;/a> and &lt;a href="https://sy.to/tntsi">Hyper Backup&lt;/a>, which I still would like to evaluate but haven&amp;rsquo;t had a chance to look into.&lt;/p>
&lt;h1 id="future">Future&lt;/h1>
&lt;p>To conclude this brief review and comparison, let me say that I&amp;rsquo;m happy with the DS923+ experience so far. I don&amp;rsquo;t think I &lt;em>need&lt;/em> it because my FreeBSD solution worked well enough, but considering that I like to use the ThinkStation as the host for my VMs and as my primary development machine&amp;mdash;aka, a fun toy&amp;mdash;I feel more at peace with a dedicated appliance that stores my precious data.&lt;/p>
&lt;p>For more details, you can visit Synology&amp;rsquo;s product pages for the &lt;a href="https://sy.to/hekgh">DS923+&lt;/a> and the &lt;a href="https://sy.to/drkom">Plus Series HDDs&lt;/a>. And if you want to roll your own FreeBSD-based solution, &lt;a href="https://docs.freebsd.org/en/books/handbook/zfs/">Chapter 22 of the FreeBSD handbook on ZFS&lt;/a> is a good place to start.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-12-13-ds923-and-thinkstation.jpg" length="329916" type="image/jpeg"/></item><item><title>Demystifying secure NFS</title><link>https://jmmv.dev/2024/11/demystifying-secure-nfs.html</link><pubDate>Sun, 03 Nov 2024 16:00:00 -0800</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/11/demystifying-secure-nfs.html</guid><description>&lt;p>I recently got a &lt;a href="https://sy.to/hekgh">Synology DS923+&lt;/a> for evaluation purposes which led me to setting up NFSv4 with Kerberos. I had done this about a year ago with FreeBSD as the host, and going through this process once again reminded me of how painful it is to secure an NFS connection.&lt;/p>
&lt;p>You see, Samba is much easier to set up, but because NFS is the native file sharing protocol of Unix systems, I felt compelled to use it instead. However, if you opt for NFSv3 (the &amp;ldquo;easy default&amp;rdquo;), you are left with a system that has zero security: traffic travels unencrypted and unsigned, and the server trusts the client when the client asserts who is who. Madness for today&amp;rsquo;s standards. Yet, when you look around, people say &amp;ldquo;oh, but NFSv3 is fine if you trust the network!&amp;rdquo; But seriously, who trusts the network in this day and age?&lt;/p>
&lt;p>You have to turn to NFSv4 &lt;em>and&lt;/em> combine it with Kerberos for a secure file sharing option. And let me tell you: the experience of setting these up and getting things to work is horrible, and the documentation out there is terrible. Most documents are operating-system specific so they only tell you what works when a specific server and a specific client talk to each other. Other documents just &lt;em>assume&lt;/em>, and thus omit, various important details of the configuration.&lt;/p>
&lt;p>So. This article is my recollection of &amp;ldquo;lab notes&amp;rdquo; on how to set this whole thing up along with the necessary background to understand NFSv4 and Kerberos. My specific setup involes the Synology DS923+ as the NFSv4 server; Fedora, Debian, and FreeBSD clients; and the supporting KDC on a pfSense (or FreeBSD) box.&lt;/p>
&lt;div class="container action-highlight p-4 my-4 d-md-none">
&lt;div class="row text-center">
&lt;p>A blog on operating systems, programming languages, testing, build systems, my own software
projects and even personal productivity. Specifics include FreeBSD, Linux, Rust, Bazel and
EndBASIC.&lt;/p>
&lt;/div>
&lt;div class="row">
&lt;div class="col">
&lt;div class="form-group">
&lt;form action="https://endtracker.azurewebsites.net/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/subscribers/add" method="post">
&lt;input type="text" name="email"
placeholder="Enter your email"
class="form-control input-sm text-center my-1"/>
&lt;button type="submit" class="btn btn-primary btn-block my-1">Subscribe&lt;/button>
&lt;/form>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="row px-2">
&lt;div class="col col-sm-5 text-left">
&lt;small>&lt;span class="subscriber-count">0&lt;/span> subscribers&lt;/small>
&lt;/div>
&lt;div class="col col-sm-7 text-right">
&lt;p>
&lt;a rel="me" href="https://mastodon.online/@jmmv">
&lt;img src="/images/badges/mastodon-logo.svg" width="32px" height="32px" alt="Follow @jmmv on Mastodon">
&lt;/a>
&lt;a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fjmmv.dev%2F&amp;amp;screen_name=jmmv">
&lt;img src="/images/badges/Twitter_logo_blue.svg" width="32px" height="32px" alt="Follow @jmmv on Twitter">
&lt;/a>
&lt;a href="/feed.xml">&lt;img src="/images/badges/feed-icon-28x28.png" alt="RSS feed">&lt;/a>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h1 id="nfsv3s-insecurity">NFSv3&amp;rsquo;s insecurity&lt;/h1>
&lt;p>NFSv3, or usually just NFS, is a protocol from the 1980s&amp;mdash;and it shows. In broad terms, NFSv3 exposes the &lt;em>inodes&lt;/em> of the underlying file system to the network. This became clear to me when I implemented &lt;a href="/software/tmpfs.html">tmpfs for NetBSD&lt;/a> in 2005 and realized that a subset of the APIs I had to support were in service of NFS. This was&amp;hellip; a mind-blowing realization. &lt;em>&amp;ldquo;Why would tmpfs, a memory file system, need NFS-specific glue?&amp;rdquo;&lt;/em>, I thought, and then I learned the bad stuff.&lt;/p>
&lt;p>Anyhow. Now that you know that NFSv3 exposes inodes, you may understand why sharing a directory over NFSv3 is an all-or-nothing option for &lt;em>the whole file system&lt;/em>. Even if you can configure &lt;code>mountd&lt;/code> to export a single directory via &lt;code>/etc/exports&lt;/code>, malicious clients can craft NFS RPCs that reference inodes outside of the shared directory. Which means&amp;hellip; they get free access to the whole file system and explains why system administrators used to put NFS-exported shares in separate partitions, mounted under &lt;code>/export/&lt;/code>, in an attempt to isolate access to subsets of data.&lt;/p>
&lt;p>To make things worse, NFSv3 has no concept of security. A client can simply assert that a request comes from UID 1000 and the server will &lt;em>trust&lt;/em> that the client is really operating on behalf of the server&amp;rsquo;s UID 1000. Which means: a malicious client can pretend to be &lt;em>any&lt;/em> user that exists on the server and gain access to &lt;em>any&lt;/em> file in the exported file system. Which then explains why the &lt;code>maproot&lt;/code> option exists as an attempt to avoid impersonating root&amp;hellip; but only root. Crazy talk.&lt;/p>
&lt;p>All in all, NFSv3 may still be OK if you really trust the network, if you compartmentalize the exported file system, and if you are sharing inconsequential stuff. But can &lt;em>you&lt;/em> trust the network? Maybe you can if you are using a P2P link, but otherwise&amp;hellip; it is really, really risky and I do not want to do that.&lt;/p>
&lt;h1 id="how-is-nfsv4-better">How is NFSv4 better?&lt;/h1>
&lt;p>NFSv4, despite having the same NFS name as NFSv3, is a completely different protocol. Here are two main differences:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>NFSv4 operates on the basis of usernames, not UIDs.&lt;/strong> Each request to the server contains a username and the server is responsible for translating that username to a local UID while verifying access permissions.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>NFSv4 operates at the path level, not the inode level.&lt;/strong> Each request to the server contains the path of the file to operate on and thus the server can apply access control based on those.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Take these two differences together and NFSv4 can implement secure access to file systems. Because the server sees usernames and paths, the server can first verify that a user is who they claim to be. And because the server can authenticate users, it can then authorize accesses at the path level.&lt;/p>
&lt;p>That said, if all you have is NFSv4, you only get the &lt;code>AUTH_SYS&lt;/code> security level, which is&amp;hellip; the same as having no security at all. In this mode, the server trusts the client and assumes that user X on the client maps exactly to user X on the server, which is almost the same as NFSv3 did.&lt;/p>
&lt;p>The real security features of NFSv4 come into play when it&amp;rsquo;s paired with Kerberos. When Kerberos is in the picture, you get to choose from the following security levels for each network share:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>krb5&lt;/code>: Requires requests to be authenticated by Kerberos, which is good to ensure only trusted users access what they should but offers zero &amp;ldquo;on-the-wire&amp;rdquo; security. Traffic flows unsigned and unencrypted, so an attacker could tamper with the data and slurp it before it reaches the client.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>krb5i&lt;/code>: Builds on &lt;code>krb5&lt;/code> to offer integrity checks on all data. Basically, all packets on the network are signed but not encrypted. This prevents spoofing packets but does not secure the data against prying eyes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>krb5p&lt;/code>: Builds on &lt;code>krb5&lt;/code> to offer encrypted data on the wire. This prevents tampering with the network traffic and also avoids anyone from seeing what&amp;rsquo;s being transferred.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Sounds good? Yeah, but unfortunately, Kerberos and its ecosystem are&amp;hellip; complicated.&lt;/p>
&lt;h1 id="kerberos-101">Kerberos 101&lt;/h1>
&lt;p>Kerberos is an authentication broker. Its goal is to detach authentication decisions between a client machine and a service running on a second machine, and move that responsibility to a third machine&amp;mdash;the Kerberos Domain Controller (KDC). Consequently, the KDC is a trusted entity between the two machines that try to communicate with each other.&lt;/p>
&lt;p>All the machines that interact with the KDC form a &lt;em>realm&lt;/em> (AKA a &lt;em>domain&lt;/em>, but not a DNS domain). Each machine needs an &lt;code>/etc/krb5.conf&lt;/code> file that describes which realms the machine belongs to and who the KDC for each realm is.&lt;/p>
&lt;p>The actors that exist within the realm are the &lt;em>principals&lt;/em>. The KDC maintains the authoritative list of principals and their authentication keys (passwords). These principals represent:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Users&lt;/strong>, which have names of the form &lt;code>&amp;lt;username&amp;gt;@REALM&lt;/code>. There has to be one of these principals for every person (or role) that interacts with the system.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Machines&lt;/strong>, which have names of the form &lt;code>host/&amp;lt;machine&amp;gt;.&amp;lt;domain&amp;gt;@REALM&lt;/code>. There has to be one of these principals for every server, and, depending on the service, the clients may need one too as is the case for NFSv4.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Services&lt;/strong>, which have names of the form &lt;code>&amp;lt;service&amp;gt;/&amp;lt;machine&amp;gt;.&amp;lt;domain&amp;gt;@REALM&lt;/code>. Some services like NFSv4 require one of these, in which case &lt;code>&amp;lt;service&amp;gt;&lt;/code> is &lt;code>nfs&lt;/code>, but others like SSH do not.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Let&amp;rsquo;s say Alice wants to log into the Kerberos-protected SSH service running on SshServer from a client called LinuxLaptop, all within the &lt;code>EXAMPLE.ORG&lt;/code> Kerberos realm.&lt;/p>
&lt;p>&lt;em>(Beware that the description below is not 100% accurate. My goal is for you to understand the main concepts so that you can operate a Kerberos realm.)&lt;/em>&lt;/p>
&lt;p>First, Alice needs to obtain a Ticket-Granting-Ticket (TGT) if she doesn&amp;rsquo;t have a valid one yet. This ticket is issued by the KDC after authenticating Alice with her password, and allows Alice to later obtain service-specific tickets without having to provide her password again. For this flow:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-11-03-demystifying-secure-nfs-kerberos-tgt-exchange.png" class="with-border" />
&lt;figcaption>Steps involved in obtaining a TGT from the KDC for a user on a client machine.&lt;/figcaption>
&lt;/figure>
&lt;ol>
&lt;li>
&lt;p>Alice issues a login request to the KDC from the client LinuxLaptop by typing &lt;code>kinit&lt;/code> (or using other tools such as a PAM module). This request carries Alice&amp;rsquo;s password.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The KDC validates Alice&amp;rsquo;s authenticity by checking her password against the KDC&amp;rsquo;s database and issues a TGT. The TGT is encrypted with the KDC&amp;rsquo;s key and includes an assertion of who Alice is and how long the ticket is valid for.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The client LinuxLaptop stores the TGT on disk. Alice can issue &lt;code>klist&lt;/code> to see the ticket:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">linux-laptop$ klist
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Credentials cache: FILE:/tmp/krb5cc_1001
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Principal: alice@EXAMPLE.ORG
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Issued Expires Principal
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Oct 26 09:04:57 2024 Oct 26 19:05:01 2024 krbtgt/EXAMPLE.ORG@EXAMPLE.ORG
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">linux-laptop$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;p>The TGT, however, is not sufficient to access a service. When Alice wants to access the Kerberos-protected SSH service running on the SshServer machine, Alice needs a ticket that&amp;rsquo;s specific to that service. For this flow:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-11-03-demystifying-secure-nfs-kerberos-tgs-exchange.png" class="with-border" />
&lt;figcaption>Steps involved in obtaining a service-specific ticket from the KDC for a user on a client machine.&lt;/figcaption>
&lt;/figure>
&lt;ol>
&lt;li>
&lt;p>Alice sends a request to the Ticket-Granting-Service (KDS) and asks for a ticket to SshServer. This request carries the TGT.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The TGS (which lives in the KDC) verifies who the TGT belongs to and verifies that it&amp;rsquo;s still valid. If so, the TGS generates a ticket for the service. This ticket is encrypted with the service&amp;rsquo;s secret key and includes details on who Alice is and how long the ticket is valid for.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The client LinuxLaptop stores the service ticket on disk. As before, Alice can issue &lt;code>klist&lt;/code> to see the ticket:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">linux-laptop$ klist
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Credentials cache: FILE:/tmp/krb5cc_1001
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Principal: alice@EXAMPLE.ORG
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Issued Expires Principal
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Oct 26 09:04:57 2024 Oct 26 19:05:01 2024 krbtgt/EXAMPLE.ORG@EXAMPLE.ORG
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Oct 26 09:05:11 2024 Oct 26 19:05:01 2024 host/ssh-server.example.org@EXAMPLE.ORG
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">linux-laptop$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;p>At this point, all prerequisite Kerberos flows have taken place. Alice can now initiate the connection to the SSH service:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-11-03-demystifying-secure-nfs-kerberos-ssh-access.png" class="with-border" />
&lt;figcaption>Accessing a remote SSH server using a Kerberos ticket without password authentication.&lt;/figcaption>
&lt;/figure>
&lt;ol>
&lt;li>
&lt;p>Alice sends the login request from the LinuxLaptop client to the SshServer server and presents the service/host-specific ticket that was granted to her earlier on.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The SshServer server decrypts the ticket with its own key, extracts details of who the request is from, and verifies that they are correct. This happens &lt;em>without talking to the KDC&lt;/em> and is only possible because SshServer trusts the KDC via a pre-shared key.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The SSH service on SshServer decides if Alice has SSH access as requested and, if so, grants such access.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Note these very important details:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>The KDC is only involved in the ticket issuance process.&lt;/strong> Once the client has a service ticket, all interactions between the client and the server happen &lt;em>without&lt;/em> talking to the KDC. This is essential to not make the KDC a bottleneck in the communication.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Each host/service and the KDC have unique shared keys that are known by both the host/service and the KDC.&lt;/strong> These shared keys are created when registering the host or service principals and are copied to the corresponding machines as part of their initial setup. These keys live in machine-specific &lt;code>/etc/krb5.keytab&lt;/code> files.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Kerberos does authentication only, not authorization.&lt;/strong> The decision to grant Alice access to the SSH service in Think is made by the service itself, not Kerberos, after asserting that Alice is truly Alice.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>As you can imagine, the KDC must be protected with the utmost security measures. If an attacker can compromise the KDC&amp;rsquo;s locally-stored database, they will get access to all shared keys so they can impersonate any user against any Kerberos-protected service in the network. That&amp;rsquo;s why attackers try to breach into an Active Directory (AD) service as soon as they infiltrate a Microsoft network because&amp;hellip; AD &lt;em>is&lt;/em> a KDC.&lt;/p>
&lt;h1 id="setting-up-the-kdc">Setting up the KDC&lt;/h1>
&lt;p>Enough theory. Let&amp;rsquo;s get our hands dirty and follow the necessary steps to set up a KDC.&lt;/p>
&lt;p>The KDC&amp;rsquo;s needs are really modest. Per the discussion above, the KDC isn&amp;rsquo;t in the hot data path of any service so the number of requests it receives are limited. Those requests are not particularly complex to serve either: at most, there is some CPU time to process cryptographic material but no I/O involved, so for a small network, any machine will do.&lt;/p>
&lt;p>In my particular case, I set up the KDC in my little pfSense box as it is guaranteed to be almost-always online. This is probably not the best of ideas security-wise, but&amp;hellip; it&amp;rsquo;s sufficient for my paranoia levels. Note that most of the steps below will work similarly on a FreeBSD box, but if you are attempting that, please go read &lt;a href="https://docs.freebsd.org/en/books/handbook/security/#kerberos5">FreeBSD&amp;rsquo;s official docs on the topic&lt;/a> instead. Those docs are one of the few decent guides on Kerberos out there.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-11-03-pfsense-router.jpg" />
&lt;figcaption>The pfSense little box that I run the KDC on.&lt;/figcaption>
&lt;/figure>
&lt;p>Here are the actors that will appear throughout the rest of this article. I&amp;rsquo;m using the real names of my setup here because, once again, these are my lab notes:&lt;/p>
&lt;ul>
&lt;li>&lt;code>MEROH.NET&lt;/code>: The name of the Kerberos realm.&lt;/li>
&lt;li>&lt;code>jmmv&lt;/code>: The user on the client machine wanting access to the NFSv4 share. The UID is irrelevant.&lt;/li>
&lt;li>&lt;code>router.meroh.net&lt;/code>: The pfSense box running the KDC.&lt;/li>
&lt;li>&lt;code>nas.meroh.net&lt;/code>: The Synology DS923+ NAS acting as the NFSv4 server.&lt;/li>
&lt;li>&lt;code>think.meroh.net&lt;/code>: A FreeBSD machine that will act as a Kerberized SSH server for testing purposes and an NFSv4 client. (It&amp;rsquo;s a ThinkStation, hence its name.)&lt;/li>
&lt;li>&lt;code>x1nano.meroh.net&lt;/code>: A Linux machine that will act as an NFSv4 client. While in reality this is running Fedora, I&amp;rsquo;ll use this hostname interchangeably for Fedora and Debian.&lt;/li>
&lt;/ul>
&lt;p>Knowing all actors, we can set up the KDC. The first step is to create the &lt;code>krb5.conf&lt;/code> for the KDC which tells the system which realm the machine belongs to. You&amp;rsquo;ll have to open up SSH access to the machine via the web interface to perform these steps.&lt;/p>
&lt;p>Here is the minimum content you need:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">[libdefaults]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> default_realm = MEROH.NET
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">[realms]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">MEROH.NET = {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> kdc = router.meroh.net
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> admin_server = router.meroh.net
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">[domain_realm]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> .meroh.net = MEROH.NET
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With that, you should be able to start the &lt;code>kdc&lt;/code> service, which is responsible for the KDC. All documentation you find out there will tell you to also start &lt;code>kadmind&lt;/code>, but if you don&amp;rsquo;t plan to do administer the KDC from another machine (why would you?), then you don&amp;rsquo;t need this service.&lt;/p>
&lt;p>pfSense&amp;rsquo;s configuration is weird because of the read-only nature of its root partition, so to do this, you have to edit the &lt;code>/cf/conf/config.xml&lt;/code> file stored in NVRAM and add this line right before the closing &lt;code>&amp;lt;/system&amp;gt;&lt;/code> tag:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-xml" data-lang="xml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">&amp;lt;shellcmd&amp;gt;&lt;/span>service kdc start&lt;span class="nt">&amp;lt;/shellcmd&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you were to set this up on a FreeBSD host instead of pfSense, you would modify &lt;code>/etc/rc.conf&lt;/code> instead and add:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">kdc_enable&lt;/span>&lt;span class="o">=&lt;/span>YES
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, from the root shell on either case:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">kdc# service kdc start
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kdc# █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is now a good time to ensure that every machine involved in the realm has a DNS record and that reverse DNS lookups work. Failure to do this will cause problems later on when attempting to mount the NFSv4 shares, and clearing those errors won&amp;rsquo;t be trivial because of caching at various levels.&lt;/p>
&lt;h1 id="creating-principals">Creating principals&lt;/h1>
&lt;p>Once the KDC is running, we must create principals for the hosts, the NFSv4 service, and the users that will be part of the realm. The client host and service principals aren&amp;rsquo;t always necessary though: SSH doesn&amp;rsquo;t require them, but NFSv4 does.&lt;/p>
&lt;p>To create the principals, we need access the KDC&amp;rsquo;s administrative console. Given that the KDC isn&amp;rsquo;t configured yet, we can only gain such access by running &lt;code>kadmin -l&lt;/code> on the KDC machine directly (the pfSense shell), which bypasses the networked &lt;code>kadmind&lt;/code> service that we did not start.&lt;/p>
&lt;p>Start &lt;code>kadmin -l&lt;/code> and initialize the realm:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">kdc# kadmin -l
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; init MEROH.NET
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">... answer questions with defaults ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, create principals for the &lt;em>users&lt;/em> that will be part of the realm:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; add jmmv
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">... answer questions with defaults ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">... but enter the desired user password ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, create principals for the &lt;em>hosts&lt;/em> (server and clients, but not the KDC) and the NFSv4 &lt;em>service&lt;/em>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; add --random-key host/think.meroh.net
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">... answer questions with defaults ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; add --random-key host/x1nano.meroh.net
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">... answer questions with defaults ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; add --random-key host/nas.meroh.net
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">... answer questions with defaults ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; add --random-key nfs/nas.meroh.net
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">... answer questions with defaults ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And finally, extract the host and service credentials into the machine-specific keytab files. Note that, for the servers, we extract both the host and any service principals they need, but for the client, we just extract the host principal. We do not export any user principals:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; ext_keytab --keytab=think.keytab host/think.meroh.net
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; ext_keytab --keytab=x1nano.keytab host/x1nano.meroh.net
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; ext_keytab --keytab=nas.keytab host/nas.meroh.net nfs/nas.meroh.net
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kadmin&amp;gt; █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You now need to copy each extracted keytab file to the corresponding machine and name it &lt;code>/etc/krb5.keytab&lt;/code>. (We&amp;rsquo;ll do this later on the Synology NAS via its web interface.) This file is what contains the shared key between the KDC and the host and is what allows the host to verify the authenticity of KDC tickets without having to contact the KDC. Make sure to protect it with &lt;code>chmod 400 /etc/krb5.keytab&lt;/code> so that nobody other than root can read it.&lt;/p>
&lt;p>If &lt;code>scp&lt;/code> is unsuitable or hard to use from the KDC to the client machines (as is my case because I restrict SSH access to the KDC to one specific machine), you can use the &lt;code>base64&lt;/code> command to print out a textual representation of the keytab and use the local clipboard to carry it to a shell session on the destination machine.&lt;/p>
&lt;h1 id="initial-client-setup">Initial client setup&lt;/h1>
&lt;p>At this point, the realm should be functional but we need to make the clients become part of the realm. We also need to install all necessary tools, like &lt;code>kinit&lt;/code>, which aren&amp;rsquo;t present by default on some systems:&lt;/p>
&lt;ul>
&lt;li>On Debian:
&lt;ul>
&lt;li>Run &lt;code>apt install krb5-user nfs-common&lt;/code>.&lt;/li>
&lt;li>Follow the prompts that the &lt;code>krb5-user&lt;/code> installer shows to configure the realm and the address of the KDC. This will auto-create &lt;code>/etc/krb5.conf&lt;/code> with the right contents so you don&amp;rsquo;t have to do anything else.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>On Fedora:
&lt;ul>
&lt;li>Run &lt;code>dnf install krb5-workstation&lt;/code>.&lt;/li>
&lt;li>Edit the system-provided &lt;code>/etc/krb5.conf&lt;/code> file to register the realm and its settings. Use the file content shown above for the KDC as the template, or simply replace all placeholders for &lt;code>example.org&lt;/code> and &lt;code>EXAMPLE.ORG&lt;/code> with the name of your DNS domain and realm.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>On FreeBSD:
&lt;ul>
&lt;li>Create the &lt;code>/etc/krb5.conf&lt;/code> file from scratch in the same way we did for the KDC.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>All set! But&amp;hellip; do you trust that you did the right thing everywhere? We could go straight into NFSv4, but due to the many pitfalls in its setup, I&amp;rsquo;d suggest you verify your configuration using a simpler service like SSH.&lt;/p>
&lt;p>To do this, modify the SSH server&amp;rsquo;s (aka &lt;code>think&lt;/code>&amp;rsquo;s configuration) &lt;code>/etc/ssh/sshd_config&lt;/code> file and add &lt;code>GSSAPIAuthentication yes&lt;/code> so that it can leverage Kerberos for authentication. Restart the SSH service and give it a go: run &lt;code>kinit&lt;/code> on the client (&lt;code>x1nano&lt;/code>) and then see how &lt;code>ssh think&lt;/code> works without typing a password anymore.&lt;/p>
&lt;p>But&amp;hellip; &lt;code>GSSAPIAuthentication&lt;/code>? What&amp;rsquo;s up with the cryptic name?&lt;/p>
&lt;p>GSS-API stands for &lt;em>Generic Security Services API&lt;/em> and is the interface that programs use to communicate with the Kerberos implementation on the machine. GSS-API is not always enabled by default for a service, and the way you enable it is service-dependent. As you saw above, all we had to do for SSH was modify the &lt;code>sshd_config&lt;/code> file&amp;hellip; but for other services, you may need to take extra steps on the server and/or the client.&lt;/p>
&lt;p>And, guess what, NFSv4 is weird on this topic. Not only we need service-specific principals for NFS, but we also need the &lt;code>gssd&lt;/code> &lt;em>daemon&lt;/em> to be running on the server &lt;em>and&lt;/em> the client machines. This is because NFSv4 is typically implemented inside the kernel, but not Kerberos, so the kernel needs a mechanism to &amp;ldquo;call into&amp;rdquo; Kerberos. And the kernel needs to do this to map kernel-level UIDs (a Unix kernel doesn&amp;rsquo;t know anything about &lt;em>usernames&lt;/em>) to Kerberos principals and vice-versa&amp;mdash;and that&amp;rsquo;s precisely what &lt;code>gssd&lt;/code> offers. So:&lt;/p>
&lt;ul>
&lt;li>On the Synology NAS:
&lt;ul>
&lt;li>Do nothing. The system handles &lt;code>gssd&lt;/code> by itself.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>On Linux:
&lt;ul>
&lt;li>You shouldn&amp;rsquo;t have to do anything if you correctly created the prerequisite &lt;code>/etc/krb5.keytab&lt;/code> early enough, but make sure the service is running with &lt;code>systemctl status rpc-gssd.service&lt;/code> (and know that this command only shows useful diagnostic logs when run as root).&lt;/li>
&lt;li>Run &lt;code>systemctl start rpc-gssd.service&lt;/code> if the service isn&amp;rsquo;t running.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>On FreeBSD:
&lt;ul>
&lt;li>Add &lt;code>gssd_enable=YES&lt;/code> to &lt;code>/etc/rc.conf&lt;/code>.&lt;/li>
&lt;li>Run &lt;code>service gssd start&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="exporting-nfsv4-services">Exporting NFSv4 services&lt;/h1>
&lt;p>It&amp;rsquo;s time to deal with NFSv4, so let&amp;rsquo;s start by configuring the server on the NAS.&lt;/p>
&lt;p>The Synology Disk Station Manager (DSM) interface&amp;mdash;the web UI for the NAS&amp;mdash;is&amp;hellip; interesting. As you might expect, it is a web-based interface but&amp;hellip; it pretends to be a desktop environment in the browser, which I find overkill and unnecessary. But it&amp;rsquo;s rather cool in its own way.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-11-03-synology-dsm-nfs-file-services.png" class="with-border" />
&lt;figcaption>Navigating the Synology DSM menus to configure the NFS file service with Kerberos.&lt;/figcaption>
&lt;/figure>
&lt;p>The first step is to enable the NFS service. Roughly follow these steps, which are illustrated in the picture just above:&lt;/p>
&lt;ul>
&lt;li>Open the &lt;strong>File Services&lt;/strong> tab of the &lt;strong>Control Panel&lt;/strong>.&lt;/li>
&lt;li>In the &lt;strong>NFS&lt;/strong> tab, set NFSv4 as the &lt;strong>Minimum NFS protocol&lt;/strong>.&lt;/li>
&lt;li>Click on &lt;strong>Advanced Settings&lt;/strong> and, in the panel that opens, enter the Kerberos realm under the &lt;strong>NFSv4 domain&lt;/strong> option.&lt;/li>
&lt;li>Click on &lt;strong>Kerberos Settings&lt;/strong> and, in the panel that opens, select &lt;strong>Import&lt;/strong> and then upload the keytab file that we generated earlier on for the NAS. This should populate the &lt;code>host&lt;/code> and &lt;code>nfs&lt;/code> principals in the list.&lt;/li>
&lt;li>Finish and save all settings.&lt;/li>
&lt;/ul>
&lt;p>That should be all to enable NFSv4 file serving.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-11-03-synology-dsm-nfs-shared-folder.png" class="with-border" />
&lt;figcaption>Navigating the Synology DSM menus to configure the properties of a single shared folder over NFS.&lt;/figcaption>
&lt;/figure>
&lt;p>Then, we need to expose shared folders over NFSv4, and we have to do this for every folder we want to share. Assuming you want to share the &lt;code>homes&lt;/code> folder as shown in the picture just above:&lt;/p>
&lt;ul>
&lt;li>Open the &lt;strong>Shared Folder&lt;/strong> tab of the &lt;strong>Control Panel&lt;/strong>.&lt;/li>
&lt;li>Select the folder you want to share (in our case, &lt;code>homes&lt;/code>), and click &lt;strong>Edit&lt;/strong>.&lt;/li>
&lt;li>In the &lt;strong>NFS Permissions&lt;/strong> tab, click either &lt;strong>Create&lt;/strong> or &lt;strong>Edit&lt;/strong> to enter the permissions for every host client that should have access to the share.&lt;/li>
&lt;li>Fill the NFS rule details. In particular, enter the hostname of the client machine, enable the &lt;strong>Allow connections from non-privileged ports&lt;/strong> option, and select the &lt;strong>Security&lt;/strong> level you desire.&lt;/li>
&lt;/ul>
&lt;p>In my case, I want &lt;code>krb5p&lt;/code> and &lt;code>krb5p&lt;/code> only so that&amp;rsquo;s the only option I enable. But your risk profile and performance needs may be different, so experiment and see what works best for you.&lt;/p>
&lt;h1 id="mounting-nfsv4-from-linux">Mounting NFSv4 from Linux&lt;/h1>
&lt;p>Now that the server is ready and we have dealt with the GSS-API prerequisites, we can start mounting NFSv4 on the clients.&lt;/p>
&lt;p>On Linux, things are pretty simple. We can mount the file system with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">x1nano# sudo mount nas:/volume1/homes /shared
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">x1nano# █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or persist the entry in &lt;code>/etc/fstab&lt;/code> if we want to:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">nas:/volume1/homes /shared nfs sec=krb5p 0 0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And then we should be able to list its contents &lt;em>assuming we&amp;rsquo;ve got a valid TGT for the current user&lt;/em> (run &lt;code>kinit&lt;/code> if it doesn&amp;rsquo;t work):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">x1nano$ ls -l /shared
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">total 0K
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">drwxrwxrwx 1 nobody users 0 Sep 27 21:11 admin
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">drwxrwxrwx 1 jmmv users 0 Nov 2 20:41 jmmv
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">drwxrwxrwx 1 nobody users 0 Oct 8 16:59 manager
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">x1nano$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Easy peasy, right? But wait&amp;hellip; why do all directories have &lt;code>777&lt;/code> permissions?&lt;/p>
&lt;p>This is rather unfortunate and I&amp;rsquo;m not sure why the Synology does this. Logging onto the DS923+ via SSH, I inspected the shared directory and realized that it has various ACLs in place to control access to the directories, but somehow, the traditional Unix permissions are all &lt;code>777&lt;/code> indeed. Not great.&lt;/p>
&lt;p>I used &lt;code>chmod&lt;/code> to fix the permissions for all directories to &lt;code>755&lt;/code> and things seem to be OK, but that doesn&amp;rsquo;t give me a lot of comfort because I do not know if the DSM will ever undo my changes or if I might have broken something.&lt;/p>
&lt;p>There might be one more problem though, which I did not encounter on Debian clients but that showed up later in Fedora and FreeBSD clients:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">x1nano$ ls -l /shared
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">total 0K
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">drwxr-xr-x 1 nobody nogroup 0 Sep 27 21:11 admin
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">drwxr-xr-x 1 nobody nogroup 0 Nov 2 20:41 jmmv
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">drwxr-xr-x 1 nobody nogroup 0 Oct 8 16:59 manager
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">x1nano$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note how all entries are owned by &lt;code>nobody:nogroup&lt;/code> which is&amp;hellip; not correct. Yet the right permissions are in effect: accessing the &lt;code>jmmv&lt;/code> directory is only possible by the &lt;code>jmmv&lt;/code> user as expected. Which means that the user mapping between Kerberos principals and local users is working correctly on the server&amp;hellip; but not on the client, where &lt;code>stat&lt;/code> isn&amp;rsquo;t returning the right information.&lt;/p>
&lt;p>I do not yet know why this issue happens, especially because I see no material differences between my Fedora and Debian configurations.&lt;/p>
&lt;h1 id="mounting-nfsv4-from-freebsd">Mounting NFSv4 from FreeBSD&lt;/h1>
&lt;p>We now have the Linux clients running just fine so it is time to pivot to FreeBSD. If we try a similar &amp;ldquo;trivial&amp;rdquo; mount command, we get an error:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">think# mount -t nfs nas:/volume1/homes /shared
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">mount_nfs: nmount: /shared: Permission denied
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">think# █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The error is pretty&amp;hellip; unspecific. It took me quite a bit of trial and error to realize that I &lt;em>had&lt;/em> to specify &lt;code>-t nfsv4&lt;/code> for it to attempt a NFSv4 connection and not NFSv3 (unlike Linux, whose &lt;code>mount&lt;/code> command attempts the highest possible version first and then falls back to older versions):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">think# mount -t nfs -o nfsv4 nas:/volume1/homes /shared
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">mount_nfs: nmount: /shared, wrong security flavor
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">think# █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>OK, progress. Now this complains that the security flavor we request is wrong. Maybe we just need to be explicit and also pass &lt;code>sec=krb5p&lt;/code> as an argument:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">think# mount -t nfs -o nfsv4,sec=krb5p nas:/volume1/homes /shared
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">mount_nfs: nmount: /shared, wrong security flavor
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">think# █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Wait, what? The mount operation still fails? This was more puzzling and also took a fair bit of research to figure out because logs on the client and on the server were just insufficient to see the problem.&lt;/p>
&lt;p>The reason for the failure is that we are trying to mount the share as root but&amp;hellip; we don&amp;rsquo;t have a principal for this user so root cannot obtain an NFSv4 service ticket to contact the NAS. So&amp;hellip; do we need to create a principal for &lt;code>root&lt;/code>? No! We do not need to provide user credentials when mounting an NFSv4 share (unlike what you might be used to with Windows shares).&lt;/p>
&lt;p>What Kerberized NFSv4 needs during the mount operation is a host ticket: the NFSv4 server checks if the client &lt;em>machine&lt;/em> is allowed to access the server and, if so, exposes the file system to it. This is done using the client&amp;rsquo;s host principal. Once the file system is mounted, however, all operations against the share carry the ticket of the &lt;em>user&lt;/em> requesting the operation.&lt;/p>
&lt;p>Knowing this, we need to &amp;ldquo;help&amp;rdquo; FreeBSD and tell it that it must use the host&amp;rsquo;s principal when mounting the share. Why this isn&amp;rsquo;t the default, I don&amp;rsquo;t know, particularly because non-root users are not allowed to mount file systems in the default configuration. Anyhow. The &lt;code>gssname=host&lt;/code> option rescues us:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">think# mount -t nfs -o nfsv4,sec=krb5p,gssname=host nas:/volume1/homes /shared
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">think# █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Which finally allows the mount operation to succeed. We should persist all this knowledge into an &lt;code>/etc/fstab&lt;/code> entry like this one:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">nas:/volume1/homes /shared nfs rw,nfsv4,gssname=host,sec=krb5p 0 0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="verifying-encryption">Verifying encryption&lt;/h1>
&lt;p>Color me skeptical, but everything I described above seems convoluted and fragile, so I did not trust that my setup was sound. Consequently, I wanted to verify that the traffic on the network was actually encrypted.&lt;/p>
&lt;p>To verify this, I installed Wireshark and ran a traffic capture against the NAS with &lt;code>host nas&lt;/code> as the filter. Then, from the client, I created a text file on the shared folder and then read it. Inspecting the captured packets confirmed that the traffic is indeed flowing in encrypted form. I could not find the raw file content anywhere in the whole trace (but I could when using anything other than &lt;code>krb5p&lt;/code>).&lt;/p>
&lt;figure>
&lt;img src="/images/2024-11-03-kerberized-nfs-wireshark.png" class="with-border" />
&lt;figcaption>Content of an NFS reply packet with Kerberos-based encryption. The packet contents are not plain text.&lt;/figcaption>
&lt;/figure>
&lt;p>And, as a final test, I tried to mount the network share &lt;em>without&lt;/em> &lt;code>krb5p&lt;/code> and confirmed that this was not possible:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl"># mount -t nfs -o nfsv4,gssname=host,sec=krb5i nas:/volume1/homes /shared
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">mount_nfs: nmount: /shared, wrong security flavor
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>All good! I think&amp;hellip;&lt;/p>
&lt;h1 id="open-questions">Open questions&lt;/h1>
&lt;p>That&amp;rsquo;s about it. But I still have a bunch of unanswered questions from this setup:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Kerberos claims to be an authentication system only, not an authorization system. However, the protocol I described above separates the TGT from the TGS, and this separation makes it sound like Kerberos could also implement authorization policies. Why doesn&amp;rsquo;t it do these?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The fact that Fedora and FreeBSD show &lt;code>nobody&lt;/code> for file ownership even when they seems to do the right thing regarding when talking to the NFSv4 server sound like a bug either in the code or in my configuration. Which is it?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Having to type &lt;code>kinit&lt;/code> after logging into the machine is annoying. I remember that, back at Google when we used Kerberos and NFS&amp;mdash;those are long gone days&amp;mdash;the right tickets would be granted after logging in or unlocking a workstation. This must have been done with the Kerberos PAM modules&amp;hellip; but I haven&amp;rsquo;t gotten them to do this yet and I&amp;rsquo;m not sure why.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The fact that the shared directories created by the Synology NAS have 777 permissions seems wrong. Why is it doing that? And does anything break if you manually tighten these permissions?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>And the most important question of all: &lt;strong>is this all worth it?&lt;/strong> I&amp;rsquo;m tempted to just use password-protected Samba shares and call it a day. I still don&amp;rsquo;t trust that the setup is correct, and I still encounter occasional problems here and there.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>If you happen to have answers to any of the above or have further thoughts, please drop a note in the comments section. And if you want to see how the DS923+ compares to my home-built FreeBSD NAS with ZFS, subscribe for a future post on that!&lt;/p>
&lt;p>Credit and disclaimers: the &lt;a href="https://sy.to/hekgh">DS923+&lt;/a> and &lt;a href="https://sy.to/drkom">the 3 drives&lt;/a> it contains that I used throughout this article were provided to me for free by Synology for evaluation purposes in exchange for blogging about the NAS. The content in this article is &lt;em>not&lt;/em> endorsed has &lt;em>not&lt;/em> been reviewed by them.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-11-03-synology-ds923plus-nas.jpg" length="232646" type="image/jpeg"/></item><item><title>BazelCon 2024 recap</title><link>https://jmmv.dev/2024/10/bazelcon-2024-recap.html</link><pubDate>Tue, 22 Oct 2024 08:00:00 -0700</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/10/bazelcon-2024-recap.html</guid><description>&lt;p>Just like that, BazelCon 2024 came and went. And just like that, Blog System/5 is about to turn 1 year old as the very first post of this newsletter was the &lt;a href="/2023/10/bazelcon-2023-et-al-trip-report.html">recap of BazelCon 2023&lt;/a>. Since then, this newsletter has amassed 1200+ subscribers and I have surpassed &lt;a href="/2024/06/20-years-of-blogging.html">20 years of blogging&lt;/a>&amp;mdash;so, thank you for your support, everyone!&lt;/p>
&lt;p>To celebrate this milestone, it&amp;rsquo;s obviously time to summarize the two events of last week: BazelCon 2024 and the adjacent Build Meetup. There is &lt;em>A LOT&lt;/em> of ground to cover. I could probably write a separate article for each of the sections below, but folks coming for a summary of the conference will want to see everything in one place&amp;hellip; so you get this massive piece instead.&lt;/p>
&lt;p>Overall, this is a 40-minute read&amp;hellip; but let&amp;rsquo;s face it: getting 3 days worth of content in less than an hour sounds like a good deal, doesn&amp;rsquo;t it? Feel free to pick and choose sections though; each stands on its own and each paragraph represents a thought I captured from some presentation or discussion.&lt;/p>
&lt;p>By the way: no LLMs were involved in this work, thus you can only imagine how much effort it went into putting this together. So, subscribe to support this and future posts and&amp;hellip; enjoy!&lt;/p>
&lt;div class="container action-highlight p-4 my-4 d-md-none">
&lt;div class="row text-center">
&lt;p>A blog on operating systems, programming languages, testing, build systems, my own software
projects and even personal productivity. Specifics include FreeBSD, Linux, Rust, Bazel and
EndBASIC.&lt;/p>
&lt;/div>
&lt;div class="row">
&lt;div class="col">
&lt;div class="form-group">
&lt;form action="https://endtracker.azurewebsites.net/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/subscribers/add" method="post">
&lt;input type="text" name="email"
placeholder="Enter your email"
class="form-control input-sm text-center my-1"/>
&lt;button type="submit" class="btn btn-primary btn-block my-1">Subscribe&lt;/button>
&lt;/form>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="row px-2">
&lt;div class="col col-sm-5 text-left">
&lt;small>&lt;span class="subscriber-count">0&lt;/span> subscribers&lt;/small>
&lt;/div>
&lt;div class="col col-sm-7 text-right">
&lt;p>
&lt;a rel="me" href="https://mastodon.online/@jmmv">
&lt;img src="/images/badges/mastodon-logo.svg" width="32px" height="32px" alt="Follow @jmmv on Mastodon">
&lt;/a>
&lt;a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fjmmv.dev%2F&amp;amp;screen_name=jmmv">
&lt;img src="/images/badges/Twitter_logo_blue.svg" width="32px" height="32px" alt="Follow @jmmv on Twitter">
&lt;/a>
&lt;a href="/feed.xml">&lt;img src="/images/badges/feed-icon-28x28.png" alt="RSS feed">&lt;/a>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h1 id="schedule-and-logistics">Schedule and logistics&lt;/h1>
&lt;p>BazelCon 2024 was hosted at the Computer History Museum in Mountain View, CA, on October 14th and 15h. The conference had a &lt;a href="https://bazelcon2024.sched.com/">single track of talks&lt;/a> and an overlapping track of BoF sessions. I attended most of the talks but just one BoF&amp;mdash;the IDE one. The conference was followed by evening socials hosted by BuildBuddy and JetBrains/EngFlow.&lt;/p>
&lt;p>The Build Meetup followed BazelCon 2024 on October 16th, and this time around it was cohosted by Meta and EngFlow in Menlo Park, CA. The meetup had three tracks: one for Bazel, one for Buck 2, and one for remote execution. I helped facilitate the Bazel track, including taking notes for the afternoon unconference (notes at the very end), and gave a 15-minute talk on Bazel at Snowflake. Facilitating the track was fun, but unfortunately this made me miss the Buck track and I&amp;rsquo;m still wanting to learn more about it.&lt;/p>
&lt;p>The notes I gathered from the talks and discussions are grouped in the following topics:&lt;/p>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#schedule-and-logistics">Schedule and logistics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#community-and-adoption">Community and adoption&lt;/a>&lt;/li>
&lt;li>&lt;a href="#remote-execution">Remote Execution&lt;/a>&lt;/li>
&lt;li>&lt;a href="#ide-support">IDE support&lt;/a>&lt;/li>
&lt;li>&lt;a href="#inner-loop-development">Inner loop development&lt;/a>&lt;/li>
&lt;li>&lt;a href="#toolchains">Toolchains&lt;/a>&lt;/li>
&lt;li>&lt;a href="#sandboxing">Sandboxing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#monorepo-issues">Monorepo issues&lt;/a>&lt;/li>
&lt;li>&lt;a href="#blzmod-and-external-dependencies">blzmod and external dependencies&lt;/a>&lt;/li>
&lt;li>&lt;a href="#secure-and-auditable-builds">Secure and auditable builds&lt;/a>&lt;/li>
&lt;li>&lt;a href="#symbolic-macros">Symbolic macros&lt;/a>&lt;/li>
&lt;li>&lt;a href="#queries">Queries&lt;/a>&lt;/li>
&lt;li>&lt;a href="#linting">Linting&lt;/a>&lt;/li>
&lt;li>&lt;a href="#testing">Testing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#missing-tooling-around-bazel">Missing tooling around Bazel&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-end">The end&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;h1 id="community-and-adoption">Community and adoption&lt;/h1>
&lt;p>The conference opened with the usual briefing and SOTU, which included information on the conference itself and the latest news in the Bazel project. These were followed by talks that touched upon the community and current adoption trends, and these are the thoughts I gathered:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Conference ownership:&lt;/strong> This was the first BazelCon &lt;em>not&lt;/em> run by Google. The conference is now owned by the Linux Foundation and Google was a sponsor at the same level of BuildBuddy and EngFlow. The Linux Foundation has run the conference excellently thanks to the dedicated team of professionals that they have for the task.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Conference history:&lt;/strong> Looking back, you should know that the Bazel team was (and still is) distributed across two sites: Google NYC and Google Munich. To mitigate the difficulties that arise from geographical distribution, the team held internal-only summits every 6 months. It wasn&amp;rsquo;t until 2017 that this internal conference became BazelCon and, after that, the team alternated between an internal event and an external event. Last year, BazelCon had 250 attendees with a waitlist that contained 200 extra folks, and this is the second year that the program committee contains non-Google folks (myself included).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Google&amp;rsquo;s investment in Bazel:&lt;/strong> Does this shift to the Linux Foundation change &lt;a href="https://bazel.build/contribute/policy">Bazel&amp;rsquo;s governance&lt;/a> to make it more community-driven? Not yet, but it&amp;rsquo;s a step in that direction! Google still owns Bazel and continues to invest in it because they can leverage contributions from strong non-Google engineers and because Google maintains a bunch of open-source projects that rely on Bazel. But Google also wants to have a more open community to guarantee that Bazel continues to thrive even if company-internal priorities change.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>The &lt;a href="https://github.com/bazel-contrib">&lt;code>bazel-contrib&lt;/code> organization&lt;/a>:&lt;/strong> To support these changes, a few folks have created a new GitHub organization and have gotten Google to donate many repos to this new org. The organization creators have a desire to start a new foundation to support and direct these projects and are looking for about 5 companies to join.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Bazel adoption stickiness:&lt;/strong> The REBELs research group at the &lt;a href="https://rebels.cs.uwaterloo.ca/">Univesity of Waterloo&lt;/a> conducted a study over &lt;a href="https://is.gd/WorldOfCode/">35,000 significant GitHub projects&lt;/a> to see why, from the 1.5% of projects that adopted Bazel, 11% of those abandoned the migration at roughly the 2-year mark.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Abandonment reasons:&lt;/strong> The reasons cited for abandoning Bazel were varied, but can be summarized as encountering technical challenges, issues with team coordination and onboarding, and seeing community trends (like Kubernetes deciding to move off of Bazel).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>If not Bazel, then what?:&lt;/strong> These projects primarily decided to move to language-specific tools, especially for languages with strong native tooling like Go and Swift. However, a significant proportion also decided to move back to &amp;ldquo;inferior&amp;rdquo; tools like CMake or even GNU Make. These projects acknowledged that these tools are less feature rich and don&amp;rsquo;t support integration with other languages, but they are conventional and easier to understand by the community.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>If Bazel, then why?:&lt;/strong> On the plus side, for the cases where Bazel stuck, we can find the reasons we expect: dependency management, faster builds, and even the influence of other projects shine as reasons for making Bazel a good choice.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Adoption suggestions:&lt;/strong> The CTO of Ergatta spoke on how his small company has been able to successfully leverage Bazel without massive investments in infrastructure. His suggestions were to keep things simple (e.g. by using the GCS-backed cache despite its flaws, or by doing the trivial &lt;code>bazel test ...&lt;/code> on CI); to expedite solutions even if not perfect (e.g. by wrapping foreign builds in Bazel); to look for champions and leverage them for adoption; to focus on &amp;ldquo;good enough&amp;rdquo; results; to write documentation for your successors; and, surprisingly, to do frequent and incremental changes to workflows instead of big-bang &amp;ldquo;improvements&amp;rdquo;.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="remote-execution">Remote Execution&lt;/h1>
&lt;p>As you know, remote execution is Bazel&amp;rsquo;s raison d&amp;rsquo;etre. Consequently, the SOTU covered various updates on this topic and many talks included related thoughts:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>SOTU updates:&lt;/strong> The remote output service is now available and bb-clientd offers an implementation. The execution log is now 100x smaller than before and only has a 3% runtime overhead, which is useful to debug cache hits. Upcoming changes include concurrent uploads to the cache in the background without blocking action completion, GCing of local caches (disk cache, install cache, etc.), and BwoB improvements to decrease incremental build times when Skymeld is in use.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Remote persistent workers:&lt;/strong> AirBnB observed 2x slower builds when not using remote persistent workers. They use dedicated pools for Java and Kotlin and route all actions into these pools, and reminded us that tagging targets comes with pitfalls because targets can spawn multiple actions and just one of them needs the worker.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Queuing control:&lt;/strong> Remote execution environments typically have different worker pools to support different build environments or to optimize cost. But note: the goal shouldn&amp;rsquo;t always be to tolerate all possible load all the time: it is reasonable to put limits on the worker pools like AirBnB does so that you can dedicate a pool for interactive builds and prioritize low latency, but also create a separate pool for CI builds where you put hard caps and tolerate queuing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Target size matters:&lt;/strong> Dealing with tons of individual small files is costly. In some cases, you may be better off tarring them up and declaring a single input to an action and making the action unpack the tar during execution. This came up during the talk on high performance builds for web monorepos, and I found this interesting because this goes in the opposite direction of tree artifacts which have often been problematic.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Action deduplication:&lt;/strong> Any remote execution service worth its money can coalesce in-flight actions to save on cost. This shines in CI with builds that have long-running actions, and Canva reports that they see 500k dedups per day on about 3 webpack builds. One consideration is that flaky test passes/failures are amplified by action coalescing: without this feature, flakes will show up as random failures over time, whereas with this feature, failures will be clustered. Extracting a signal around flakiness becomes harder.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Determinism checks:&lt;/strong> While not necessarily an issue with remote execution, having remote execution exacerbates the problem of non-deterministic actions in a build. Spend the time to set up a CI job to &lt;a href="https://www.youtube.com/watch?v=XItY0LmdiFA">look for non-determinism&lt;/a> and alert on it. Specific problems that often arise are timestamps in JARs (&lt;code>rules_antrl&lt;/code> is impacted), absolute paths (&lt;code>rules_kotlin&lt;/code> is impacted), or diagnostics information (Scala &lt;code>sdeps&lt;/code> file is problematic).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Postmortems and learnings:&lt;/strong> Ulf Adams gave a talk reflecting on four different incidents that EngFlow&amp;rsquo;s remote execution service suffered. Personally, seeing companies introspect their failures makes &lt;em>me&lt;/em> trust them &lt;em>more&lt;/em> than not, but YMMV. In any case, I do not necessarily want to go over the incidents per se, but I do want to go over the recommendations:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Avoid RPC cycles and set timeouts:&lt;/strong> The call graph between services must not have cycles, which is hard to enforce because these may show up organically over time. Also make sure to have timeouts on all RPCs, although the RE protocol makes this difficult because it has long-running RPCs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Make sure &lt;a href="https://grpc.io/docs/guides/flow-control/">flow control&lt;/a> is configured:&lt;/strong> This is a feature of gRPC that allows a server to keep memory consumption in check and is achieved by &amp;ldquo;slowing down&amp;rdquo; incoming traffic (thus &amp;ldquo;controlling the flow&amp;rdquo;). When proxying traffic in a server, special care must be taken to connect the flow control logic of the input and output streams of the proxy. Otherwise, a slow client can have ripple effects through the system and cause OOMs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Auto-scaling is tricky to get right:&lt;/strong> From a cost-savings perspective, you want to downsize servers as quickly as possible and increase them slowly. But if you do this and there is queuing, it&amp;rsquo;s possible that auto-scaling will make queuing worse.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Guardrails build trust:&lt;/strong> You might say that user-induced problems are not service problems: after all, if, for example, they cause compile actions to time out after 20 minutes, it&amp;rsquo;s &amp;ldquo;their fault&amp;rdquo; for doing the wrong thing. However, if this happens, there obviously is something wrong&amp;mdash;and it&amp;rsquo;d be nice for the service provider to notice and have prevention features in place just like your water provider can detect leaks. (And yet&amp;hellip; AWS &lt;em>still&lt;/em> doesn&amp;rsquo;t have a way to put a limit on spend.)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="ide-support">IDE support&lt;/h1>
&lt;p>If you have had to support any developers converting from &amp;ldquo;legacy&amp;rdquo; build systems to Bazel, you may have realized that&amp;hellip; the IDE features they are used to don&amp;rsquo;t quite work after the migration. Things have gotten better in the IDE space for Bazel but aren&amp;rsquo;t great yet. Fortunately, there are a few news that give hope:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>&lt;a href="https://jb.gg/new-bazel-plugin">New JetBrains plugin for IntelliJ&lt;/a>:&lt;/strong> Released to the marketplace during the conference, this new plugin provides tighter integration between IntelliJ and Bazel through the BSP abstraction layer. This new plugin can represent the Bazel build graph in the IDE, offers syntax highlighting for &lt;code>bazelrc&lt;/code> files (and, feature request: could offer docs on hover), supports inserting breakpoints in Starlark evaluation, implements &amp;ldquo;fast build&amp;rdquo; correctly, and optimizes the sync process. There are some gaps compared to the old plugin, but I&amp;rsquo;m super-excited by what&amp;rsquo;s coming because the old Google-owned plugin is&amp;hellip; underwhelming. See the &lt;a href="https://jb.gg/new-bazel-feature">feature list&lt;/a> and the &lt;a href="https://jb.gg/new-bazel-roadmap">roadmap&lt;/a> for more details.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Build Server Protocol (BSP):&lt;/strong> The BSP is a new protocol that aims to achieve the same thing that the Language Server Protocol (LSP) did: namely, solve the M:N problem between IDEs and build systems. The idea is to have an intermediate abstraction for the build system so that M IDEs can talk to N build systems by means of an intermediary process that converts the BSP to build actions. The BSP is still young though and, right now, it&amp;rsquo;s pretty rigid because it has deep knowledge of the languages it supports. In particular, there is no support for custom rules yet, so get involved if you want to see this happen.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Old plugin for JetBrains:&lt;/strong> The old plugin will be fully supported until mid-2025 (although I predict they&amp;rsquo;ll have to backpedal on this and extend this date). This plugin has many shortcomings because it originates from Android Studio and contains assumptions about Android and about Google&amp;rsquo;s own infrastructure. That said, there is a new feature called &amp;ldquo;query sync&amp;rdquo; that optimizes the sync process massively&amp;hellip; at the expense of having to manually &amp;ldquo;enable analyze&amp;rdquo; for any files where deep IDE integration is desired. The problem is: you &lt;em>have&lt;/em> to enable analysis to get insights on generated files, and generated files tend to be everywhere thanks to protobuf, so&amp;hellip; you&amp;rsquo;ll likely find yourself &amp;ldquo;enabling analyze&amp;rdquo; all the time. Shrug.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Good IDE support is critical:&lt;/strong> Having a good IDE experience is crucial for developers, but the irony is that the people that often work on build migrations or the IDE are &lt;em>experts&lt;/em> and know how to tolerate imperfect IDE support. This is not the case for most developers, particularly those that must get up to speed&amp;hellip; so keep that in mind. Running user interviews, surveys, etc. is a necessity to understand what people truly perceive as problems.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Compilation database:&lt;/strong> JetBrains is not the only contender in the IDE space. There are many more (wink, wink, Emacs) and BSP will make offering a Bazel experience within them possible. Until that&amp;rsquo;s ready, though, you may need to manually deal with a &amp;ldquo;compilation database&amp;rdquo;, especially if you want to integrate with VSCode. A compilation database is, simply put, a JSON file that contains the commands to compile each and every translation unit in a project. There are various options to generate one of these with Bazel, all with different trade-offs:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Intercept builds:&lt;/strong> Use &lt;code>bear&lt;/code> as a wrapper to run the full clean build. This works with some build systems, but unfortunately, Bazel&amp;rsquo;s client/server model doesn&amp;rsquo;t allow &lt;code>bear&lt;/code> to intercept the actions it spawns.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Extra actions:&lt;/strong> Add a &amp;ldquo;listener&amp;rdquo; extra action that listens for &lt;code>CppCompile&lt;/code>. This also requires a full clean build and is deprecated by aspects, but is the approach that &lt;code>kythe&lt;/code> uses.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Action graph query:&lt;/strong> Does not require a full build, just a warmed up analysis cache. However, this does not support tree artifacts. Examples: &lt;code>bazel-compile-commands-extractor&lt;/code>, &lt;code>bazel-compile-commands&lt;/code> (two forks).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Aspect:&lt;/strong> Does not require a full build, but the generated commands may not be identical to the ones that are actually used. For example: if you have a code generator that produces a tree artifact and then is fed as an input to a &lt;code>cc_library&lt;/code>, then the tree artifact is represented as a &lt;code>CppCompileActionTemplate&lt;/code> that is only expanded to a specific command line at runtime.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Non-blocking queries:&lt;/strong> The Bazel server has a global lock that prevents it from running more than one command at a time. This is a problem because the IDE needs to issue Bazel queries all the time, but those queries conflict with other user actions like builds and tests. There are various solutions to this problem, which we discussed in the unconference:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Separate output bases:&lt;/strong> This &lt;em>works&lt;/em> but it&amp;rsquo;s heavy-handed because you end up with two full separate builds and two separate Bazel server processes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Simpler tools:&lt;/strong> Many operations on build files do not require the full weight of Bazel. Google has implemented simpler tools that operate on those precisely to bypass the overhead of calling ito Bazel. These tools include &lt;code>buildifier&lt;/code>, &lt;code>buildozer&lt;/code>, or the &amp;ldquo;fast builds&amp;rdquo; feature of the IntelliJ plugin.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Parallel Skyframe evaluation:&lt;/strong> Skyframe&amp;rsquo;s inherent design is to be purely functional, so in principle it should be possible to perform multiple operations on the graph in parallel. Unfortunately, there is a lot of mutable state outside of Skyframe in Bazel and, while theoretically possible, fixing this is &lt;em>a lot&lt;/em> of work.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Implementing a depserver:&lt;/strong> Instead of running a separate Bazel on the same machine to answer queries, you could push this responsibility to an external service. Such a service is easy to build (you can imagine running &lt;code>bazel query ...&lt;/code> on each commit) but the problem arises when you want to issue queries against this service from modified source trees.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="inner-loop-development">Inner loop development&lt;/h1>
&lt;p>Bazel is &lt;em>the&lt;/em> tool that glues together the &lt;em>inner loop&lt;/em> of the development process. As a reminder, the inner loop refers to the the edit, build, and test cycle, and Bazel has tentacles on these three stages. Various talks gave thoughts on this topic:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Avoid Bazel in the inner loop:&lt;/strong> AirBnB reported that they noticed significant overhead and lack of incrementality in certain operations when trying to hook up Bazel into the IDE. They have the equivalent of &amp;ldquo;fast builds&amp;rdquo; in their own IDE plugin and have reduced incremental builds from 30 seconds to 1 second. This is a &lt;em>big deal&lt;/em> for interactive builds. Personally, I think that having to side-step Bazel is ridiculous: Bazel is designed to be optimal and has perfect knowledge of the state of the world, yet it&amp;rsquo;s too slow for quick operations.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Integrate with native tooling:&lt;/strong> Bazel works across many ecosystems, but as such, it&amp;rsquo;s friend to none. For example: Go developers want to work with the (super-fast) Go tooling so, if it&amp;rsquo;s feasible, it&amp;rsquo;s interesting to allow using such tooling directly. The folks at LinkedIn created a rule that generates an &lt;code>.envrc&lt;/code> file to expose the native Go tools and relies on &lt;a href="https://direnv.net/">&lt;code>direnv&lt;/code>&lt;/a> to make this work transparently for users.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>macOS as a client is common:&lt;/strong> Even for shops where all software runs on Linux, developers tend to have Macs and want to work locally. You may or may not agree, but if you &lt;em>have&lt;/em> to support inner-loop development on the Mac, there are various things to consider. One is that cross-platform caching for machine-independent actions like Java may not work, doubling cache requirements; for this, you may consider using &amp;ldquo;universal binaries&amp;rdquo; (aka shell scripts that wrap multiple binaries and choose the right one at runtime). And you should really set up non-determinism checks.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>No build files:&lt;/strong> To my dismay, tons of people seem to really want to &lt;em>not&lt;/em> have build files. This makes me sad because manually-curated build files serve as &lt;em>documentation&lt;/em> for the &lt;em>conceptual architecture&lt;/em> of a project and allow, at PR review time, to see changes to the interactions between components. You might say that &lt;code>#include&lt;/code>s or package &lt;code>import&lt;/code>s in the source are sufficient for this&amp;mdash;but they really aren&amp;rsquo;t: they are too fine-grained and &amp;ldquo;innocent-looking&amp;rdquo;. I think I&amp;rsquo;ll need to write a follow-up article on this point.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="toolchains">Toolchains&lt;/h1>
&lt;p>Bazel supports targeting multiple architectures and systems, and at the core of this support lie toolchains. There were various talks that touched on them:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Toolchains 101:&lt;/strong> In Bazel, rules convert providers to action templates, and toolchains help convert those templates to actions by replacing two things in the templates: the paths to the tools required by the action, and the arguments to pass to those tools. Simple, right? But defining toolchains has always been a black art.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Simplified toolchains:&lt;/strong> Google brings a new way to define toolchains that&amp;rsquo;s much easier than the original one. The new mechanism relies on build rules: a toolchain rule declares a toolchain, and the toolchain depends on a &amp;ldquo;tool map&amp;rdquo; rule, which is a flat map of tool names (strategically specified as labels for typo- and type-checking, not flat strings) to binaries (also specified as labels). These tools are also connected to argument rules that define the sequence of arguments to apply to each tool. The support for &amp;ldquo;features&amp;rdquo; in core Bazel can potentially be removed in favor of this.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Default toolchain no more:&lt;/strong> Following on the previous, Google said that there is a desire to remove the concept of a default C++ toolchain in favor of explicitly defining a hermetic toolchain like almost all other rules do. This was met with cheering from the audience.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Debugging:&lt;/strong> It&amp;rsquo;s funny how, despite the complexity of toolchains and how we should make things easier for users&amp;hellip; &lt;code>--toolchain_resolution_debug&lt;/code>&amp;rsquo;s help claims that the flag is only for experts. &lt;a href="https://github.com/bazelbuild/bazel/pull/19926">PR 19926&lt;/a> made the debugging messages much clearer, but you should still understand the toolchain resolution algorithm to make sense of issues, and the talk on &amp;ldquo;Creating C++ toolchains easily&amp;rdquo; explained that.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Universal binaries:&lt;/strong> Bazel doesn&amp;rsquo;t like sharing artifacts across different architectures. This is generally the right thing to do but, for platform-agnostic languages like Java, this has the potential of doubling remote execution and caching costs. There is an &lt;a href="https://github.com/bazelbuild/bazel/discussions/18378">ongoing discussion upstream&lt;/a> on what to do with this issue and various partial implementations in the code (like the &lt;code>--experimental_platform_in_output_dir&lt;/code> flag), but no great solution yet. In the meantime, the alternative is to implement &amp;ldquo;universal binaries&amp;rdquo;. The idea is to create a shell script that wraps a binary tool built for various systems and selects, at runtime, which one to run.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>C++ shows up everywhere:&lt;/strong> Even if your build doesn&amp;rsquo;t seem to require C++ anywhere, the requirement to have this toolchain installed shows up pretty much everywhere because of&amp;hellip; protobuf. bzlmod has the potential to fix this because the protobuf bzlmod packages ship with prebuilt binaries.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="sandboxing">Sandboxing&lt;/h1>
&lt;p>In order to offer reproducible builds, Bazel offers sandboxing features at the action level to ensure that actions can only do what they are supposed to do. I used to work in this space while on the Bazel team, so all talks on this topic were really interesting. Here are the notes:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>macOS sandboxing is slow:&lt;/strong> Ah, the issue that never dies and that came up again in the unconference. While it might be true that macOS handles symlinks poorly, I&amp;rsquo;m still not convinced that there is anything that makes macOS sandboxing inherently slow. Yes, it will be less sandbox-y than what you get on Linux, but performance-wise it should be OK. All of my testing years ago didn&amp;rsquo;t show &lt;code>sandbox-exec&lt;/code> as a performance bottleneck, and the sandbox doesn&amp;rsquo;t kill Bazel&amp;rsquo;s own build performance so&amp;hellip; something &lt;em>else&lt;/em> is at play here. Which brings us to the next point.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Persistent state:&lt;/strong> Some compilers like Objective C&amp;rsquo;s build a module cache as they run. This cache is intended to be shared across invocations of the compiler. However, when the sandbox is in effect, sharing of this cache becomes impossible, which in turn makes builds incredibly slow. It may &lt;em>seem&lt;/em> as if the sandbox itself is slow, but the real problem here is the lack of state sharing across actions and I suspect this is why people remain convinced that macOS sandboxing is slow. And, by the way, this impacts more than just Objective C.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Sandboxing scenarios:&lt;/strong> Is it really necessary to sandbox &lt;em>all&lt;/em> rules? &lt;code>genrule&lt;/code>s, sure, but if we can reason about the behavior of specific rules, we can probably disable sandboxing for them and remain correct, which would decrease build-time overheads. (Remember that the sandbox was never about security.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>File monitoring:&lt;/strong> An alternative to sandboxing is to monitor the activity of build actions and validate, post-build, that they didn&amp;rsquo;t access things they were not supposed to. This is much faster than sandboxing as it avoids the need to construct on-disk sandboxes&amp;hellip; but fails when tools decide to read directories. So&amp;hellip; this approach isn&amp;rsquo;t really feasible. JavaScript tooling loves to expand globs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Hermetic sandboxing:&lt;/strong> The traditional Bazel sandbox does not restrict reading system-wide paths so it cannot guarantee that an action doesn&amp;rsquo;t access certain system headers or arbitrary tools in &lt;code>/bin&lt;/code>. There is a &amp;ldquo;new&amp;rdquo; hermetic sandbox for Linux that fixes these issues&amp;mdash;but, obviously, is hard to put in practice. See &lt;code>--experimental_use_hermetic_linux_sandbox&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Base POSIX system:&lt;/strong> With a hermetic sandbox, it becomes very tempting to model the host system&amp;rsquo;s base libraries and tools (bash and the like) as a toolchain. &lt;a href="https://github.com/tweag/rules_sh">&lt;code>rules_sh&lt;/code>&lt;/a> from Tweag can help here.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Local remote execution:&lt;/strong> A different mechanism to achieve sandboxing would be to use remote execution against a local service. This service could run on a VM or similar and offer a mechanism to cross-compile against weird architectures. This wouldn&amp;rsquo;t be quite the same as the Docker strategy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Separation of responsibilities:&lt;/strong> Somebody mentioned that it&amp;rsquo;d be neat to separate the sandboxing logic into its own thing that isn&amp;rsquo;t in Bazel&amp;rsquo;s core. I agree. The &lt;code>linux-sandbox&lt;/code> binary that Bazel contains is somewhat akin to this, and I previously implemented something similar in &lt;a href="https://github.com/jmmv/sandboxctl">&lt;code>sandboxctl&lt;/code>&lt;/a> (for very different reasons). In any case, this would need to be multi-platform and written in a decent, safe language (&lt;em>cough&lt;/em> Rust &lt;em>cough&lt;/em>).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="monorepo-issues">Monorepo issues&lt;/h1>
&lt;p>With scale come problems, and large monorepos tend to exacerbate issues that have always existed in small repos but that go unnoticed otherwise. Here are some of the issues that talks mentioned:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Glob handling:&lt;/strong> Globs are expensive to handle in Bazel, especially when running on networked file systems. Globs are parsed and then expressed as a DAG of functions within Skyframe. Right now, due to limitations in Java&amp;rsquo;s concurrency model, globs are evaluated twice, but Java 21&amp;rsquo;s green threads may alleviate this. Related, I was chatting about this with one of the Buck 2 authors and he said that they take a very different approach and don&amp;rsquo;t find the issues that Google described: basically, they traverse the file system once and then apply the glob as an in-memory only operation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Monolithic targets:&lt;/strong> Any large piece of code that has grown without a build system that enforces boundaries across components will grow at least one component that has thousands of source files in it with cyclic dependencies among them. These are harmful to the organization as they tend to reflect the &amp;ldquo;broken windows&amp;rdquo; culture of the team. Tinder explained how they tackle this problem, with basically boils down to: ensuring there are tests, ensuring that leadership is onboard with the cleanup, creating automation to extract files from the monolith (most of the work is repetitive), and then creating visualization tools to graph the problem and identify subsets of files that can be easily extracted.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Gradual rollout of library updates:&lt;/strong> During the unconference, we discussed how to deal with version upgrades of a single library in a world that favors the &amp;ldquo;One version policy&amp;rdquo;.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Prerequisites:&lt;/strong> We agreed that, to succeed at upgrading a library, it&amp;rsquo;s critical to have tests in place and leverage tooling like &lt;code>buildifier&lt;/code> to perform the upgrades automatically where possible.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Big bang upgrade:&lt;/strong> One approach to the problem is to do the version upgrade in one go. This is sometimes really hard to do, and can also be risky because any problem anywhere in the repo would imply a rollback for everyone.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Treat it as an exception:&lt;/strong> Another approach is to get an exception from the &amp;ldquo;One version policy&amp;rdquo; and temporarily allow two versions of the library in the repo. You can leverage the &lt;code>alias&lt;/code> rule to introduce an indirection to the version in use, and then slowly migrate packages. This can work but diamond conflicting dependencies can be a real problem.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Parallel build:&lt;/strong> Yet another approach is to create a parallel build and test infrastructure for the new version. Maybe use a build flag to select the version to use; maybe duplicate the targets and rename them. The goal with this approach is to keep the &amp;ldquo;new version&amp;rdquo; on the side until it all works, and then submit a trivial &amp;ldquo;3-line PR&amp;rdquo; that flips the default. For this scenario, &amp;ldquo;project files&amp;rdquo; would be useful to group the new set of targets and automatically set the right configuration for them; Buck 2 has something similar in its &lt;code>PACKAGE&lt;/code> construct.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Different package names:&lt;/strong> And another approach is to do what Java tends to do, which is to &lt;em>rename&lt;/em> the libraries on major version upgrades so that they do not conflict at all. This is something that the upstream library maintainers have to do, not you. The problem with this approach is that semantic version is not always accurate and upstream maintainers tend to do breaking changes during minor version upgrades without realizing it.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>git pain:&lt;/strong> Bazel and large monorepos go hand in hand, but Git&amp;mdash;the most popular version control system&amp;mdash;doesn&amp;rsquo;t like large repos very much: some operations scale with the size of the history (&lt;code>git clone&lt;/code>) while others scale with the size of the repo (&lt;code>git status&lt;/code>) instead of scaling with the size of the change.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Mitigation tools:&lt;/strong> There are various options that can mitigate the problem, including the &lt;a href="https://git-scm.com/docs/git-fsmonitor--daemon">FSMonitor&lt;/a>, the &amp;ldquo;untracked cache&amp;rdquo;, keeping &lt;code>.gitignore&lt;/code> small, and using sparse checkouts.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Materializing a portion of the tree:&lt;/strong> Uber previously tried using a Bazel query to find all files required for a build and then using the result of the query to just check those files out from Git. While this can work, it led to confusing error messages when there were missing files. And also, this poses the question: where do you run the query given that the query needs access to the full repo?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>git archives:&lt;/strong> Git is a very chatty protocol. Mark Zeren from Broadcom reports that, while Perforce can easily saturate the network, Git can rarely do so on a similar repo. The slowdown is even visible when using the loopback network interface. Prebuilding &lt;code>git-archive&lt;/code>s periodically and using those to build the initial state of a Git clone can significantly improve performance. He mentioned that it&amp;rsquo;d be good to build a service to automate this, and offered help in doing so.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Delegating file accesses:&lt;/strong> There is an open feature request in &lt;a href="https://github.com/bazelbuild/bazel/issues/16380">#16380&lt;/a> asking for a way to delegate file accesses to another tool. This, to me, sounds like FUSE. Someone brought up that FUSE doesn&amp;rsquo;t work well on macOS anymore and that NFS can be used as an alternative, and Meta&amp;rsquo;s EdenFS or Buildbarn&amp;rsquo;s bb-clientd&amp;rsquo;s do that just fine. Another option is to implement a custom VFS instance within Bazel to directly talk to a remote file system.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Resource consumption:&lt;/strong> Bazel&amp;rsquo;s memory usage is&amp;hellip; troublesome. First, because Bazel uses a lot of memory, and second because of the JVM&amp;rsquo;s heap limits. Newer Bazel versions were able to reduce &lt;em>retained&lt;/em> heap memory by 95% after a build and 25-30% overall during a build&amp;hellip; but that&amp;rsquo;s not very useful because &lt;em>peak&lt;/em> memory consumption is what matters in many cases. The team is looking at various alternate solutions to tackle this problem:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Skyfocus:&lt;/strong> Bazel 8 contains a new feature called Skyfocus, in experimental state. The idea is to perform GC based on a user-defined working set. This is useful in large monorepos where users want to only work on a small portion of the tree, but there are questions about the usability aspects and UX of this solution.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Skeletal analysis:&lt;/strong> The goal of this work is to try to change the evaluation model to reduce peak memory by 20%. The idea is to trade memory usage for wall time, which means this will be useful for CI but could be harmful for interactive builds.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>(Remote) Analysis caching:&lt;/strong> The loss of the Bazel analysis cache across reconfigurations or server restarts has always been a problem and a major usability annoyance, so this is finally being looked at. The goal is to be able to cache configured targets, aspects, action execution results, and other Bazel-internal state, possibly storing this information in the &amp;ldquo;standard&amp;rdquo; remote cache by using a special key. This could also help with a ~70% heap and runtime reduction for analysis phases from cold start, and it could be used to prune entire subgraphs during the execution phase. This would be a massive improvement for IDE interactions, as IDEs primarily rely on analysis-time operations only.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Distributed analysis:&lt;/strong> The idea is to shard analysis across Bazel workers. This sounds&amp;hellip; OK for gigantic monorepos like Google&amp;rsquo;s but I&amp;rsquo;m not sure I see the use case outside of that environment. Still in very early stages of discussion.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="blzmod-and-external-dependencies">blzmod and external dependencies&lt;/h1>
&lt;p>The &lt;code>WORKSPACE&lt;/code> in Bazel has always been a wart: Google did &lt;em>not&lt;/em> have this feature in Blaze but the need to support out-of-tree third-party dependencies became glaringly obvious when they open-sourced Bazel. The &lt;code>WORKSPACE&lt;/code> was then bolted on and we have been suffering from its deficiencies for years. bzlmod promises to fix them all, and of course there were talks (and a BoF) on it:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>SOTU dependency updates:&lt;/strong> bzlmod now provides a vendor mode to download all dependencies of a target or all targets, which is very useful for CI/CD offline builds (e.g. to exercise disaster recovery). &lt;code>MODULE.bazel&lt;/code> now has include support and &lt;code>use_repo_rule&lt;/code> for easier migration. The lock file format has been revamped to minimize merge conflicts. The &lt;code>WORKSPACE&lt;/code> is disabled by default in Bazel 8 and won&amp;rsquo;t be available in Bazel 9. As for future plans, the repo cache will be shared across workspaces and will include the results of evaluation, not just downloads.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>SOTU rule updates:&lt;/strong> The Python, Android, Java, and shell rules have all moved to Starlark. Protobuf and Android support have moved as well, and this comes with new things like &amp;ldquo;mobile install v3&amp;rdquo;. The goal for next year is to complete the Starlarkification effort (with possible new extension points to call into Bazel from the Build API).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>SOTU Starlark and the Build API:&lt;/strong> All struct providers are gone; aspects can propagate to target toolchains; and there are now dormant dependencies for tests. As for upcoming changes: symbolic macros in Bazel 8 (more later); rule finalizers; and types for Starlark (which are compatible with Python 3 but &lt;em>not&lt;/em> with Buck).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Handling internal artifacts:&lt;/strong> It is common for vendors to provide binary blobs that don&amp;rsquo;t integrate with Bazel. Azul&amp;rsquo;s JDK is an example. For these, it is interesting to consume them into the build via a workspace repository, but the question then becomes how to do this with bzlmod. The answer is to &amp;ldquo;layer multiple registries&amp;rdquo;: you&amp;rsquo;d have a company-internal registry that exposes these binaries, and then fall back to the BCR for everything else.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Disabling the BCR:&lt;/strong> For companies that want to have tight control on what they access during the build, it&amp;rsquo;s perfectly possible to bypass the BCR, either by pointing Bazel to an internal registry or by vendoring sources. The &lt;code>bazel vendor&lt;/code> subcommand, which I didn&amp;rsquo;t know about, can come in handy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Missing packages:&lt;/strong> There are some glaring omissions from the BCR right now, which include googleapis, gRPC, and &lt;code>rules_scala&lt;/code>. The maintainers of the former are not very cooperative, but progress is being made. As for gRPC, there is a desire to avoid pulling in support for all languages all the time, which makes this technically harder.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="secure-and-auditable-builds">Secure and auditable builds&lt;/h1>
&lt;p>Maintaining the integrity of the software provenance chain is difficult and &amp;ldquo;recent&amp;rdquo; events like the &lt;a href="https://en.wikipedia.org/wiki/XZ_Utils_backdoor">XZ Utils Backdoor&lt;/a> have shown the criticality of, at least, having an audit trail of what goes into software builds. This is where SBOM and other techniques come into play, and Bazel-based builds are perfectly positioned to provide these assurances:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>SBOMs:&lt;/strong> There are many different formats to produce a &lt;a href="https://www.ntia.gov/page/software-bill-materials">Software bill of materials (SBOM)&lt;/a>, and a few of them like CycloneDX and SPDX are standardized. It&amp;rsquo;s important that these files are &amp;ldquo;merge-able&amp;rdquo; so that, when you import a third-party dependency, you can assemble their own SBOM.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Types of targets:&lt;/strong> It&amp;rsquo;s tricky to generate the SBOM, and not all rules to generate one did or do the right thing. For example: do all deps belong in the SBOM? What about deps that say &lt;code>neverlink=1&lt;/code>? What about build-time only deps like compilers?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Rules:&lt;/strong> Bazel has had a &lt;code>licenses&lt;/code> package-level function for a long time, but it is insufficient to generate an SBOM. The newer &lt;a href="https://github.com/bazelbuild/rules_license">&lt;code>rules_license&lt;/code>&lt;/a> offers a much more complete solution to tracking licenses and generating SBOMs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Build assurance:&lt;/strong> Other than declaring licenses, it&amp;rsquo;s important to be able to verify &lt;em>where&lt;/em> builds came from. &lt;a href="https://slsa.dev/">SLSA&lt;/a> defines various levels that capture different requirements. At the very least, you should target SLSA 2, which requires builds to be produced from trusted environments and to prevent arbitrary users from tampering with those builds (e.g. only allowing uploads to the Action Cache by CI). SLSA 3 is even more strict but RE doesn&amp;rsquo;t yet provide a mechanism to implement it; there are some talks to support it in V3 and maybe backport it to V2, but no concrete plans yet.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="symbolic-macros">Symbolic macros&lt;/h1>
&lt;p>Macros are problematic: they tend to cause targets to require public visibility; they tend to pollute the list of available targets in a package (the output of &lt;code>bazel query&lt;/code> is unusable); and they have the potential of bloating the build graph massively. There was a full talk from Google on a new feature that&amp;rsquo;ll make these less of an issue:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>New feature:&lt;/strong> Bazel 8 ships with &lt;em>symbolic macros&lt;/em>, which are first-class citizens of the build graph. Symbolic macros are declared similarly to rules: instead of specifying &lt;code>rule()&lt;/code>, you specify &lt;code>macro()&lt;/code> and provide a set of attributes and an implementation function. The names of the targets and outputs that these macros produce are constrained to a set of well-defined patterns.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Limitations:&lt;/strong> One known deficiency is that &lt;code>**kwargs&lt;/code> doesn&amp;rsquo;t work as it used to because there is no way to specify this in an attribute list. The way to mitigate this and make it easier to wrap rules will be attribute inheritance. Similarly, while symbolic macros can lead to lazy expansion, this is not yet implemented. Both of these limitations will be addressed later in the 8.x series.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Epilogs:&lt;/strong> Legacy macros used to call &lt;code>native.existing_rules()&lt;/code> and lead to the &amp;ldquo;epilog macro&amp;rdquo; idiom that can be found in large monorepos. These macros are expected to appear at the end of a build file&amp;hellip; but there is no way to enforce this. With symbolic macros, this idiom is now a supported feature via the &lt;code>finalizer=True&lt;/code> attribute, which then makes these macros be expanded &lt;em>after&lt;/em> the full build file has been processed irrespective of any other targets.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Downsides:&lt;/strong> Symbolic macros were well-received by conference attendees but have received some pushback inside of Google. The reason is that, because these macros will allow lazy evaluation, it&amp;rsquo;s possible that they&amp;rsquo;ll encourage even larger packages that only become problematic in some (critical) scenarios.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Legacy macros:&lt;/strong> There are no plans to remove legacy macros right now&amp;mdash;and even if there were, it&amp;rsquo;d take years to eliminate them due to the need to remain backwards-compatible for multiple releases.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="queries">Queries&lt;/h1>
&lt;p>For people coming from other build systems, the ability to query a build graph may sound like an alien concept. But Bazel has first-class features to do just this, and they can come in handy when writing automation to modify the source tree or to select tests for execution.&lt;/p>
&lt;p>There was a full 30-minute talk dedicated to queries. I do not intend to repeat everything that was said because everything is in &lt;a href="https://bazel.build/query/language">the query documentation&lt;/a>&amp;hellip; but I did learn a bunch of stuff:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Target provenance:&lt;/strong> &lt;code>bazel query --output=build ...&lt;/code> shows &amp;ldquo;fictitious&amp;rdquo; attributes like &lt;code>generator_{name,function,location}&lt;/code> which indicate what produced a target. Useful to understand what macros and globs are doing. These attributes can also be used in filters.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Variables:&lt;/strong> Queries can have variables in them to avoid repeating complex parts. For example: &lt;code>let v = foo/... in allpaths($v, //common) intersect $v&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Removing implicit dependencies:&lt;/strong> These often pollute query results with noise and can be removed with &lt;code>--noimplicit_deps&lt;/code>. Arguably, this flag should be the default.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Graphing:&lt;/strong> It is possible to generate a Graphviz file by using &lt;code>--output=graph&lt;/code> and then graphing it with &lt;code>dot -Tsvg -o graph.svg&lt;/code>. It is important to filter the query or otherwise the resulting graph is overwhelming&amp;mdash;aka useless.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Running code on the graph:&lt;/strong> The &lt;code>cquery&lt;/code> command supports &lt;code>--output=starlark --starlark:expr=...&lt;/code> which allows running a piece of Starlark code on every target selected by the query. The current target is bound to the &lt;code>target&lt;/code> variable. This is useful to, for example, query transitive runtime JARs. And because Starlark is almost Python, you can explore the API by using the &lt;code>dir&lt;/code> function with expressions like &lt;code>dir(target)&lt;/code> or &lt;code>dir(target.actions)&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Action queries:&lt;/strong> The &lt;code>aquery&lt;/code> command is a different beast from the other queries and allows inspecting the action graph. Something like &lt;code>bazel aquery 'mnemonic(CppCompiler, //...)'&lt;/code> shows all inputs, outputs, and command lines for every action. This can be particularly useful to understand toolchain configuration changes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Filtering by file:&lt;/strong> The &lt;code>outputs&lt;/code> and &lt;code>inputs&lt;/code> filters can be used to filter actions by paths: e.g. you can find actions that produce a particular file with &lt;code>outputs(&amp;quot;.*path/to/h&amp;quot;, deps(//...)))&lt;/code>. (Beware that the path filter is an anchored regexp.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Performance:&lt;/strong> Running many queries in a loop doesn&amp;rsquo;t scale. Consider building a bigger query if possible and then do post-processing (the opposite advice of when talking to a database server), or using an aspect to dump all info in a single run.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="linting">Linting&lt;/h1>
&lt;p>Back at Google, Jin and I hosted an intern circa 2017 to work on &amp;ldquo;autolint&amp;rdquo;: an innovative tool that was supposed to automate running arbitrary lint checks on arbitrary codebases. The project never materialized&amp;hellip; but Aspect.dev just announced &lt;a href="https://github.com/aspect-build/rules_lint">&lt;code>rules_lint&lt;/code>&lt;/a> which basically does the same thing and integrates with Bazel neatly.&lt;/p>
&lt;p>To summarize the talk, &lt;code>rules_lint&lt;/code> provides two disjoint features in one:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Formatting:&lt;/strong> The goal of formatting is to change a codebase to adhere to predefined standards and stop the bitchering about white space. A quote from the talk I liked was &amp;ldquo;because while your code might be art&amp;hellip; it isn&amp;rsquo;t ASCII art&amp;rdquo;.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Broad support:&lt;/strong> Supporting formatters is easy, and there are formatters for almost all languages, so a goal here is to try to support as many as possible.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Speed:&lt;/strong> Formatting must be deterministic and fast so that it can be hooked into the IDE and/or in pre-commit hooks.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Not part of the build graph:&lt;/strong> It&amp;rsquo;s tempting to try to hook formatting as an action, but formatting requires unconstrained access to the workspace to modify source files, and many source files that you&amp;rsquo;d like to lint don&amp;rsquo;t necessarily have build rules (e.g. Markdown documentation).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Implementation:&lt;/strong> Formatting relies on &lt;code>rules_multitool&lt;/code> which automates the process of downloading a bunch of tools and &lt;code>rules_multirun&lt;/code> which allows running multiple tools in parallel.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Git blame:&lt;/strong> When applying formatting changes, don&amp;rsquo;t forget to register the commits that did so in &lt;code>.git-blame-ignore-revs&lt;/code>. This will prevent your name from showing up in &lt;code>git blame&lt;/code> operations as the author of all files.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Linting:&lt;/strong> The goal of linting is to detect potential problems in the code, but the constraints are different: the problems aren&amp;rsquo;t always conclusive, the solutions aren&amp;rsquo;t always unique nor safe to apply automatically, and analyzing a file may require analyzing more than one at once.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Speed:&lt;/strong> Linting is slow because it often requires using tools akin to compilers. As such, it should never be added to pre-commit hooks and should instead be plumbed through in CI.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Implementation:&lt;/strong> The desire is to have a uniform interface for linting all languages and don&amp;rsquo;t want to have extra rules nor modify build files. The obvious answer to this is to use an aspect which then augments existing rules with reports and patches to fix up the source tree. Validation actions were not an option because using them requires modifying the rulesets, and a premise of this work was to &lt;em>not&lt;/em> do that.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Interface:&lt;/strong> Invoking an aspect and handling the resulting patch files is complicated, so Aspect.dev has augmented the Bazel client (the CLI) with an additional &lt;code>lint&lt;/code> subcommand that automates this process. Interesting, and I suppose this could be turned into a pluggable mechanism by making Bazel look for &lt;code>bazel-&amp;lt;command&amp;gt;&lt;/code> binaries at a well-known location (like the &lt;code>git&lt;/code> dispatcher does). This reminded me of Twitter&amp;rsquo;s &lt;a href="https://v1.pantsbuild.org/">Pants&lt;/a> build system, which is much more than just for building.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="testing">Testing&lt;/h1>
&lt;p>Bazel is often sold as a &amp;ldquo;build tool&amp;rdquo; but its secret sauce is that it really is a &amp;ldquo;test tool&amp;rdquo;. The true benefits of using Bazel come from realizing that not all tests have to run all the time, and that their results can be safely cached. Various talks touched upon how to do testing effectively:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Profiling data:&lt;/strong> Collecting profiling details of tests &lt;em>by default&lt;/em>, and exposing these details as test outputs, is very useful because developers can then use this information to understand how to optimize long tests on their own. AirBnB reported that they do this.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Test granularity:&lt;/strong> We like to think that each test deserves its own test target&amp;hellip; but that&amp;rsquo;s not necessarily how users think or how they interact with tests. Many times, developers end up running certain groups of tests in unison, and if they always do that, combining those tests under a single target will save build and execution time at the expense of a less &amp;ldquo;pure&amp;rdquo; dependency graph.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Tests in the build:&lt;/strong> In certain scenarios, users would like to depend on test outputs as part of the build process. One example that Luminar Technologies provided was the need to generate a detailed report of test results and code coverage for industry compliance reasons. The report generation should be an action that depends on the test outputs, but that&amp;rsquo;s not doable. As a compromise, they made the tests run as part of the build by replacing &lt;code>cc_test&lt;/code> with a macro that spawns a non-test action with a custom runner (but only on CI).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Smoke tests:&lt;/strong> Related to the previous and during the unconference, Luminar Technologies also brought up the desire to gate parts of the build and test process on &amp;ldquo;smoke tests&amp;rdquo;. In other words: once issuing the build/test operation, if we detect that a simple test fails, we shouldn&amp;rsquo;t continue the build or run any of the more expensive tests. The audience seemed to agree that this belonged into a separate tool. Ulf reminded us that there should still be a flag called &lt;code>--test_keep_going&lt;/code>, which can be set to false to stop execution as soon as one test fails.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Test selection:&lt;/strong> When folks adopt Bazel, their initial approach to CI is to simply do &lt;code>bazel test ...&lt;/code> in a job. This works well&amp;hellip; until it doesn&amp;rsquo;t, because even in the presence of test caching, processing the whole monorepo becomes expensive. Uber explains how even an &lt;code>rdeps&lt;/code> query becomes expensive and they&amp;rsquo;ve had to find other solutions. A key insight is that 71% of changes to their monorepo only change source files and only 15% change at least one build file. This means that the majority of changes do not modify the build graph structure significantly, so there is room for reusing state from previous builds (e.g. keeping the Bazel server alive) to compute newly-affected targets.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Flaky test granularity:&lt;/strong> Bazel detects flakes at the test target level&amp;hellip; but a test target can contain tens or hundreds of individual test cases. Which means that if a single test case is flaky and we conclude that its target is flaky, subsequent builds could skip lots of valid test cases. By implementing custom flaky test case detection via &lt;code>junit.xml&lt;/code> report files, Uber was able to surface 3500 additional tests that would have been skipped by the naive target-based approach and could re-add those to the build.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Profile-guided test execution:&lt;/strong> In large test invocations, it&amp;rsquo;s common for one test to be the long pole. If by lack of luck this test ends up running last, it ends dragging the critical path of the test run and lengthening the p99 of test executions. It&amp;rsquo;d be nice if Bazel supported &amp;ldquo;profile-guided execution&amp;rdquo; so that such long poles could be scheduled earlier. Jin reported that this has been looked at but one problem is that there hasn&amp;rsquo;t been a good place to store this information across invocations. Ulf mentioned that when we have async action execution (which Java 21 makes possible), we should be able to improve action scheduling and implement this.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Actions per test:&lt;/strong> One interesting detail that Ulf shared is that test targets generate three actions, possibly more: one to execute the test itself, another to generate the &lt;code>junit.xml&lt;/code> file if the test failed to do so, and another to perform coverage post-processing. If retries, sharding, or &lt;code>--runs_per_test&lt;/code> are involved, there can be even more actions. So, if it becomes necessary, there is precedent to add more actions to perform other types of post-processing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>CI runners:&lt;/strong> A topic that came up during the unconference was that starting Bazel on CI runners is very painful because of the need to download repos, repopulate the local disk cache, and rebuild the analysis graph. The general advice is to prevent discarding CI runners across builds, but that&amp;rsquo;s not always possible (e.g. security might not like to share runners across different jobs). For Kubernetes, stateful sets provide a mechanism to preserve the disk cache even during auto-scaling.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="missing-tooling-around-bazel">Missing tooling around Bazel&lt;/h1>
&lt;p>A fun topic that arose in the unconference was: &amp;ldquo;what support tools do you wish Google had open sourced when they published Bazel, or that the community had built?&amp;rdquo; There were many ideas from the audience:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Scripting language for use in genrules:&lt;/strong> Writing genrules that work across systems is difficult because they invoke the shell, and the tools that are often called do not always support the same syntax (&lt;a href="/2021/08/useless-use-of-gnu.html">thanks GNU&lt;/a>). It&amp;rsquo;d be nice to have a language that allowed writing genrules in a platform-agnostic way. WASM was brought up as well as having a Starlark interpreter with file system access.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Affected target determination:&lt;/strong> The goal of this tool would be to answer the question of which targets are impacted by a change to the source tree. &lt;a href="https://github.com/Tinder/bazel-diff">&lt;code>bazel-diff&lt;/code>&lt;/a> more or less does this. An &lt;code>rdeps&lt;/code> query sounds like the right solution, but in practice it is tricky because changes to a toolchain aren&amp;rsquo;t visible in a query; they do show up in &lt;code>cquery&lt;/code> though, but then you have to be aware of all possible configurations to do the determination correctly.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Build queuing:&lt;/strong> The goal here is to have a service that can queue entire builds and not just actions. This might exist in some CI systems already.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Unifying bazelisk with bazel:&lt;/strong> Users are confused with the fact that &lt;code>sudo apt install bazel&lt;/code> is available but generally does the wrong thing, and that it&amp;rsquo;d be good if &lt;code>bazelisk&lt;/code>&amp;rsquo;s features existed within Bazel. I explained that we built something similar to &lt;code>bazelisk&lt;/code> at Snowflake but for arbitrary tools as a way to hijack command line executions and redirect them to checked-in tools; maybe we should open-source it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>buildozer for bzl files:&lt;/strong> Automating edits to &lt;code>bzl&lt;/code> files would be handy, although this is a hard problem. Maybe it could be scoped down to e.g. automatically clean up old repository rules that aren&amp;rsquo;t referenced.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Java library to programmatically edit build and &lt;code>bzl&lt;/code> files:&lt;/strong>. The current Starlark library isn&amp;rsquo;t useful because it&amp;rsquo;s not standalone and it doesn&amp;rsquo;t preserve the original file structure. A workaround is to extract the AST from Python because these files remain syntactically valid from Python&amp;rsquo;s perspective.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Documentation website for all the rules:&lt;/strong> The BCR could fulfill this, but it currently does not have links to documentation.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="the-end">The end&lt;/h1>
&lt;p>And finally, here are the few leftover toughts that I did not know how to fit in any of the sections above:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Nix:&lt;/strong> There were many talks about Nix on the original submissions, but only a couple made it to the final schedule. The folks at Modus Crate described how they use &lt;code>rules_nixpkgs&lt;/code> to build external dependencies and how they stage those in an NFS-shared &lt;code>/nix/store/&lt;/code> tree. Because Bazel invokes Nix and stages the dependencies during its external repository phase, the execution phase and the remote workers can assume that the Nix packages are in the right locations and things just work.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>RPM integration:&lt;/strong> In a similar spirit to using Nix to install packages and/or build containers, we also have &lt;a href="https://github.com/rmohr/bazeldnf">bazeldnf&lt;/a>. This ruleset provides the &lt;code>rpm&lt;/code> repository rule to import RPM packages and the &lt;code>rpmtree&lt;/code> build rule to combine one or more RPM into a tarball that can later be fed to other rules like &lt;code>container_layer&lt;/code>. In this context, you might also be interested in the &lt;a href="https://github.com/NixOS/patchelf">&lt;code>patchelf&lt;/code>&lt;/a> ruleset.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Migrations:&lt;/strong> Migration talks are getting old, but there were some good insights from MongoDB&amp;rsquo;s talk. One such insight was that, as in any software rewrite, the &amp;ldquo;old&amp;rdquo; build system doesn&amp;rsquo;t stay still during a migration, and the set of features to migrate grows over time. Another such insight was that, if you can couple the new build system to the old one (by making one rely on the other for increasing portions of the code), you can minimize the time in which new features get added to the old build and not the new one. The downside is that you&amp;rsquo;ll be building throwaway work, but overall it will pay off.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>My talk about Snowflake:&lt;/strong> I gave a 15-minute &amp;ldquo;migration&amp;rdquo; talk about what we are doing with Bazel at Snowflake. Because of what I said just above, I skipped over all of the migration topics and instead focused on explaining 5 different technical challenges that we faced and how we fixed them. I was met with lots of questions after the talk, which was really nice, but which means that we owe a full migration post when we are done (very soon now!).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>I hear you want the raw notes? Are you sure? I really don&amp;rsquo;t recommend that you look&amp;hellip; but if you must, here they are: &lt;a href="/notes/2024-10-14-bazelcon.md">BazelCon day 1 notes&lt;/a>, &lt;a href="/notes/2024-10-15-bazelcon.md">BazelCon day 2 notes&lt;/a>, and the &lt;a href="/notes/2024-10-16-build-meetup.md">Build Meetup&amp;rsquo;s notes&lt;/a>.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-10-22-bazelcon.jpg" length="362903" type="image/jpeg"/></item><item><title>The costs of the i386 to x86-64 upgrade</title><link>https://jmmv.dev/2024/10/x86-64-programming-models.html</link><pubDate>Mon, 07 Oct 2024 09:00:00 -0700</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/10/x86-64-programming-models.html</guid><description>&lt;p>If you read my previous article on &lt;a href="/2024/09/dos-memory-models.html">DOS memory models&lt;/a>, you may have dismissed everything I wrote as &amp;ldquo;legacy cruft from the 1990s that nobody cares about any longer&amp;rdquo;. After all, computers have evolved from sporting 8-bit processors to 64-bit processors and, on the way, the amount of memory that these computers can leverage has grown orders of magnitude: the 8086, a 16-bit machine with a 20-bit address space, could only use 1MB of memory while today&amp;rsquo;s 64-bit machines can theoretically access 16EB.&lt;/p>
&lt;p>All of this growth has been in service of ever-growing programs. But&amp;hellip; even if programs are now more sophisticated than they were before, do they all &lt;em>really&lt;/em> require access to a 64-bit address space? Has the growth from 8 to 64 bits been a net positive in performance terms?&lt;/p>
&lt;p>Let&amp;rsquo;s try to answer those questions to find some very surprising answers. But first, some theory.&lt;/p>
&lt;h1 id="code-density">Code density&lt;/h1>
&lt;p>An often overlooked property of machine code is &lt;em>code density&lt;/em>, a loose metric that quantifies how many instructions are required to execute a given &amp;ldquo;action&amp;rdquo;. I double-quote action because an &amp;ldquo;action&amp;rdquo; in this context is a subjective operation that captures a programmer-defined outcome.&lt;/p>
&lt;p>Suppose, for example, that you want to increment the value of a variable that resides in main memory, like so:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="line">&lt;span class="cl">&lt;span class="k">static&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="n">a&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>On a processor with an ISA that provides complex instructions and addressing modes, you can potentially do so in just one machine instruction:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-asm" data-lang="asm">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">; Take the 32-bit quantity at the address indicated by the
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">; 00001234h immediate, increment it by one, and store it in the
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">; same memory location.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="nf">add&lt;/span> &lt;span class="no">dword&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">1234&lt;/span>&lt;span class="no">h&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In contrast, on a processor with an ISA that strictly separates memory accesses from other operations, you&amp;rsquo;d need more steps:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-asm" data-lang="asm">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">li&lt;/span> &lt;span class="no">r8&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1234&lt;/span>&lt;span class="no">h&lt;/span> &lt;span class="c1">; Load the address of the variable into r8.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="nf">lm&lt;/span> &lt;span class="no">r1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="no">r8&lt;/span> &lt;span class="c1">; Load the content of the address into r1.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="nf">li&lt;/span> &lt;span class="no">r2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="c1">; Load the immediate value 1 into r2.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="nf">add&lt;/span> &lt;span class="no">r3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="no">r1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="no">r2&lt;/span> &lt;span class="c1">; r3 = r1 + r2
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="nf">sm&lt;/span> &lt;span class="no">r8&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="no">r3&lt;/span> &lt;span class="c1">; Store the result in r3 into the address in r8.
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Which ISA is better, you ask? Well, as usual, there are pros and cons to each approach:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The former type of ISA offers a denser representation of the operation so it can fit more instructions in less memory. But, because of the semantic complexity of each individual instruction, the processor has to do more work to execute them (possibly requiring more transistors and drawing more power). This type of ISA is usually known as &lt;a href="https://en.wikipedia.org/wiki/Complex_instruction_set_computer">CISC&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The latter type of ISA offers a much more verbose representation of the operation but the instructions that the processor executes are simpler and can likely be executed faster. This type of ISA is usually known as &lt;a href="https://en.wikipedia.org/wiki/Reduced_instruction_set_computer">RISC&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>This is the eternal debate between CISC and RISC but note that the distinction between the two is not crystal clear: some RISC architectures support &lt;em>more&lt;/em> instructions that CISC architectures, and contemporary Intel processors translate their CISC-style instructions into internal RISC-style &lt;a href="https://en.wikipedia.org/wiki/Intel_microcode">micro-operations&lt;/a> immediately after decoding the former.&lt;/p>
&lt;h1 id="its-the-bytes-that-matter">It&amp;rsquo;s the bytes that matter&lt;/h1>
&lt;p>Code density is not about instruction &lt;em>count&lt;/em> though. Code density is about the &lt;em>aggregate size&lt;/em>, in bytes, of the instructions required to accomplish a specific outcome.&lt;/p>
&lt;p>In the example I presented above, the single CISC-style &lt;code>mov&lt;/code> x86 instruction is encoded using anywhere from 2 to 8 bytes depending on its arguments, whereas the fictitious RISC-style instructions typically take 4 bytes each. Therefore, the CISC-style snippet would take 8 bytes total whereas the RISC-style snippet would take 20 bytes total, while achieving the same conceptual result.&lt;/p>
&lt;p>Is that increase in encoded bytes bad? Not necessarily because &amp;ldquo;bad&amp;rdquo; is subjective and we are talking about trade-offs here, but it&amp;rsquo;s definitely an important consideration due to its impact in various dimensions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Memory usage&lt;/strong>: Larger programs take more memory. You might say that memory is plentiful these days, and that&amp;rsquo;s true, but it wasn&amp;rsquo;t always the case: when computers could only address 1 MB of RAM or less, the size of a program&amp;rsquo;s binary mattered a lot. Furthermore, even today, memory &lt;em>bandwidth&lt;/em> is limited, and this is an often-overlooked property of a system.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Disk space&lt;/strong>: Larger programs take more disk space. You might say that disk space is plentiful these days too and that program code takes a tiny proportion of the overall disk: most space goes towards storing media anyway. And that&amp;rsquo;s true, but I/O bandwidth is not as unlimited as disk space, and loading these programs from disk isn&amp;rsquo;t cheap. Have you noticed how utterly slow is it to open an Electron-based app?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>L1 thrashing&lt;/strong>: Larger or more instructions mean that you can fit fewer of them in the L1 instruction cache. Proper utilization of this cache is critical to achieve reasonable levels of computing performance, and even though main memory size can now be measured in terabytes, the L1 cache is still measured in kilobytes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>We could beat the dead horse over CISC vs. RISC further but that&amp;rsquo;s not what I want to focus on. What I want to focus on is one very specific dimension of the instruction encoding that affects code density in all cases. And that&amp;rsquo;s the &lt;em>size of the addresses&lt;/em> (pointers) required to reference program code or data.&lt;/p>
&lt;p>Simply put, the larger the size of the addresses in the program&amp;rsquo;s code, the lower the code density. The lower the code density, the more memory and disk space we need. And the more memory code takes, the more L1 thrashing we will see.&lt;/p>
&lt;p>So, for good performance, we probably want to optimize the way we represent addresses in the code and take advantage of surrounding context. For example: if we have a tight loop like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="line">&lt;span class="cl">&lt;span class="k">static&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">100&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">a&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The corresponding assembly code may be similar to this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-asm" data-lang="asm">&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">mov&lt;/span> &lt;span class="no">ecx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="c1">; i = 0.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="nl">repeat:&lt;/span> &lt;span class="nf">cmp&lt;/span> &lt;span class="no">ecx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">100&lt;/span> &lt;span class="c1">; i &amp;lt; 100?
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="nf">je&lt;/span> &lt;span class="no">out&lt;/span> &lt;span class="c1">; If i == 100, jump to out.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="nf">add&lt;/span> &lt;span class="no">dword&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">01234&lt;/span>&lt;span class="no">h&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="c1">; Increment a.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="nf">add&lt;/span> &lt;span class="no">ecx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="c1">; i++.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="nf">jmp&lt;/span> &lt;span class="no">repeat&lt;/span> &lt;span class="c1">; Jump back to repeat.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="nl">out:&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now the questions are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Given that &lt;code>repeat&lt;/code> and &lt;code>out&lt;/code> are labels for memory addresses, how do we encode the &lt;code>je&lt;/code> (jump if equal) and &lt;code>jmp&lt;/code> (unconditional jump) instructions as bytes? On a 32-bit machine, do we encode the full 32-bit addresses (4 bytes each) in each instruction, or do we use 1-byte relative short pointers because the jump targets are within a 127-byte distance on either side of the jump instruction?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>How do we represent the address of the &lt;code>a&lt;/code> variable? Do we express it as an absolute address or as a relative one? Do we store &lt;code>1234h&lt;/code> as a 32-bit quantity or do we use fewer bytes because this specific number is small?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>How big should &lt;code>int&lt;/code> be? The &lt;code>dword&lt;/code> above implies 32 bits but&amp;hellip; that&amp;rsquo;s just an assumption I made when writing the snippet. &lt;code>int&lt;/code> could have been 16 bits, or 64 bits, or anything else you can imagine (which is a design mistake in C, if you ask me).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The answers to the questions above are &lt;em>choices&lt;/em>: we can decide whichever encoding we want for code and data addresses, and these choices have consequences on code density. This was true many years ago during the DOS days, which is why we had short, near, and far pointer types, and &lt;a href="https://web.eece.maine.edu/~vweaver/papers/iccd09/iccd09_density.pdf">is still true today&lt;/a> as we shall see.&lt;/p>
&lt;h1 id="the-switch-to-32-bits">The switch to 32 bits&lt;/h1>
&lt;p>It is no secret that programming 16-bit machines was limiting, and we can identify at least two reasons to back this claim.&lt;/p>
&lt;p>First, 16-bit machines usually had limited address spaces. It&amp;rsquo;s important to understand that a processor&amp;rsquo;s native register size has no direct connection to the size of the addresses the processor can reference. In theory, these machines could have leveraged larger chunks of memory and, in fact, the 16-bit 8086 proved this to be true with its 20-bit address space. But the point is that, &lt;em>in the era&lt;/em> of 16-bit machines, 1 MB of RAM was considered &amp;ldquo;a lot&amp;rdquo; and so machines didn&amp;rsquo;t have much memory.&lt;/p>
&lt;p>Second, 16-bit machines can only operate on 16-bit quantities at a time, and 16 bits only allow representing integers with limited ranges: a signed number between -32K and +32K isn&amp;rsquo;t&amp;hellip; particularly large. Like before, it&amp;rsquo;s important to clarify that the processor&amp;rsquo;s native register size does not impose restrictions on the size of the numbers the processor can manipulate, but it does add limitations on how &lt;em>fast&lt;/em> it can do so: operating 32-bit numbers on 16-bit machines requires at least twice the number of instructions for each operation.&lt;/p>
&lt;p>So, when 32-bit processors entered the scene, it was pretty much a no-brainer that programs had to become 32-bit by default: all memory addresses and default integer types grew to 32 bits. This allowed programmers to not worry about memory limits for a while: 32 bits can address 4GB of memory, which was a huge amount back then and should &lt;em>still&lt;/em> be huge if it wasn&amp;rsquo;t due to bloated software. Also, this upgrade allowed programmers to efficiently manipulate integers with large-enough ranges for most operations.&lt;/p>
&lt;p>In technical mumbo-jumbo, what happened here was that C adopted the ILP32 programming model: integers (I), longs (L), and pointers (P) all became 32 bits, whereas chars and shorts remained 8-bit and 16-bit respectively.&lt;/p>
&lt;p>It didn&amp;rsquo;t have to be this way though: if we look at the model for 16-bit programming, shorts and integers were 16-bit whereas longs were 32-bit, so why did integers change size but longs remained the same as integers? I do not have an answer for this, but if longs had become 64 bits back then, maybe we wouldn&amp;rsquo;t be in the situation today where &lt;a href="https://en.wikipedia.org/wiki/Year_2038_problem">2038 will bring mayhem to Unix systems&lt;/a>.&lt;/p>
&lt;h1 id="evolving-towards-x86-64">Evolving towards x86-64&lt;/h1>
&lt;p>4GB of RAM were a lot when 32-bit processors launched but slowly became insufficient as software kept growing. To support those needs, there were crutches like &lt;a href="https://en.wikipedia.org/wiki/Physical_Address_Extension">Intel&amp;rsquo;s PAE&lt;/a>, which allowed manipulating up to 64GB of RAM on 32-bit machines without changing the programming model, but they were just that: hacks.&lt;/p>
&lt;p>The thing is: it&amp;rsquo;s not only software that grew. It&amp;rsquo;s the &lt;em>kinds of things&lt;/em> that people wanted to do with software that changed: people wanted to edit high-resolution photos and video as well as play more-realistic games, and writing code to achieve those goals on a limited 4GB address space was possible but not &lt;em>convenient&lt;/em>. With 64-bit processors, &lt;code>mmap&lt;/code>ing huge files made those programs easier to write, and using native 64-bit integers made them faster too. So 64-bit machines became mainstream sometime around the introduction of the Athlon 64 processor and the Power Mac G5, both in 2003.&lt;/p>
&lt;p>So what happened to the programming model? ILP32 was a no-brainer for 32-bit machines, but were LP64 or ILP64 no-brainers for 64-bit machines? These 64-bit models were definitely tempting because they allowed the programmer to leverage the machine&amp;rsquo;s resources freely. Larger pointers allowed addressing &amp;ldquo;unlimited&amp;rdquo; memory transparently, and a larger long type naturally bumped file offsets (&lt;code>ino_t&lt;/code>), timestamps (&lt;code>time_t&lt;/code>), array lengths (&lt;code>size_t&lt;/code>), and more to 64 bits as well. Without a lot of work from programmers, programs could &amp;ldquo;just do more&amp;rdquo; by simply recompiling them.&lt;/p>
&lt;p>But there was a downside to that choice. According to the theory I presented earlier, LP64 would make programs bigger and would decrease code density when compared to ILP32. And lower code density could lead to L1 instruction cache thrashing, which is an important consideration to this day because a modern Intel Core i7-14700K from 2023 has just 80 KB of L1 cache and an Apple Silicon M3 from 2023 has less than 200KB. (These numbers are&amp;hellip; &lt;em>not big&lt;/em> when you stack them up against the multi-GB binaries that comprise modern programs, are they?)&lt;/p>
&lt;p>We now know that LP64 was the preferred choice and that it became the default programming model for 64-bit operating systems, which means we can compare its impact against ILP32. So what are the consequences? Let&amp;rsquo;s take a look.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-10-07-x86-64-iso-images.png" class="with-border" alt="Comparison of ISO image sizes for various operating systems" />
&lt;/figure>
&lt;p>Wait, what? The FreeBSD x86-64 installation image is definitely larger than the i386 one&amp;hellip; but all other images are &lt;em>smaller&lt;/em>? What&amp;rsquo;s going on here? This contradicts everything I said above!&lt;/p>
&lt;h1 id="down-the-rabbit-hole">Down the rabbit hole&lt;/h1>
&lt;p>I was genuinely surprised by this and I had to dig a bit. Cracking open the FreeBSD bootonly image revealed some differences in the kernel (slightly bigger binaries, but different sets of modules) which made it difficult to compare the two. But looking into the Debian netinst images, I did find that almost all i386 binaries were larger than their x86-64 counterparts.&lt;/p>
&lt;p>To try to understand why that was, the first thing I did was compile a simple hello-world program on my x86-64 Debian VM, targeting both 64-bit and 32-bit binaries:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ gcc-14 -o hello64 hello.c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ i686-linux-gnu-gcc-14 -o hello32 hello.c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ file hello32 hello64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hello32: ELF 32-bit LSB pie executable, Intel 80386, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux.so.2, for GNU/Linux 3.2.0, not stripped
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hello64: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=f1bf851d7f1d56ae5d50eb136793066f67607e06, for GNU/Linux 3.2.0, not stripped
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ ls -l hello32 hello64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rwxrwxr-x 1 jmmv jmmv 15040 Oct 4 17:45 hello32
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-rwxrwxr-x 1 jmmv jmmv 15952 Oct 4 17:45 hello64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Based on this, it &lt;em>looks&lt;/em> as if 32-bit binaries are indeed smaller than 64-bit binaries. But a &amp;ldquo;hello world&amp;rdquo; program is trivial and not worth 15kb of code: those 15kb shown above are definitely &lt;em>not&lt;/em> code. They are probably mostly ELF overhead. Indeed, if we look at just the text portion of the binaries:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ objdump -h hello32 | grep text
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 14 .text 00000169 00001060 00001060 00001060 2**4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ objdump -h hello64 | grep text
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 14 .text 00000103 0000000000001050 0000000000001050 00001050 2**4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&amp;hellip; we find that &lt;code>hello32&lt;/code>&amp;rsquo;s text is 169h bytes whereas &lt;code>hello64&lt;/code>&amp;rsquo;s text is 103h bytes. And if we disassemble the two:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ objdump --disassemble=main hello32
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">00001189 &amp;lt;main&amp;gt;:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1189: 8d 4c 24 04 lea 0x4(%esp),%ecx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 118d: 83 e4 f0 and $0xfffffff0,%esp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1190: ff 71 fc push -0x4(%ecx)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1193: 55 push %ebp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1194: 89 e5 mov %esp,%ebp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1196: 53 push %ebx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1197: 51 push %ecx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1198: e8 28 00 00 00 call 11c5 &amp;lt;__x86.get_pc_thunk.ax&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 119d: 05 57 2e 00 00 add $0x2e57,%eax
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11a2: 83 ec 0c sub $0xc,%esp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11a5: 8d 90 14 e0 ff ff lea -0x1fec(%eax),%edx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11ab: 52 push %edx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11ac: 89 c3 mov %eax,%ebx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11ae: e8 8d fe ff ff call 1040 &amp;lt;puts@plt&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11b3: 83 c4 10 add $0x10,%esp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11b6: b8 00 00 00 00 mov $0x0,%eax
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11bb: 8d 65 f8 lea -0x8(%ebp),%esp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11be: 59 pop %ecx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11bf: 5b pop %ebx
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11c0: 5d pop %ebp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11c1: 8d 61 fc lea -0x4(%ecx),%esp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 11c4: c3 ret
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ objdump --disassemble=main hello64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">0000000000001139 &amp;lt;main&amp;gt;:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1139: 55 push %rbp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 113a: 48 89 e5 mov %rsp,%rbp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 113d: 48 8d 05 c0 0e 00 00 lea 0xec0(%rip),%rax # 2004 &amp;lt;_IO_stdin_used+0x4&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1144: 48 89 c7 mov %rax,%rdi
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1147: e8 e4 fe ff ff call 1030 &amp;lt;puts@plt&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 114c: b8 00 00 00 00 mov $0x0,%eax
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1151: 5d pop %rbp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1152: c3 ret
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We observe massive differences in the machine code generated for the trivial &lt;code>main&lt;/code> function. The 64-bit code is definitely &lt;em>smaller&lt;/em> than the 32-bit code, contrary to my expectations. But the code is also &lt;em>very&lt;/em> different; so different, in fact, that ILP32 vs. LP64 doesn&amp;rsquo;t explain it.&lt;/p>
&lt;p>The first difference we can observe is around calling conventions. The i386 architecture has a limited number of registers, favors passing arguments via the stack, and only 3 registers can be clobbered within a function. x86-64, on the other hand, prefers passing arguments through registers as much as possible and defines 7 registers as volatile.&lt;/p>
&lt;p>The second difference is that we don&amp;rsquo;t see 64-bit addresses anywhere in the code above. Jump addresses are encoded using near pointers, and data addresses are specified as offsets over a 64-bit base previously stored in a register. I found it smart that those addresses are relative to the program counter (the RIP register).&lt;/p>
&lt;p>There may be more differences, but these two alone seem to be the reason why 64-bit binaries end up being more compact than 32-bit ones. &lt;em>On Intel x86&lt;/em>, that is. You see: Intel x86&amp;rsquo;s instruction set is so versatile that the compiler and the ABI can play tricks to hide the cost of 64-bit pointers.&lt;/p>
&lt;p>Is that true of more RISC-y 64-bit architectures though? I installed the PowerPC 32-bit and 64-bit toolchains and ran the same test. And guess what? The PowerPC 64-bit binary was indeed larger than the 32-bit one, so &lt;em>maybe&lt;/em> it&amp;rsquo;s true. Unfortunately, running a broader comparison than this is difficult: there is no full operating system I can find that ships both builds any longer, and ARM images can&amp;rsquo;t easily be compared.&lt;/p>
&lt;h1 id="its-all-about-the-data">It&amp;rsquo;s all about the data&lt;/h1>
&lt;p>OK, fine, we&amp;rsquo;ve settled that 64-bit &lt;em>code&lt;/em> isn&amp;rsquo;t necessarily larger than 32-bit code, at least on Intel, and thus any adverse impact on the L1 instruction cache is probably negligible. But&amp;hellip; what about &lt;em>data density&lt;/em>?&lt;/p>
&lt;p>Pointers don&amp;rsquo;t only exist in instructions or as jump targets. They also exist within the most modest of data types: lists, trees, graphs&amp;hellip; all contain pointers in them. And in those, unless the programmer explicitly plays complex &lt;a href="https://v8.dev/blog/pointer-compression">tricks to compress pointers&lt;/a>, we&amp;rsquo;ll usually end up with larger data structures by simply jumping to LP64. The same applies to the innocent-looking &lt;code>long&lt;/code> type by the way, which appears throughout codebases and also grows with this model.&lt;/p>
&lt;p>And &lt;em>this&lt;/em>&amp;mdash;a decrease in data density&amp;mdash;is where the real performance penalty comes from: it&amp;rsquo;s not so much about the code size but about the data size.&lt;/p>
&lt;p>Let&amp;rsquo;s take a look. I wrote a simple program that creates a linked list of integers with 10 million nodes and then iterates over them in sequence. Each node is 8 bytes in 32-bit mode (4 bytes for the &lt;code>int&lt;/code> and 4 bytes for the &lt;code>next&lt;/code> pointer), whereas it is 16 bytes in 64-bit mode (4 bytes for the &lt;code>int&lt;/code>, &lt;em>4 bytes of padding&lt;/em>, and 8 bytes for the &lt;code>next&lt;/code> pointer). I then compiled that program in 32-bit and 64-bit mode, measured it, and ran it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ gcc -o list64 list.c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ i686-linux-gnu-gcc-14 -o list32 list.c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ objdump -h list32 | grep text
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 13 .text 000001f6 00001070 00001070 00001070 2**4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ objdump -h list64 | grep text
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 14 .text 000001b0 0000000000001060 0000000000001060 00001060 2**4
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ hyperfine --warmup 1 ./list32 ./list64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Benchmark 1: ./list32
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Time (mean ± σ): 394.2 ms ± 2.1 ms [User: 311.4 ms, System: 83.1 ms]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Range (min … max): 392.1 ms … 398.2 ms 10 runs
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Benchmark 2: ./list64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Time (mean ± σ): 502.4 ms ± 4.5 ms [User: 334.9 ms, System: 167.8 ms]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Range (min … max): 494.9 ms … 509.5 ms 10 runs
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Summary
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ./list32 ran
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1.27 ± 0.01 times faster than ./list64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As before with the hello-world comparison, this simple microbenchmark&amp;rsquo;s 32-bit code continues to be slightly larger than its 64-bit counterpart (1F6h bytes vs 1B0h). However, its runtime is &lt;em>27% faster&lt;/em>, and it is no wonder because the doubling of the linked list node size implies doubling the memory usage and thus the halving of cache hits. We can confirm the impact of this using the &lt;code>perf&lt;/code> tool:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ perf stat -B -e cache-misses ./list32 2&amp;gt;&amp;amp;1 | grep cpu_core
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 2,400,339 cpu_core/cache-misses:u/ (99.33%)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ perf stat -B -e cache-misses ./list64 2&amp;gt;&amp;amp;1 | grep cpu_core
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 4,687,156 cpu_core/cache-misses:u/ (99.34%)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The 64-bit build of this microbenchmark incurs almost double the cache misses than the 32-bit build.&lt;/p>
&lt;p>Of course, this is just a microbenchmark, and tweaking it slightly will make it show very different results and make it say whatever we want. I tried to add jitter to the memory allocations so that the nodes didn&amp;rsquo;t end up as consecutive in memory, and then the 64-bit version executed faster. I suspect this is due to the memory allocator having a harder time handling memory when the address space is limited.&lt;/p>
&lt;p>The impact on real world applications is harder to quantify. It is difficult to find the same program built in 32-bit and 64-bit mode and to run it on the same kernel. It is even more difficult to find one such program where the difference matters. But at the end of the day, the differences exist, and I bet they are more meaningful than we might think in terms of bloat&amp;mdash;but I did not intend to write a research paper here, so I&amp;rsquo;ll leave that investigation to someone else&amp;hellip; or another day.&lt;/p>
&lt;h1 id="the-x32-abi">The x32 ABI&lt;/h1>
&lt;p>There is one last thing to discuss before we depart. While we find ourselves using the LP64 programming model on x86-64 processors, remember that this was a choice and there were and are other options on the table.&lt;/p>
&lt;p>Consider this: we could have made the operating system kernel leverage 64 bits to gain access to a humongous address space, but we could have kept the user-space programming model as it was before&amp;mdash;that is, we could have kept ILP32. And we could have gone even further and optimized the calling conventions to reduce the binary code size by leveraging the additional general-purpose registers that x86-64 provides.&lt;/p>
&lt;p>And in fact, this exists and is known as &lt;a href="https://en.wikipedia.org/wiki/X32_ABI">x32&lt;/a>.&lt;/p>
&lt;p>We can install the x32 toolchain and see that it effectively works as we&amp;rsquo;d imagine:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ gcc -o hello64 hello.c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ x86_64-linux-gnux32-gcc-14 -o hellox32 hello.c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ objdump --disassemble=main hello64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">0000000000001139 &amp;lt;main&amp;gt;:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1139: 55 push %rbp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 113a: 48 89 e5 mov %rsp,%rbp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 113d: 48 8d 05 c0 0e 00 00 lea 0xec0(%rip),%rax # 2004 &amp;lt;_IO_stdin_used+0x4&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1144: 48 89 c7 mov %rax,%rdi
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1147: e8 e4 fe ff ff call 1030 &amp;lt;puts@plt&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 114c: b8 00 00 00 00 mov $0x0,%eax
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1151: 5d pop %rbp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1152: c3 ret
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ objdump --disassemble=main hellox32
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">00401126 &amp;lt;main&amp;gt;:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 401126: 55 push %rbp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 401127: 89 e5 mov %esp,%ebp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 401129: b8 04 20 40 00 mov $0x402004,%eax
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 40112e: 89 c0 mov %eax,%eax
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 401130: 48 89 c7 mov %rax,%rdi
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 401133: e8 f8 fe ff ff call 401030 &amp;lt;puts@plt&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 401138: b8 00 00 00 00 mov $0x0,%eax
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 40113d: 5d pop %rbp
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 40113e: c3 ret
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, the &lt;code>main&lt;/code> method of our hello-world program is really similar between the 64-bit and 32-bit builds, but pay close attention to the x32 version: it uses the same calling conventions as x86-64, but it contains a mixture of 32-bit and 64-bit registers, delivering a more compact binary size.&lt;/p>
&lt;p>Unfortunately:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ ./hellox32
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">zsh: exec format error: ./hellox32
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can&amp;rsquo;t run the resulting binary. x32 is an ABI that impacts the kernel interface too, so these binaries cannot be executed on a regular x86-64 kernel. Sadly, and as far as I can tell, x32 is pretty much abandonware today. Gentoo claims to support it but there are no official builds of any distribution I can find that are built in x32 mode.&lt;/p>
&lt;hr>
&lt;p>In the end, even though LP64 &lt;a href="https://unix.org/version2/whatsnew/lp64_wp.html">wasn&amp;rsquo;t the obvious choice&lt;/a> for x86-64, it&amp;rsquo;s the compilation mode that won and stuck.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-10-07-x86-64-cover-image.jpg" length="992004" type="image/jpeg"/></item><item><title>Revisiting the DOS memory models</title><link>https://jmmv.dev/2024/09/dos-memory-models.html</link><pubDate>Mon, 30 Sep 2024 08:00:00 -0700</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/09/dos-memory-models.html</guid><description>&lt;p>At the beginning of the year, I wrote a bunch of articles on the various tricks DOS played to overcome the tight memory limits of x86&amp;rsquo;s real mode. There was one question that came up and remained unanswered: what were the various &amp;ldquo;models&amp;rdquo; that the compilers of the day offered? Take a look at Borland Turbo C++&amp;rsquo;s code generation menu:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-09-30-turbo-cxx-memory-models.png" class="with-border" />
&lt;figcaption>Borland Turbo C++ showing its Code Generation menu, which displays the list of models we cover in this article.&lt;/figcaption>
&lt;/figure>
&lt;p>Tiny, small, medium, compact, large, huge&amp;hellip; What did these options mean? What were their effects? And, more importantly&amp;hellip; is any of that legacy relevant today in the world of 64-bit machines and gigabytes of RAM? To answer those questions, we must start with a brief review of the 8086 architecture and the binary formats supported by DOS.&lt;/p>
&lt;h1 id="8086-segmentation">8086 segmentation&lt;/h1>
&lt;p>In the 8086 architecture, which is the architecture that DOS targeted, memory references are composed of two parts: a &lt;strong>2-byte segment&lt;/strong> &amp;ldquo;identifier&amp;rdquo; and a &lt;strong>2-byte offset&lt;/strong> within the segment. These pairs are often expressed as &lt;code>segment:offset&lt;/code>.&lt;/p>
&lt;p>Segments are contiguous 64KB chunks of memory and are identified by their base address. To be able to address the full 1MB of memory that the 8086 supports, segments are offset from each other by 16 bytes. As you can deduce from this, segments overlap, which means that a specific physical memory position can be referenced by many segment/offset pairs.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-09-30-dos-memory-models-8086-segments.png" />
&lt;figcaption>Representation of two consecutive 8086 segments and how a single physical memory address can be expressed as different segment/offset pairs.&lt;/figcaption>
&lt;/figure>
&lt;p>As an example, the segmented address &lt;code>B800h:0032h&lt;/code> corresponds to the physical address &lt;code>B8032h&lt;/code> computed via &lt;code>B800h * 10h + 0032h&lt;/code>. While this pair is human-readable, that&amp;rsquo;s not how the machine-level instructions encode it. Instead, instructions rely on &lt;strong>segment registers&lt;/strong> to specify the segment to access, and the 8086 supports four of these: CS (code segment), DS (data segment), ES (extra data segment), and SS (stack segment). Knowing this, accessing this sample memory position requires first loading &lt;code>B800h&lt;/code> into &lt;code>DS&lt;/code> and then referencing &lt;code>DS:0032h&lt;/code>.&lt;/p>
&lt;p>One of the reasons instructions rely on segment registers instead of segment identifiers on every memory access is efficiency: encoding the segment register to use requires only 2 bits (we have 4 segment registers in total) vs. the 2 bytes that would be necessary to store the segment base. More on this later.&lt;/p>
&lt;h1 id="com-files">COM files&lt;/h1>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/COM_file">COM files&lt;/a> are the most trivial executable format you can think of: they contain raw machine code that can be placed at pretty much any memory location and executed without any sort of post-processing. There are no relocations, no shared libraries, no nothing to worry about: you can just blit the binary into memory and run it.&lt;/p>
&lt;p>The way this works is by leveraging the 8086 segmented architecture: the COM image is loaded into &lt;em>any&lt;/em> segment and always at offset 100h within that segment. All memory addresses within the COM image must be relative to this offset (which explains the &lt;code>ORG 100h&lt;/code> you might have seen in the past), but the image doesn&amp;rsquo;t need to know which segment it is loaded into: the loader (DOS in our case, but COM files come from CP/M actually) sets CS, DS, ES, and SS to the exact same segment and transfers control to &lt;code>CS:100h&lt;/code>.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-09-30-dos-memory-models-com-file.png" />
&lt;figcaption>At the top, representation of a COM file as it is stored on disk. At the bottom, an explanation of how DOS proceeds to load this COM file into two different segments.&lt;/figcaption>
&lt;/figure>
&lt;p>Magic! COM files are essentially PIE (&lt;a href="https://en.wikipedia.org/wiki/Position-independent_code">Position Independent Executables&lt;/a>) without requiring any sort of MMU or fancy memory management by the kernel.&lt;/p>
&lt;p>Unfortunately, not everything is rosy. The problem with COM files is that they are limited in size: because they are loaded into one segment and segments are 64KB-long at most, the largest a COM file can be is 64KB (minus the 256 bytes reserved for the PSP at the front). This includes code &lt;em>and&lt;/em> data, and 64KB isn&amp;rsquo;t much of either. Obviously, when the COM program is running, it has free reign over the processor and can access any memory outside of its single segment by resetting CS, DS, ES, and/or SS, but all memory management is left to the programmer.&lt;/p>
&lt;h1 id="exe-files">EXE files&lt;/h1>
&lt;p>To resolve the limitations of COM files in DOS, Microsoft came up with a different executable format for DOS: the &lt;a href="https://en.wikipedia.org/wiki/DOS_MZ_executable">EXE file&lt;/a>, also known as an MZ executable.&lt;/p>
&lt;p>Compared to COM files, EXE files have some internal structure and are not bound by the 64KB limit: they can contain larger code and data blocks in them. But&amp;hellip; how is that possible given that 8086 segments are still 64KB-long at most? The answer is simple: EXE files contain &lt;em>multiple&lt;/em> segments and spread code and data over them.&lt;/p>
&lt;p>To support multiple segments at runtime, EXE files contain relocation information in their header. Conceptually, relocations tell the loader which positions in the binary image contain &amp;ldquo;incomplete&amp;rdquo; pointers that need to be fixed up with the segment base addresses of the segments after they are loaded in memory. DOS, acting as the loader, is responsible for doing this patching.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-09-30-dos-memory-models-exe-file.png" />
&lt;figcaption>Representation of an EXE file and the content of one of its code segments. Within the code, we can see a near pointer and two far pointers that will require patching at runtime to point to the other segments in the binary.&lt;/figcaption>
&lt;/figure>
&lt;p>How many segments go into the EXE though? Well, it depends, because not all programs have the same needs. Some programs are tiny overall and could fit in a COM file. Other programs contain large portions of data but little code. Another set of programs contain a lot of code and data. Etc.&lt;/p>
&lt;p>So then the question becomes: how can the one-size-fits-all EXE format support these options in an efficient manner? This is where memory models become important, but to talk about those, we must do another detour through pointer types.&lt;/p>
&lt;h1 id="pointer-types">Pointer types&lt;/h1>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Locality_of_reference">The locality principle&lt;/a> says that &amp;ldquo;a processor tends to access the same set of memory locations repetitively over a short period of time&amp;rdquo;. This is easy to reason about: code normally runs almost-sequentially and data is often packed in consecutive chunks of memory like arrays or structs.&lt;/p>
&lt;p>Because of this, expressing &lt;em>all&lt;/em> memory addresses as 4-byte &lt;code>segment:offset&lt;/code> pairs would be wasteful, and this is where 8086&amp;rsquo;s segmentation plays in our favor again. We can first load a segment register with the base address of &amp;ldquo;all of our data&amp;rdquo; and then all we need to do is record addresses as offsets within that segment. The fewer times we have to reload segment registers, the better because the less information we have to carry around in every instruction and in every memory reference.&lt;/p>
&lt;p>But we can&amp;rsquo;t just always use offsets within a single segment because we may be dealing with more than one segment. And offsets come in various sizes so using a unique size for them all would be wasteful too. Which means memory addresses, or &lt;em>pointers&lt;/em>, need to have different shapes and forms, each best suited for a specific use case.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-09-30-dos-memory-models-short-pointer.png" />
&lt;figcaption>Representation of two short pointers: one addressing a higher memory address (possibly a forward jump to skip a conditional branch) and one addressing a lower memory address (probably a backwards jump to return to the beginning of a loop).&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;strong>Short pointers&lt;/strong> take just 1 byte and express a &lt;em>relative&lt;/em> address from the instruction being executed. These are specially useful in jump instructions to keep their binary representation compact: jumps appear in every conditional or loop, and in many cases, conditional branches and loop bodies are so short that minimizing the amount of code required to express these branch points is worthwhile.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-09-30-dos-memory-models-near-pointer.png" />
&lt;figcaption>Representation of a near pointer.&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;strong>Near pointers&lt;/strong> can reference addresses within the 64KB segment implied &amp;ldquo;by context&amp;rdquo; and are 2-byte long. For example, an instruction like &lt;code>JMP 12829h&lt;/code> does not usually need to carry information about the segment this address references because code jumps are almost-always within the same CS of the code issuing the jump. Similarly, an instruction like &lt;code>MOV AX, [5610h]&lt;/code> assumes that the given address references the currently-selected DS so that it doesn&amp;rsquo;t have to express the segment every time. The offset encoded by the near pointer can be relative or absolute.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-09-30-dos-memory-models-far-pointer.png" />
&lt;figcaption>Representation of a far pointer referencing an address in another segment.&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;strong>Far pointers&lt;/strong> can reference any memory address by encoding a segment and an offset. They are 4-byte long. When used in pointer arithmetic, the segment stays fixed and only the offset varies. This is relevant, for example, when iterating over arrays as we can load the base address into DS or ES just once and then manipulate the offset within the segment. However, this means that such iteration has a maximum range of 64KB.&lt;/p>
&lt;p>&lt;strong>Huge pointers&lt;/strong> are like far pointers in that they are also 4-byte long and can reference any memory address, but they eliminate the 64KB limitations around pointer arithmetic. They do so by recomputing the segment and offset portions on every memory access (remember that segments are overlapping so we can come up with multiple segment/offset pairs for any physical address). As you can imagine, this requires extra code on every memory access and thus huge pointers impose a noticeable tax on run time.&lt;/p>
&lt;h1 id="memory-models">Memory models&lt;/h1>
&lt;p>And now that we know about 8086 segmentation, EXE files, and pointer types&amp;hellip; we can finally tie all of these concepts together to demystify the memory models we used to see in old compilers for DOS.&lt;/p>
&lt;p>Here is the breakdown:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Tiny&lt;/strong>: This is the memory model of COM images. The whole program fits in one 64KB segment and all segment registers are set to this one segment at startup. This means that all pointers within the program are short or near because they always reference this same 64KB segment.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Small&lt;/strong>: Uses near pointers everywhere, but the data and stack segments are different from the code segment. This means that these programs have 64KB for code and 64KB for data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Compact&lt;/strong>: Uses short pointers for the code but far pointers for the data. This means that these programs can use the full 1 MB memory space for data and, as such, it was particularly useful for games where the code would be as tight as possible while being able to load and reference all assets in memory.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Medium&lt;/strong>: The opposite from compact. Uses far pointers for code and short pointers for data. This model is weird because, if you had a program with a lot of code, it was probably the kind of program that handled a lot of data too.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Large&lt;/strong>: Uses far pointers everywhere so both code and data can reference the full 1 MB address space. However, because of what far pointers are, all memory &lt;em>offsets&lt;/em> are 64 KB at most which means data structures and arrays are limited in size.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Huge&lt;/strong>: Uses huge pointers everywhere. This overcomes the limitations of the large model by emitting code to compute the absolute addresses of every memory access and allows structs and arrays that span over 64 KB of memory. Obviously, this comes at a cost: the program code is now larger and the runtime cost is much bigger.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>And that&amp;rsquo;s it!&lt;/p>
&lt;p>It is worth highlighting that these models were all &lt;em>conventions&lt;/em> that a vintage C compiler used to emit code. If you were writing assembly by hand, you could mix-and-match pointer types to do whatever you wanted given that these concepts had no special meaning to the OS.&lt;/p>
&lt;h1 id="evolving-to-todays-world">Evolving to today&amp;rsquo;s world&lt;/h1>
&lt;p>Everything I have told you about until now is legacy stuff that you could easily dismiss as useless knowledge. Or could you?&lt;/p>
&lt;p>One thing I did &lt;em>not&lt;/em> touch upon is the concept of &lt;strong>code density&lt;/strong> and how it relates to performance. The way we choose to express pointers in the code has a direct impact on code density, so when we evolve computing from 16-bit machines like the 8086 to contemporary 64-bit machines, pointer representations grow by &lt;em>a lot&lt;/em> and we face some hard choices.&lt;/p>
&lt;p>But to explain all of this and answer the performance questions, you&amp;rsquo;ll have to wait for the next article. So subscribe now to not miss out on that one!&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-09-30-turbo-cxx-memory-models.png" length="140059" type="image/jpeg"/></item><item><title>Windows NT vs. Unix: A design comparison</title><link>https://jmmv.dev/2024/09/windows-nt-vs-unix-design.html</link><pubDate>Mon, 09 Sep 2024 08:30:00 -0700</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/09/windows-nt-vs-unix-design.html</guid><description>&lt;p>Over the years, I&amp;rsquo;ve repeatedly heard that Windows NT is a very advanced operating system and, being a Unix person myself, it has bothered me to not know &lt;em>why&lt;/em>. I&amp;rsquo;ve been meaning to answer this question for years and I can do so now, which means I want to present you my findings.&lt;/p>
&lt;p>My desire to know about NT&amp;rsquo;s internals started in 2006 when I applied to the Google Summer of Code program to develop Boost.Process. I needed such a library for ATF, but I also saw the project as a chance to learn something about the Win32 API. This journey then continued in 2020 with me &lt;a href="/2020/10/bye-google-hi-microsoft.html">choosing to join Microsoft&lt;/a> after a long stint at Google and me buying the &lt;a href="https://www.amazon.com/Windows-Internals-Part-architecture-management/dp/0735684189?crid=2F7UR8S48RP6O&amp;amp;dib=eyJ2IjoiMSJ9.p9cBb_-Q8GjuK0z0kDLKG6xoExPM_2QWt_jn0PlqVBSWYNyqRp2Cd7MHXFeQ4EiRACaX_Y_9xzECC0YpECzSl5kCBD3u1KUPduAgmnO732G9aqw1aLdQszw8LIXBOE1cYvOf3KYQmQ5vdFV6i4eFOttVvIa2XerkHVGiPd1OzTk32tEOchCbnUqpzW3QqCG7AjEmmKHFGuo5T2_UQDUERaSVRa26oAZHYuePCzDrwbY.bcHnZQWFYjmL64ZRnMieVsUH5JVx-T-WY88kj8V-uno&amp;amp;dib_tag=se&amp;amp;keywords=windows+internals&amp;amp;qid=1725808358&amp;amp;sprefix=windows+internals%2Caps%2C155&amp;amp;sr=8-1&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=08d00ee830fe99b6e648add99e1b64c5&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">Windows Internals&lt;/a> 5th edition book in 2021 (which I never fully read due to its incredible detail and length). None of these made me learn what I wanted though: the ways in which NT fundamentally differs from Unix, if at all.&lt;/p>
&lt;p>Then, at the end of 2023, the &lt;a href="/2023/11/windows-nt-peeking-into-the-cradle.html">Showstopper&lt;/a> book sparked this curiosity once again. And soon, a new thought came to mind: the Windows Internals 5th edition book was too obtuse but&amp;hellip; what about the first edition? Surely it must have been easier to digest because the system was much simpler back in the early 1990s. So lo and behold, I searched for this edition, found it under the title &lt;a href="https://www.amazon.com/Inside-Windows-NT-Helen-Custer/dp/155615481X?crid=1DW4GVN0DXZR4&amp;amp;dib=eyJ2IjoiMSJ9.T5IY8OXVnanYLCjv3cX4UA18lTT67S00r-GHOWD5mzHAUtLk4np5tyXDZB6t25N5JGVVo4y_Yi-4Fv6TrXMJ_rs_BjLK_hTqetPsJAsHRsaw6ZbhMH07OitAfS2LpgEmdUNfdU8KoIM8BEJHof4aPIJMHkemWy0IFcaXoyQ9TLMcgLdTlMVF5Yqen-dG6NZeJ03UYK9NJXzHMgt4noQO1UmhxTMA2xw2Bhi-GDbZT4k.vGalnM5AX1xcE149fGsl5IjcbdD3yE0gw0F7_RojBV4&amp;amp;dib_tag=se&amp;amp;keywords=inside+windows+nt&amp;amp;qid=1725808269&amp;amp;sprefix=inside+windows+nt%2Caps%2C160&amp;amp;sr=8-1&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=3f47c0d7cea553fed13057b16d417b6c&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">Inside Windows NT&lt;/a>, read it cover to cover, and took notes to evaluate NT vs. Unix.&lt;/p>
&lt;p>Which brings me to this article&amp;mdash;a collection of thoughts comparing the design of NT (July 1993) against contemporary Unix systems such as 4.4BSD (June 1994) or Linux 1.0 (March 1994). Beware that, due to my background, the text is written from the point of view of a Unix &amp;ldquo;expert&amp;rdquo; and an NT &amp;ldquo;clueless&amp;rdquo;, so it focuses on describing the things that NT does differently.&lt;/p>
&lt;div class="container action-highlight p-4 my-4 d-md-none">
&lt;div class="row text-center">
&lt;p>A blog on operating systems, programming languages, testing, build systems, my own software
projects and even personal productivity. Specifics include FreeBSD, Linux, Rust, Bazel and
EndBASIC.&lt;/p>
&lt;/div>
&lt;div class="row">
&lt;div class="col">
&lt;div class="form-group">
&lt;form action="https://endtracker.azurewebsites.net/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/subscribers/add" method="post">
&lt;input type="text" name="email"
placeholder="Enter your email"
class="form-control input-sm text-center my-1"/>
&lt;button type="submit" class="btn btn-primary btn-block my-1">Subscribe&lt;/button>
&lt;/form>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="row px-2">
&lt;div class="col col-sm-5 text-left">
&lt;small>&lt;span class="subscriber-count">0&lt;/span> subscribers&lt;/small>
&lt;/div>
&lt;div class="col col-sm-7 text-right">
&lt;p>
&lt;a rel="me" href="https://mastodon.online/@jmmv">
&lt;img src="/images/badges/mastodon-logo.svg" width="32px" height="32px" alt="Follow @jmmv on Mastodon">
&lt;/a>
&lt;a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fjmmv.dev%2F&amp;amp;screen_name=jmmv">
&lt;img src="/images/badges/Twitter_logo_blue.svg" width="32px" height="32px" alt="Follow @jmmv on Twitter">
&lt;/a>
&lt;a href="/feed.xml">&lt;img src="/images/badges/feed-icon-28x28.png" alt="RSS feed">&lt;/a>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h1 id="mission">Mission&lt;/h1>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Unix">Unix&lt;/a>&amp;rsquo;s history is long&amp;mdash;much longer than NT&amp;rsquo;s. Unix&amp;rsquo;s development started in 1969 and its primary goal was to be a convenient platform for programmers. Unix was inspired by &lt;a href="https://en.wikipedia.org/wiki/Multics">Multics&lt;/a>, but compared to that other system, Unix focused on simplicity which is a trait that let it triumph over Multics. Portability and multitasking were not original goals of the Unix design though: these features were retrofitted in the many &amp;ldquo;forks&amp;rdquo; and reinventions of Unix years later.&lt;/p>
&lt;p>On Microsoft&amp;rsquo;s side, the first release of MS-DOS launched in August 1981 and the first release of &amp;ldquo;legacy Windows&amp;rdquo; (the DOS-based editions) launched in November 1985. While MS-DOS was a widespread success, it wasn&amp;rsquo;t until &lt;a href="https://en.wikipedia.org/wiki/Windows_3.0">Windows 3.0&lt;/a> in May 1990 that Windows started to really matter. Windows NT was conceived in 1989 and saw the light with the NT 3.1 release in July 1993.&lt;/p>
&lt;p>This timeline gave Microsoft an edge: the design of NT started 20 years after Unix&amp;rsquo;s, and Microsoft already had a large user base thanks to MS-DOS and legacy Windows. The team at Microsoft designing NT had the hindsight of these developments, previous experience developing other operating systems, and access to more modern technology, so they could &amp;ldquo;shoot for the moon&amp;rdquo; with the creation of NT.&lt;/p>
&lt;p>In particular, NT started with the following design goals as part of its mission, which are in stark contrast to Unix&amp;rsquo;s:&lt;/p>
&lt;ol>
&lt;li>portability,&lt;/li>
&lt;li>support for multiprocessing systems (SMP), and&lt;/li>
&lt;li>compatibility with DOS, legacy Windows, OS/2, and POSIX.&lt;/li>
&lt;/ol>
&lt;p>These were not goals to scoff at and meant that NT started with solid design principles from the get go. In other words: these features were all present from day one and not bolted on at a later stage like they were in many Unixes.&lt;/p>
&lt;h1 id="the-kernel">The kernel&lt;/h1>
&lt;p>Now that we know some of these design goals and constraints, let&amp;rsquo;s take a look at the specifics of how they are implemented.&lt;/p>
&lt;p>Unix is, with few exceptions like &lt;a href="https://www.minix3.org/">Minix&lt;/a> or &lt;a href="https://www.gnu.org/software/hurd/">GNU Hurd&lt;/a>, implemented as a monolithic kernel that exposes a collection of system calls to interact with the facilities offered by the operating system. NT, on the other hand, is a hybrid between a monolithic kernel and a microkernel: the privileged component, known as the &lt;em>executive&lt;/em>, presents itself as a collection of modular components to user-space &lt;em>subsystems&lt;/em>. The user-space subsystems are special processes which &amp;ldquo;translate&amp;rdquo; the APIs that the applications consume (be it POSIX, OS/2, etc.) into executive system calls.&lt;/p>
&lt;p>One important piece of the NT executive is the Hardware Abstraction Layer (HAL), a module that provides abstract primitives to access the machine&amp;rsquo;s hardware and that serves as the foundation for the rest of the kernel. This layer is the key that allows NT to run on various architectures, including i386, Alpha, and PowerPC. To put the importance of the HAL in perspective, contemporary Unixes were coupled to a specific architecture: yes, Unix-the-concept was portable because there existed many different variants for different machines, but the implementation was not. SunOS originally only supported the Motorola 68000; 386BSD was the first port of BSD to the Intel architecture; IRIX was the Unix variant for Silicon Graphic&amp;rsquo;s MIPS-based workstations; and so on. This explains why NetBSD&amp;rsquo;s main focus on portability via a minimal shim over the hardware was so interesting at the time: other operating systems, except NT, did &lt;em>not&lt;/em> have this internal clean design, and NT had come years before.&lt;/p>
&lt;p>Another important piece of the NT executive is its support for multiprocessing systems and its preemptive kernel. The kernel has various interrupt levels (&lt;a href="https://en.wikipedia.org/wiki/Spl_(Unix)">SPLs&lt;/a> in BSD terminology) to determine what can interrupt what else (e.g. a clock interrupt has higher priority than a disk interrupt) but, more importantly, the kernel threads can be preempted by other kernel threads. This is &amp;ldquo;of course&amp;rdquo; what every high-performance Unix system does today, but it&amp;rsquo;s not how many Unixes started: those systems started with a kernel that didn&amp;rsquo;t support preemption nor multiprocessing; then they added support for user-space multiprocessing; and then they added kernel preemption. The latter is the hardest step of all and explains the &lt;a href="https://en.wikipedia.org/wiki/FreeBSD_version_history#FreeBSD_5">FreeBSD 5.0&lt;/a> saga fiasco. So it is interesting to see that NT started with the right foundations from its inception.&lt;/p>
&lt;h1 id="objects">Objects&lt;/h1>
&lt;p>NT is an object-oriented kernel. You might think that Unix is too: after all, processes are defined by a struct and file system implementations deal with vnodes (&amp;ldquo;virtual nodes&amp;rdquo;, not to be confused with inodes which are a file system-specific implementation detail). But that&amp;rsquo;s not quite the same as what NT does: NT forces all of these different objects to have a common representation in the system.&lt;/p>
&lt;p>You can rightfully be skeptical about this because&amp;hellip; how can you offer a meaningful abstraction over such disparate things as processes and file handles? You can&amp;rsquo;t, really, but NT forced all of these to inherit from a common object type and, surprisingly, this results in some nice properties:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Centralized access control:&lt;/strong> Objects are exclusively created by the &lt;em>object manager&lt;/em>, which means there is a single place in the code to enforce policy. This is powerful because the semantics for, say, permission checks, can be defined in just one location and applied uniformly throughout the system. NetBSD concluded this was a good idea too, but it wasn&amp;rsquo;t until 2001 that it gained its &lt;a href="https://man.netbsd.org/NetBSD-9.3/kauth.9">Kernel Authorization (kauth)&lt;/a> framework.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Common identity:&lt;/strong> Objects have identities and they are all represented in a single tree. This means that there is a unique namespace for all objects, no matter if we are talking about processes, file handles, or pipes. The objects in the tree are addressable via names (paths) and different portions of the tree can be owned by different subsystems. For example, a portion of the tree can represent a mounted file system, and thus traversing that subtree&amp;rsquo;s root node will cause the file system to resolve the remainder of the path. This is akin to the VFS layer of a Unix system, with the difference that the VFS is exclusively about file systems whereas the object tree is about &lt;em>every single kernel object&lt;/em>. It&amp;rsquo;s true that Unix has attempted to shoehorn other types of non-file objects into the file system via &lt;code>/proc/&lt;/code>, &lt;code>/sys/&lt;/code>, and the like&amp;mdash;but these feel like afterthoughts compared to what NT offers.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Unified event handling:&lt;/strong> All object types have a &lt;em>signaled&lt;/em> state, whose semantics are specific to each object type. For example, a process object enters the signaled state when the process exits, and a file handle object enters the signaled state when an I/O request completes. This makes it trivial to write event-driven code (ehem, async code) in userspace, as a single wait-style system call can await for a group of objects to change their state&amp;mdash;no matter what type they are. Try to wait for I/O and process completion on a Unix system at once; it&amp;rsquo;s painful.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Objects are an NT-specific construct though, and they don&amp;rsquo;t generalize well to all of the APIs that NT intended to support. An example of this is the POSIX subsystem: POSIX doesn&amp;rsquo;t have the same concept of objects as NT does, yet NT has to offer some sort of compatibility with POSIX applications. For this reason, while the POSIX subsystem allocates objects from the executive, this subsystem must keep its own bookkeeping to represent the corresponding POSIX entities and performs the logical translation between the two on the fly. The Win32 subsystem, on the other hand, just hands objects to clients without an intermediary.&lt;/p>
&lt;h1 id="processes">Processes&lt;/h1>
&lt;p>Processes are a common entity in both NT and Unix but they aren&amp;rsquo;t quite the same. In Unix, processes are represented in a tree, which means that each process has a parent and a process can have zero or more children. In NT, however, there is no such relationship: processes can &amp;ldquo;inherit&amp;rdquo; resources from their creators&amp;mdash;any type of object, basically&amp;mdash;but they are standalone entities after they are created.&lt;/p>
&lt;p>What wasn&amp;rsquo;t common back when NT was designed were threads: Mach was the first Unix-like kernel to integrate threads in 1985, which means that other Unixes adopted this concept later on and had to retrofit it into their existing designs. For example, Linux chose to represent threads as processes, each with its own PID, in its 2.0 release in June 1996; and NetBSD didn&amp;rsquo;t get threads, represented as separate entities from processes, until its 2.0 release in 2004. Contrary to Unix, NT chose to support threads from the very beginning, knowing that they were a necessity for high-performance computing on SMP machines.&lt;/p>
&lt;p>NT doesn&amp;rsquo;t have signals in the traditional Unix sense. What it does have, however, are &lt;em>alerts&lt;/em>, and these can be kernel mode and user mode. User mode alerts must be waited for as any other object and kernel mode alerts are invisible to processes. The POSIX subsystem uses kernel mode alerts to emulate signals. Note that signals have often been called a wart in Unix because of the way they interfere with process execution: handling signals correctly is a really difficult endeavor, so it sounds like NT&amp;rsquo;s alternative is more elegant.&lt;/p>
&lt;p>An interesting recent development in NT-land has been the introduction of &lt;a href="https://www.microsoft.com/en-us/research/project/drawbridge/">picoprocesses&lt;/a>. Up until this feature was added, processes in NT were quite heavyweight: new processes would get a bunch of the NT runtime libraries mapped in their address space at startup time. In a picoprocess, the process has minimal ties to the Windows architecture, and this is used to implement Linux-compatible processes in WSL 1. In a way, picoprocesses are closer to Unix processes than native Windows processes, but they are not used for much anymore&amp;mdash;even if they have only existed since August 2016&amp;mdash;because of the move to WSL 2.&lt;/p>
&lt;p>Lastly, as much as we like to bash Windows for security problems, NT started with an advanced security design for early Internet standards given that the system works, basically, as a capability-based system. The first user process that starts after logon gets an access token from the kernel representing the privileges of the user session, and the process and its subprocesses must supply this token to the kernel to assert their privileges. This is different from Unix where processes just have identifiers and the kernel needs to keep track of what each process can do in the process table.&lt;/p>
&lt;h1 id="compatibility">Compatibility&lt;/h1>
&lt;p>As mentioned in the introduction, a major goal of NT was to be compatible with applications written for legacy Windows, DOS, OS/2 and POSIX. One reason for this was technical, as this forced the system to have an elegant design; the other reason was political, as NT was a joint development with IBM and NT &lt;em>had&lt;/em> to support OS/2 applications even if, in the end, NT ended up &lt;em>being&lt;/em> Windows.&lt;/p>
&lt;p>This need for compatibility forced NT&amp;rsquo;s design to be significantly different than Unix&amp;rsquo;s. In Unix, user-space applications talk to the kernel directly via its system call interface, and this interface &lt;em>is&lt;/em> the Unix interface. Oftentimes, &lt;a href="https://utcc.utoronto.ca/~cks/space/blog/programming/Go116OpenBSDUsesLibc">but not always&lt;/a>, the C library provides the glue to call the kernel and applications never issue system calls themselves&amp;mdash;but that&amp;rsquo;s a minor detail.&lt;/p>
&lt;p>Contrast this to NT where applications do &lt;em>not&lt;/em> talk to the executive (the kernel) directly. Instead, each application talks to one specific protected &lt;em>subsystem&lt;/em>, and these subsystems are the ones that implement the APIs of the various operating systems that NT wanted to be compatible with. These subsystems are implemented as user-space servers (they are not inside the NT &amp;ldquo;microkernel&amp;rdquo;). Support for Windows applications comes from the Win32 server, which is special because it&amp;rsquo;s the only one that&amp;rsquo;s directly visible by users: it controls console programs and DOS terminals, and it has certain privileges for performance reasons.&lt;/p>
&lt;p>Compared to traditional Unix, NT&amp;rsquo;s design is very different because the BSDs and Linux have a monolithic kernel. These kernels expose a system call interface that userspace applications leverage to interact directly with the system. The BSDs, however, have offered support to run alternate &lt;em>binaries&lt;/em> for a long time, all within the monolithic kernel: the way this works is by exposing different system call tables to userspace depending on the binary that&amp;rsquo;s being run, and then translating those &amp;ldquo;foreign&amp;rdquo; system calls to whatever the kernel understands. Linux has limited support for this as well via &lt;a href="https://man7.org/linux/man-pages/man2/personality.2.html">&lt;em>personalities&lt;/em>&lt;/a>.&lt;/p>
&lt;p>Even though the BSD approach is quite different from how NT handles supporting other systems, WSL 1 is extremely similar and is not a subsystem in the literal terms that subsystems were originally defined. In WSL 1, the NT kernel marks Linux processes as picoprocesses and, from there on, exposes a different system call interface to them. Within the NT kernel, those Linux-specific system calls are translated into NT operations and served within the same kernel&amp;mdash;just like BSD&amp;rsquo;s Linux compatibility does. The only problem is that, NT not being Unix, its &amp;ldquo;emulation&amp;rdquo; of Linux is tricky and much slower than what BSD can offer. It&amp;rsquo;s a pity that &lt;a href="/2020/11/wsl-lost-potential.html">WSL 2 lost the essence of this design&lt;/a> and went with a full-on VM design&amp;hellip;&lt;/p>
&lt;p>To finish this section, two more interesting details: a goal of NT&amp;rsquo;s design was to allow seamless I/O redirection between subsystems, all from a single shell; and subsystems are exposed to applications via &lt;em>ports&lt;/em> which are, of course, NT objects and are similar to how Mach allows processes and servers to communicate.&lt;/p>
&lt;h1 id="virtual-memory">Virtual memory&lt;/h1>
&lt;p>NT, just as Unix, relies on a Memory Management Unit (MMU) with pagination to offer protection across processes and to offer virtual memory. Paging in user-space processes is a common mechanism to give them a larger address space than the amount of physical memory on a machine. But one thing that put NT ahead of contemporary Unix systems is that the kernel itself can be paged out to disk too. Obviously not the whole kernel&amp;mdash;if it all were pageable, you&amp;rsquo;d run into the situation where a resolving kernel page fault requires code from a file system driver that was paged out&amp;mdash;but large portions of it are. This is not particularly interesting these days because kernels are small compared to the typical installed memory on a machine, but it certainly made a big difference in the past where every byte was precious.&lt;/p>
&lt;p>Additionally, while we take the way virtual memory and paging works these days for granted, this was a big area of research back when NT was designed. Older Unix implementations had separate memory caches for the file system and virtual memory, and it wasn&amp;rsquo;t until 1987 that &lt;a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;amp;type=pdf&amp;amp;doi=a36b01c8fd5071f64b981dd3ffc66a6bce56736d">SunOS implemented a unified virtual memory architecture&lt;/a> to reduce the overheads of this old design.&lt;/p>
&lt;p>In contrast, NT started with a unified memory architecture from the beginning. You&amp;rsquo;d say that this was easy to do because they had the hindsight of the inefficiencies found in Unix and could see the solution that SunOS had implemented before the design of NT started. But regardless, this made NT &amp;ldquo;more advanced&amp;rdquo; that many alternate operating systems back then, and it has to be noted that other systems like NetBSD didn&amp;rsquo;t catch up until 2002 with the implementation of the &lt;a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix2000/freenix/full_papers/silvers/silvers.pdf">Unified Buffer Cache (UBC)&lt;/a> in NetBSD 1.6.&lt;/p>
&lt;p>An interesting difference between NT and Unix is how they manage and represent shared memory. In NT, shared memory sections are (surprise) objects and are thus subject to the exact same access validation checks as any other object. Furthermore, they are addressable in the same manner as any other object because they are part of the single object tree. Contrast this to Unix where this feature is bolted on: shared memory objects have a &lt;a href="https://man7.org/linux/man-pages/man3/shm_open.3.html">different namespace&lt;/a>, a different API to every other entity, and thus typical permissions don&amp;rsquo;t apply to them.&lt;/p>
&lt;h1 id="io-subsystem">I/O subsystem&lt;/h1>
&lt;p>Early versions of Unix only supported one file system. For example, it wasn&amp;rsquo;t until 4.3BSD in 1990 that the BSDs gained the Virtual File System (VFS) abstraction to support more than just UFS. NT, on the other hand, started with a design that allowed multiple file systems.&lt;/p>
&lt;p>In order to support multiple file systems, the kernel has to expose their namespaces in some way. Unix combines the file systems under a single file hierarchy via mount points: the VFS layer provides the mechanisms to identify which nodes correspond to the root of a file system and redirects requests to those file system drivers when traversing a path. NT has a similar design even if, from the standard user interface, file systems appear as disjoint drives: internally, the executive represents file systems as objects in the object tree, and each object is responsible for parsing the remainder of a path. Those file system objects are remapped as DOS drives so that userspace can access them. And guess what? The DOS drives are also objects under a separate subtree that redirects I/O to the file systems they reference.&lt;/p>
&lt;p>In file system terms, NT ended up shipping with NTFS. NTFS was a really advanced file system for its time even if we like to bash on it for its poor performance (a &lt;a href="https://www.youtube.com/watch?v=qbKGw8MQ0i8">misguided claim&lt;/a>). The I/O subsystem of NT, in combination with NTFS, brought 64-bit addressing, journaling, and even Unicode file names. Linux didn&amp;rsquo;t get 64-bit file support until the late 1990s and didn&amp;rsquo;t get journaling until ext3 launched in 2001. &lt;a href="https://www.usenix.org/conference/1999-usenix-annual-technical-conference/soft-updates-technique-eliminating-most">Soft updates&lt;/a>, an alternate fault tolerance mechanism, didn&amp;rsquo;t appear in FreeBSD until 1998. And Unix represents &lt;a href="/2023/12/strings-encodings-nuls-and-bazel.html">filenames as nul-terminated byte arrays&lt;/a>, not Unicode.&lt;/p>
&lt;p>Other features that NT included at launch were disk stripping and mirroring&amp;mdash;what we know today as RAID&amp;mdash; and device hot plugging. These features were not a novelty given that SunOS did include RAID support since the early 1990s, but what&amp;rsquo;s interesting is that these were all accounted for as part of the original design.&lt;/p>
&lt;p>At a higher level, &lt;em>the&lt;/em> thing that makes the I/O subsystem of NT much more advanced than Unix&amp;rsquo;s is the fact that its interface is asynchronous in nature and has been like that since the very beginning. To put this in perspective, FreeBSD didn&amp;rsquo;t see support for &lt;a href="https://man7.org/linux/man-pages/man7/aio.7.html">&lt;code>aio(7)&lt;/code>&lt;/a> until FreeBSD 3.0 in 1998, and Linux didn&amp;rsquo;t see this either until Linux 2.5 in 2002. And even if support for asynchronous I/O has existed in Unix systems for more than 20 years now, it&amp;rsquo;s still not widespread: few people know of these APIs, the vast majority of applications don&amp;rsquo;t use them, and their performance is poor. Linux&amp;rsquo;s &lt;a href="https://en.wikipedia.org/wiki/Io_uring">&lt;code>io_uring&lt;/code>&lt;/a> is a relatively recent addition that improves asynchronous I/O, but it has been a significant source of security vulnerabilities and is not in widespread use.&lt;/p>
&lt;h1 id="networking">Networking&lt;/h1>
&lt;p>The Internet is everywhere today, but when NT was designed, that was not the case. Looking back at the Microsoft ecosystem, DOS 3.1 (1987) included the foundations for file sharing in the FAT file system, yet the &amp;ldquo;OS&amp;rdquo; itself did not provide any networking features: a separate product called Microsoft Networks (MS-NET) did. Windows 3.0 (1990) included support for NetBIOS, which allowed primitive printer and file sharing on local networks, but support for TCP/IP was nowhere to be seen.&lt;/p>
&lt;p>In contrast, Unix &lt;em>was&lt;/em> the Internet: all foundational Internet protocols were written for and with it. During the design of NT, it was therefore critical to account for good network support, and indeed NT did launch with networking features. As a result, NT did support both Internet protocols and the traditional LAN protocols used in pre-existing Microsoft environments, which put it ahead of Unix in corporate environments.&lt;/p>
&lt;p>An an example, take NT&amp;rsquo;s network domains. In Unix, network administrators typically synchronized user accounts across machines by hand; they&amp;rsquo;d maybe use the X.500 directory protocol (1988) and Kerberos (1980s) for user authentication, which systems like SunOS implemented, but these technologies weren&amp;rsquo;t particularly simple. Instead, NT offered &lt;em>domains&lt;/em> from the get go, which integrated directory and authentication features, and it seems to me that these &amp;ldquo;won the day&amp;rdquo; in corporate networks because they were much easier to set up and were built into the system.&lt;/p>
&lt;p>The goal of synchronized user accounts is to share resources across machines, primarily files, and when doing so, the way to represent permissions matters. For the longest time, Unix only offered the simplistic read/write/execute permission sets for each file. NT, on the other hand, came with advanced ACLs from the get go&amp;mdash;something that&amp;rsquo;s still a sore spot on Unix. Even though Linux and the BSDs now have ACLs too, their interfaces are inconsistent across systems and they feel like an alien add-on to the design of the system. On NT, ACLs work at the object level, which means they apply consistently throughout all kernel features.&lt;/p>
&lt;p>And speaking of sharing files, we must talk about networked file systems. In Unix, the de facto file system was NFS, whereas on NT it was SMB. SMB was inherited from MS-NET and LAN Manager and is implemented in the kernel via a component called the &lt;em>redirector&lt;/em>. In essence, the redirector is &amp;ldquo;just&amp;rdquo; one more file system, like NFS is on Unix, that traps file operations and sends them over the network, which brings us to comparing RPC systems.&lt;/p>
&lt;p>Even though protobuf and gRPC may seem like novel ideas due to their widespread use, they are based on old ideas. On Unix, we had Sun RPC from the early 1980s, primarily to support NFS. Similarly, NT shipped with built-in RPC support via its own DSL&amp;mdash;known as MIDL to specify interface definitions and to generate code for remote procedures&amp;mdash;and its own facility to implement RPC clients and servers.&lt;/p>
&lt;p>Moving down the stack, Unix systems have never been big on supporting arbitrary drivers: remember that Unix systems were typically coupled to specific machines and vendors. NT, on the other hand, intended to be an OS for &amp;ldquo;any&amp;rdquo; machine and was sold by a software company, so supporting drivers written by others was critical. As a result, NT came with the Network Driver Interface Specification (NDIS), an abstraction to support network card drivers with ease. To this day, manufacturer-supplied drivers are just not a thing on Linux, which leads to interesting contraptions like the &lt;a href="https://ndiswrapper.sourceforge.net/wiki/index.php/Main_Page">ndiswrapper&lt;/a>, a very popular shim in the early 2000s to be able to reuse Windows drivers for WiFi cards on Linux.&lt;/p>
&lt;p>Finally, another difference between NT and Unix lies in their implementation of named pipes. Named pipes are a local construct in Unix: they offer a mechanism for two processes on the same machine to talk to each other with a persistent file name on disk. NT has this same functionality, but its named pipes can operate over the network. By placing a named pipe on a shared file system, two applications on different computers can communicate with each other without having to worry about the networking details.&lt;/p>
&lt;h1 id="user-space">User-space&lt;/h1>
&lt;p>We are getting close to the end, I promise. There are just a few user-space topics to briefly touch on:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Configuration:&lt;/strong> NT centralized system and application configuration under a database known as the &lt;em>registry&lt;/em>, freeing itself from the old &lt;code>CONFIG.SYS&lt;/code>, &lt;code>AUTOEXEC.BAT&lt;/code> and the myriad INI files that legacy Windows used. This made some people very angry, but in the end, a unified configuration interface is beneficial to everyone: applications are easier to write because there is a single foundation to support, and users have an easier time tuning their system because there is just one place to look at.&lt;/p>
&lt;p>Unix, on the other hand, is still plagued by dozens of DSLs and inconsistent file locations. Each program that supports a configuration file has its own made-up syntax, and knowing which locations the program reads is difficult and not always well-documented. The Linux ecosystem has pushed for a more NT-like approach via XDG and dconf (previously GConf) but&amp;hellip; it&amp;rsquo;s an uphill battle: while desktop components use these technologies exclusively, the foundational components of the system may never adopt them, leaving an inconsistent mess behind.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Internationalization:&lt;/strong> Microsoft, being the large company that was already shipping Windows 3.x across the world, understood that localization was important and made NT support such feature from the very beginning. Contrast this to Unix where UTF support didn&amp;rsquo;t start to show up until the late 1990s, and supporting different languages came via the optional &lt;code>gettext&lt;/code> add-on.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>The C language:&lt;/strong> One thing Unix systems like FreeBSD and NetBSD have fantasized about for a while is coming up with their own dialect of C to &lt;a href="https://cs.rochester.edu/u/jzhou41/papers/freebsd_checkedc.pdf">implement the kernel in a safer manner&lt;/a>. This has never gone anywhere except, maybe, for Linux relying on GCC-only extensions. Microsoft, on the other hand, had the privilege of owning a C compiler, so they did do this with NT, which is written in Microsoft C. As an example, NT relies on Structured Exception Handling (SEH), a feature that adds try/except clauses to handle software and hardware exceptions. I wouldn&amp;rsquo;t say this is a big plus, but it&amp;rsquo;s indeed a difference.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>NT was groundbreaking technology when it launched. As I presented above, many of the features we take for granted today in systems design were present in NT since its inception, whereas almost all other Unix systems had to gain those features slowly over time. As a result, such features don&amp;rsquo;t always integrate seamlessly with Unix philosophies.&lt;/p>
&lt;p>Today, however, it&amp;rsquo;s not clear to me that NT is truly &amp;ldquo;more advanced&amp;rdquo; than, say, Linux or FreeBSD. It is true that NT had more solid design principles at the onset and more features that its contemporary operating systems, but nowadays&amp;hellip; the differences are blurry. Yes, NT is advanced, but not significantly more so than modern Unixes.&lt;/p>
&lt;p>What I find disappointing is that, even though NT has all these solid design principles in place&amp;hellip; bloat in the UI doesn&amp;rsquo;t let the design shine through. The sluggishness of the OS even on super-powerful machines is &lt;a href="/2023/06/fast-machines-slow-machines.html">painful to witness&lt;/a> and might even lead to the demise of this OS.&lt;/p>
&lt;p>I&amp;rsquo;ll leave you with the books used to write this article in case you want to go through my learning journey. I had to skip over tons of interesting details, as you can imagine, so these are worth a read:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.amazon.com/Inside-Windows-NT-Helen-Custer/dp/155615481X?crid=1DW4GVN0DXZR4&amp;amp;dib=eyJ2IjoiMSJ9.T5IY8OXVnanYLCjv3cX4UA18lTT67S00r-GHOWD5mzHAUtLk4np5tyXDZB6t25N5JGVVo4y_Yi-4Fv6TrXMJ_rs_BjLK_hTqetPsJAsHRsaw6ZbhMH07OitAfS2LpgEmdUNfdU8KoIM8BEJHof4aPIJMHkemWy0IFcaXoyQ9TLMcgLdTlMVF5Yqen-dG6NZeJ03UYK9NJXzHMgt4noQO1UmhxTMA2xw2Bhi-GDbZT4k.vGalnM5AX1xcE149fGsl5IjcbdD3yE0gw0F7_RojBV4&amp;amp;dib_tag=se&amp;amp;keywords=inside+windows+nt&amp;amp;qid=1725808269&amp;amp;sprefix=inside+windows+nt%2Caps%2C160&amp;amp;sr=8-1&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=3f47c0d7cea553fed13057b16d417b6c&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">Inside Windows NT, 1st edition&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/Design-Implementation-4-4-Operating-System/dp/0201549794?crid=289Z6M1NYSR8L&amp;amp;dib=eyJ2IjoiMSJ9.4h3ssrq_vTu9MaMquFvEbw.YxrfgRofVfFKstV74Q_LR-XOseMUlCrcvkehHW6y5Yc&amp;amp;dib_tag=se&amp;amp;keywords=design+and+implementation+of+4.4bsd&amp;amp;qid=1725808321&amp;amp;sprefix=design+and+implementation+of+4.4bsd%2Caps%2C158&amp;amp;sr=8-1&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=a0d6cc7fb69bc72b4649609390255bff&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">The Design and Implementation of the BSD 4.4 operating system&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>And if you want to &lt;em>continue&lt;/em> my journey and truly dive deep into how each piece of &lt;em>modern&lt;/em> NT and Unix works, the newer editions are a must read:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.amazon.com/Windows-Internals-Part-architecture-management/dp/0735684189?crid=2F7UR8S48RP6O&amp;amp;dib=eyJ2IjoiMSJ9.p9cBb_-Q8GjuK0z0kDLKG6xoExPM_2QWt_jn0PlqVBSWYNyqRp2Cd7MHXFeQ4EiRACaX_Y_9xzECC0YpECzSl5kCBD3u1KUPduAgmnO732G9aqw1aLdQszw8LIXBOE1cYvOf3KYQmQ5vdFV6i4eFOttVvIa2XerkHVGiPd1OzTk32tEOchCbnUqpzW3QqCG7AjEmmKHFGuo5T2_UQDUERaSVRa26oAZHYuePCzDrwbY.bcHnZQWFYjmL64ZRnMieVsUH5JVx-T-WY88kj8V-uno&amp;amp;dib_tag=se&amp;amp;keywords=windows+internals&amp;amp;qid=1725808358&amp;amp;sprefix=windows+internals%2Caps%2C155&amp;amp;sr=8-1&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=08d00ee830fe99b6e648add99e1b64c5&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">Windows Internals, part 1, 7th edition&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/Windows-Internals-Part-2-7th/dp/0135462401?pd_rd_w=nmV0o&amp;amp;content-id=amzn1.sym.3858a394-39a9-4946-90e6-86a3153d2546&amp;amp;pf_rd_p=3858a394-39a9-4946-90e6-86a3153d2546&amp;amp;pf_rd_r=FHEDS8GZCH0T6TA52GQD&amp;amp;pd_rd_wg=VUb9D&amp;amp;pd_rd_r=9f324aa8-7d91-4b6f-8c48-a8f09f1e8717&amp;amp;pd_rd_i=0135462401&amp;amp;psc=1&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=a174f72e73013533f3ee8ca3f2cedd1c&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">Windows Internals, part 2, 7th edition&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/Design-Implementation-FreeBSD-Operating-System/dp/0321968972?crid=23A95KGH3XOCE&amp;amp;dib=eyJ2IjoiMSJ9.WhRSQ2C6OFnyaT9IERCxYyLspyOoCkFSreJbs2Nia7zcYcNs68qdHuNBiIvERx987eDwl_H7YrMkTIj1575PGQ.orMCRh-1sVoQgFKgV2RtSiAxU3OA5fYRt6oAI3SV-p0&amp;amp;dib_tag=se&amp;amp;keywords=the+design+and+implementation+of+freebsd&amp;amp;qid=1725850081&amp;amp;s=books&amp;amp;sprefix=the+design+and+implementation+of+freebsd%2Cstripbooks%2C136&amp;amp;sr=1-1&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=ac0d39d3f598c50cd1d9dc650d2b4dd7&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">The Design and Implementation of the FreeBSD operating system, 2nd edition&lt;/a>.&lt;/li>
&lt;/ul></description><enclosure url="https://jmmv.dev/images/2024-09-09-windows-nt-vs-unix-design.jpg" length="1049554" type="image/jpeg"/></item><item><title>Picking glibc versions at runtime</title><link>https://jmmv.dev/2024/08/glibc-versions-runtime.html</link><pubDate>Sun, 11 Aug 2024 10:15:00 +0200</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/08/glibc-versions-runtime.html</guid><description>&lt;p>In a recent work discussion, I came across an argument that didn&amp;rsquo;t sound quite right. The claim was that we needed to set up containers in our developer machines in order to run tests against a modern glibc. The justifications were that using &lt;code>LD_LIBRARY_PATH&lt;/code> to load a different glibc didn&amp;rsquo;t work and statically linking glibc wasn&amp;rsquo;t possible either.&lt;/p>
&lt;p>But&amp;hellip; running a program against a version of glibc that&amp;rsquo;s different from the one installed on the system seems like a pretty standard requirement, doesn&amp;rsquo;t it? Consider this: how do the developers of glibc test their changes? glibc has existed for much longer than containers have. And before containers existed, they surely weren&amp;rsquo;t testing glibc changes by installing modified versions of the library over the system-wide one and YOLOing it.&lt;/p>
&lt;p>So. What options do we really have? To answer this question, we need to look at how dynamic binaries work and what glibc is.&lt;/p>
&lt;h1 id="static-binaries">Static binaries&lt;/h1>
&lt;p>Static binaries are binaries whose program code is all contained in one executable. Such binaries can access system services via system calls&amp;mdash;or else their utility would be minimal because they wouldn&amp;rsquo;t be able to interact with the OS&amp;mdash;but their binary code is, in a sense, &amp;ldquo;complete&amp;rdquo;: what you see in the executable is exactly what gets laid out in memory for execution.&lt;/p>
&lt;p>Here is how a sample static binary looks like. This is for a &amp;ldquo;hello world&amp;rdquo; program written in Go, which was the easiest thing to put together because Go is designed to bypass the C library &lt;a href="https://utcc.utoronto.ca/~cks/space/blog/programming/Go116OpenBSDUsesLibc">almost everywhere&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ file hello_go
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hello_go: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=2H_kXH_UFAOR5K6ibAke/aj_2VBvakhN2KujNxtMN/K5TtKP8HHiX65VdbA19s/KxdoZHQEiOxCEbwRE346, with debug_info, not stripped
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the above, note that &lt;code>file&lt;/code> claims that our sample &lt;code>hello_go&lt;/code> program is &lt;em>statically linked&lt;/em>.&lt;/p>
&lt;p>When we ask the shell to run this &lt;code>hello_go&lt;/code> binary, the shell forks a new process and uses the &lt;code>exec(2)&lt;/code> family of system calls to replace the running image of the process with the content of &lt;code>hello_go&lt;/code>. And because &lt;code>hello_go&lt;/code> is statically linked, the text segment of the binary is loaded verbatim into the process.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-08-11-glibc-versions-runtime-static.png" />
&lt;figcaption>Representation of how a static binary is directly mapped into a process when executed.&lt;/figcaption>
&lt;/figure>
&lt;h1 id="dynamic-binaries">Dynamic binaries&lt;/h1>
&lt;p>Let&amp;rsquo;s compare the above to a dynamically-linked binary by looking at the same &amp;ldquo;hello world&amp;rdquo; program written in C and linked against glibc:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ file hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">hello_c: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=feeb95b014ef8151780e23fc7ca15de0599f05df, for GNU/Linux 3.2.0, not stripped
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that &lt;code>file&lt;/code> now claims that our sample &lt;code>hello_c&lt;/code> program is &lt;em>dynamically linked&lt;/em> and, indeed, if we look at its libraries&amp;mdash;quick reminder: &lt;a href="/2023/07/ldd-untrusted-binaries.html">don&amp;rsquo;t use &lt;code>ldd&lt;/code> on untrusted executables&lt;/a>!&amp;mdash;we see that glibc (&lt;code>libc.so.6&lt;/code>) is there:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ ldd hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> linux-vdso.so.1 (0x00007f54f25aa000)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> libc.so.6 =&amp;gt; /lib64/libc.so.6 (0x00007f54f239b000)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> /lib64/ld-linux-x86-64.so.2 (0x00007f54f25ac000)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But wait a second. There is more than just &lt;code>libc.so.6&lt;/code> in the output of &lt;code>ldd&lt;/code>. In particular, there is this other &lt;code>/lib64/ld-linux-x86-64.so.2&lt;/code> &amp;ldquo;library&amp;rdquo; and, if you pay close attention to the earlier output of &lt;code>file&lt;/code>, you&amp;rsquo;ll note that it claims that this is an &lt;em>interpreter&lt;/em>. Why is there an &amp;ldquo;interpreter&amp;rdquo; if we are running machine code?! Did they lie to us when they said C was a compiled language?&lt;/p>
&lt;p>Not so fast, no. &amp;ldquo;Interpreter&amp;rdquo; in this case should be read as &amp;ldquo;loader&amp;rdquo;, but the &amp;ldquo;interpreter&amp;rdquo; word comes from the nomenclature of the ELF headers stored in the program:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ readelf -l hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Elf file type is EXEC (Executable file)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Entry point 0x401040
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">There are 13 program headers, starting at offset 64
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Program Headers:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Type Offset VirtAddr PhysAddr
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> FileSiz MemSiz Flags Align
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> INTERP 0x0000000000000318 0x0000000000400318 0x0000000000400318
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0x000000000000001c 0x000000000000001c R 0x1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To understand what this interpreter thing is all about, we need to look at what a dynamically-linked executable is. In essence, the program code in the binary is not &lt;em>complete&lt;/em>: the code contains &amp;ldquo;gaps&amp;rdquo; in it that need to be filled with references to portions of code supplied by other libraries before it can begin executing. We can peek into what these &amp;ldquo;gaps&amp;rdquo; are:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ readelf -r hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Relocation section &amp;#39;.rela.dyn&amp;#39; at offset 0x4c0 contains 2 entries:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Offset Info Type Sym. Value Sym. Name + Addend
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">000000403fd8 000100000006 R_X86_64_GLOB_DAT 0000000000000000 __libc_start_main@GLIBC_2.34 + 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">000000403fe0 000300000006 R_X86_64_GLOB_DAT 0000000000000000 __gmon_start__ + 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Relocation section &amp;#39;.rela.plt&amp;#39; at offset 0x4f0 contains 1 entry:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Offset Info Type Sym. Value Sym. Name + Addend
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">000000404000 000200000007 R_X86_64_JUMP_SLO 0000000000000000 puts@GLIBC_2.2.5 + 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>readelf&lt;/code> tells us that the &lt;code>hello_c&lt;/code> binary has three different &amp;ldquo;relocations&amp;rdquo;: in other words, it has three different &amp;ldquo;gaps&amp;rdquo; that need to be patched at runtime with the address of the code that supplies those symbols. The program &lt;em>cannot run&lt;/em> without those being filled in first. So if it cannot begin executing before those references are resolved, how does the program run?&lt;/p>
&lt;p>You might think that the kernel is somehow responsible for dealing with dynamic libraries, but that&amp;rsquo;s not the case. Dynamic libraries are a user-space concept and this is precisely where the interpreter comes into play.&lt;/p>
&lt;p>When we ask the kernel to &lt;code>exec(2)&lt;/code> a dynamically-linked ELF binary, the kernel loads the &lt;em>interpreter&lt;/em> into the process image, &lt;em>not&lt;/em> the code of the binary, and passes the path of the binary to the interpreter.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-08-11-glibc-versions-runtime-dynamic.png" />
&lt;figcaption>Representation of how a dynamic binary is loaded into memory with the help of the dynamic linker, along with glibc and the fixup of a relocation.&lt;/figcaption>
&lt;/figure>
&lt;p>The interpreter, &lt;a href="https://man7.org/linux/man-pages/man8/ld.so.8.html">&lt;code>ld-linux.so&lt;/code>&lt;/a> in our case, is then in charge of setting up the remaining of the process by loading all required shared libraries into the process and then resolving relocations (&amp;ldquo;filling the gaps&amp;rdquo;) among them. In particular, the dynamic linker is the one that loads glibc into the process.&lt;/p>
&lt;p>We can see that this is the case by asking the dynamic linker to print its own file accesses by setting &lt;code>LD_DEBUG=files&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ LD_DEBUG=files /lib64/ld-linux-x86-64.so.2 ./hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: file=./hello_c [0]; generating link map
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: dynamic: 0x0000000000403e08 base: 0x0000000000000000 size: 0x0000000000004010
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: entry: 0x0000000000401040 phdr: 0x0000000000400040 phnum: 13
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: file=libc.so.6 [0]; needed by ./hello_c [0]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: file=libc.so.6 [0]; generating link map
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: dynamic: 0x00007f9aad868960 base: 0x00007f9aad682000 size: 0x00000000001f0b70
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: entry: 0x00007f9aad6ac260 phdr: 0x00007f9aad682040 phnum: 14
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: calling init: /lib64/ld-linux-x86-64.so.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: calling init: /lib64/libc.so.6
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: initialize program: ./hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: transferring control: ./hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Hello, world!
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: calling fini: [0]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: calling fini: /lib64/libc.so.6 [0]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369: calling fini: /lib64/ld-linux-x86-64.so.2 [0]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 75369:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As a final note, see how I chose to invoke the dynamic linker &lt;em>explicitly&lt;/em>&amp;mdash;just as the kernel would do when asked to launch a dynamically-linked binary&amp;mdash;instead of directly running &lt;code>hello_c&lt;/code>.&lt;/p>
&lt;h1 id="playing-with-glibc">Playing with glibc&lt;/h1>
&lt;p>OK, now that we know how a dynamically-linked ELF executable is loaded into memory, let&amp;rsquo;s play with glibc.&lt;/p>
&lt;p>First, we download glibc, build and install it into a temporary directory like &lt;code>/tmp/sysroot/&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ git clone https://sourceware.org/git/glibc.git
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ cd glibc
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">glibc$ git checkout release/2.40/master
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">glibc$ mkdir build
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">glibc/build$ ../configure --prefix=/tmp/sysroot
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">glibc/build$ make -j $(nproc)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">glibc/build$ make -j $(nproc) install
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">glibc/build$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With that done, we should be able to use the new glibc. But&amp;hellip;&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ LD_LIBRARY_PATH=/tmp/sysroot/lib ./hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Floating point exception (core dumped)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Kaboom.&lt;/p>
&lt;p>Indeed, as my coworker claimed, it doesn&amp;rsquo;t seem to be possible to load a different glibc at runtime than the one provided by the system. (YMMV. I got this to work on another system.) Looking at the stack trace of the core dump, we see:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">Stack trace of thread 75841:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#0 0x00007f227d93903b n/a (/tmp/sysroot/lib/libc.so.6 + 0x15403b)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#1 0x00007f227d9f19f6 _dl_sysdep_start (ld-linux-x86-64.so.2 + 0x1c9f6)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#2 0x00007f227d9f335e _dl_start_final (ld-linux-x86-64.so.2 + 0x1e35e)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#3 0x00007f227d9f2048 _start (ld-linux-x86-64.so.2 + 0x1d048)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Our process is crashing within glibc as soon as it is loaded by &lt;code>ld-linux.so&lt;/code>. The details of the crash are not interesting for our purposes, but can we do anything about them?&lt;/p>
&lt;h1 id="playing-with-the-dynamic-linker">Playing with the dynamic linker&lt;/h1>
&lt;p>What&amp;rsquo;s more interesting is to peek into what&amp;rsquo;s shipped by glibc. We can do this by looking under the &lt;code>/tmp/sysroot/&lt;/code> hierarchy that we generated earlier and finding:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ ls /tmp/sysroot/lib/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ld-linux-x86-64.so.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">libc.so
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">libc.so.6
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is &lt;em>a ton&lt;/em> more stuff in there. But what&amp;rsquo;s important to notice is that &lt;em>the dynamic linker ships with glibc&lt;/em>. The two are closely coupled, and using one without the other can lead to the kinds of crashes shown earlier.&lt;/p>
&lt;p>And remember: as we saw above, it is possible to manually launch the dynamic linker. So if we do the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ LD_LIBRARY_PATH=/tmp/sysroot/lib /tmp/sysroot/lib/ld-linux-x86-64.so.2 ./hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Hello, world!
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We don&amp;rsquo;t get a crash anymore. What&amp;rsquo;s even better is that we also &lt;em>seem&lt;/em> to get what we expect without even setting &lt;code>LD_LIBRARY_PATH&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ /tmp/sysroot/lib/ld-linux-x86-64.so.2 ./hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Hello, world!
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We should confirm that the previous invocation of &lt;code>hello_c&lt;/code> is actually using our own glibc version and not the system-supplied one. To do that, we can &lt;code>strace&lt;/code> the dynamic linker; it&amp;rsquo;s just a program after all:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ strace /tmp/sysroot/lib/ld-linux-x86-64.so.2 ./hello_c 2&amp;gt;&amp;amp;1 | grep libc.so
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">openat(AT_FDCWD, &amp;#34;/tmp/sysroot/lib/libc.so.6&amp;#34;, O_RDONLY|O_CLOEXEC) = 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Indeed, &lt;code>hello_c&lt;/code> did use the newly-built glibc by &amp;ldquo;just&amp;rdquo; asking the newly-built dynamic loader to execute the binary.&lt;/p>
&lt;p>To summarize, glibc ships both the &lt;code>ld-linux.so&lt;/code> dynamic linker and the &lt;code>libc.so.6&lt;/code> shared library. The two are closely coupled. Running a binary against the C library without also using a matching dynamic linker can lead to crashes. But running a binary directly with a specific dynamic linker ensures that the binary picks up the matching C library.&lt;/p>
&lt;h1 id="putting-this-to-use">Putting this to use&lt;/h1>
&lt;p>Alright, now that we know that it is actually possible to run a binary against a version of glibc that&amp;rsquo;s &lt;em>not&lt;/em> the one provided by the system, we can try to solve the original problem we faced: running tests against a modern glibc.&lt;/p>
&lt;p>One possibility is to modify the commands used to run the tests to prefix them by &lt;code>.../lib/ld-linux-x86-64.so.2&lt;/code>. This could work but I posit is difficult to integrate with a test execution environment because we don&amp;rsquo;t necessarily control the commands that end up executing the test binaries.&lt;/p>
&lt;p>Another possibility is to modify the way the tests programs are being built by supplying additional linker arguments&amp;mdash;say, via the traditional &lt;code>LDFLAGS&lt;/code>&amp;mdash;to point the linker at a different interpreter with the &lt;code>--Wl,--dynamic-linker&lt;/code> flag:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ cc -o hello_c -Wl,--dynamic-linker=/tmp/sysroot/lib/ld-linux-x86-64.so.2 hello.c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ strace ./hello_c 2&amp;gt;&amp;amp;1 | grep libc.so
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">openat(AT_FDCWD, &amp;#34;/tmp/sysroot/lib/libc.so.6&amp;#34;, O_RDONLY|O_CLOEXEC) = 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Better. Now the built binary &amp;ldquo;knows&amp;rdquo; which glibc to use without us having to specify it explicitly every time we attempt to launch the program.&lt;/p>
&lt;p>What if we aren&amp;rsquo;t &lt;em>building&lt;/em> the binaries? What if we are just reusing a binary that already exists? &lt;code>patchelf&lt;/code> to the rescue:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ cc -o hello_c hello.c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ strace ./hello_c 2&amp;gt;&amp;amp;1 | grep libc.so
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">openat(AT_FDCWD, &amp;#34;/lib64/libc.so.6&amp;#34;, O_RDONLY|O_CLOEXEC) = 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ patchelf --set-interpreter /tmp/sysroot/lib/ld-linux-x86-64.so.2 ./hello_c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ strace ./hello_c 2&amp;gt;&amp;amp;1 | grep libc.so
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">openat(AT_FDCWD, &amp;#34;/tmp/sysroot/lib/libc.so.6&amp;#34;, O_RDONLY|O_CLOEXEC) = 3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Voila. We have modified an existing executable to use whichever glibc we desire.&lt;/p>
&lt;h1 id="versioning-glibc">Versioning glibc&lt;/h1>
&lt;p>But hold on a second. Can we use &lt;em>whichever&lt;/em> glibc really? Is it actually true that glibc versions are completely interchangeable? Unfortunately not.&lt;/p>
&lt;p>A binary built against an old version of glibc is guaranteed to run against a newer version of glibc without recompilation. But the counterpart is not true: a binary built against a new glibc may not run against an older glibc.&lt;/p>
&lt;p>This poses a problem: how can we upgrade glibc across separate production and development environments? If we upgrade production first, binaries built in development environments will continue to work properly but developers may not be able to reproduce bugs found in production. And if we upgrade development environments first, the binaries they produce may be incompatible with production, leading to random crashes later on.&lt;/p>
&lt;p>Here is an idea I&amp;rsquo;ve seen work in the past: let&amp;rsquo;s extend the sysroot concept presented above and version it!&lt;/p>
&lt;p>We start with &lt;code>/usr/sysroot/v1/&lt;/code> which contains the first version of the core system libraries we have to support. All binaries that we build for production deployment are explicitly linked against the &lt;code>ld-linux.so&lt;/code> of this directory, so they are always validated against &lt;code>v1&lt;/code> of the sysroot. &lt;code>v1&lt;/code> remains immutable, which means new builds of the binaries will continue to work as tested in the past.&lt;/p>
&lt;p>Whenever we want to upgrade glibc, we create a &lt;em>new&lt;/em> sysroot, &lt;code>/usr/sysroot/v2/&lt;/code>. This new &lt;code>v2&lt;/code> is deployed to all environments (development and production) that may need to use the new glibc. This deployment step is risk-free because nothing is yet dependent on &lt;code>v2&lt;/code>. After that, we proceed to rebuild all binaries that we want to test and run, this time pointing at the &lt;code>ld-linux.so&lt;/code> within &lt;code>v2&lt;/code>. Deploying these new binaries causes them to use the new version.&lt;/p>
&lt;p>As long as the &lt;code>/usr/sysroot/&lt;/code> subdirectories remain immutable and synchronized across all environments in which binaries run, we are guaranteed to always get a deterministic version of glibc for a given binary.&lt;/p>
&lt;p>That&amp;rsquo;s just a design sketch though. It may or it may not work for you. As I mentioned earlier, I&amp;rsquo;ve seen this work well in a large corporate environment before, but it introduces some extra complexity that you need to take care of.&lt;/p>
&lt;hr>
&lt;p>What&amp;rsquo;s the moral of the story? Containers are, usually, not the best solution to a systems problem. While they might be coerced to deliver the desired results, they come with a heavy cost. Unix has been around for a long time and there exist alternate solutions to problems that don&amp;rsquo;t require full replicas of a functioning system. A ton of the bloat we face today in development tools and deployments comes from the abuse of heavyweight containers. So, every time you think &amp;ldquo;containers!&amp;rdquo;, pause to explore alternatives&amp;mdash;you may find cheaper, more portable, and simpler solutions.&lt;/p>
&lt;p>If you want to learn more about this and related topics, I&amp;rsquo;d recommend grabbing a copy of the classic &lt;a href="https://www.amazon.com/Linkers-Kaufmann-Software-Engineering-Programming/dp/1558604960?&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=421e6bc20c93925de6766b135a24239a&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">&amp;ldquo;Linkers and Loaders&amp;rdquo; book&lt;/a>. It&amp;rsquo;s old, but it&amp;rsquo;s still very relevant.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-08-11-glibc-versions-runtime-dynamic.png" length="36593" type="image/jpeg"/></item><item><title>Kyua graduates</title><link>https://jmmv.dev/2024/08/kyua-graduates.html</link><pubDate>Fri, 02 Aug 2024 08:45:00 -0700</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/08/kyua-graduates.html</guid><description>&lt;p>After years of inactivity, the Kyua project has graduated as an open source citizen and has &lt;a href="https://github.com/freebsd/kyua/">a new home&lt;/a> under the FreeBSD umbrella!&lt;/p>
&lt;p>But uh&amp;hellip; wait, what is Kyua and why is this exciting? To resolve confusion and celebrate this milestone, I&amp;rsquo;d like to revisit what Kyua is, how it came to be, why I stopped working on it for a while, why that was a problem for FreeBSD&amp;mdash;and, indirectly, NetBSD&amp;mdash;and how Kyua being free software has helped keep it alive.&lt;/p>
&lt;h1 id="the-birth-of-atf">The birth of ATF&lt;/h1>
&lt;p>Let&amp;rsquo;s begin by talking about Kyua&amp;rsquo;s predecessor: the Automated Testing Framework (ATF).&lt;/p>
&lt;p>In 2005, I was &lt;a href="/2005/06/soc-accepted.html">one of the lucky pioneers&lt;/a> of the Google Summer of Code (GSoC) program. During that summer, I wrote &lt;a href="http://web.archive.org/web/20071214080206/http://netbsd-soc.sourceforge.net/projects/tmpfs/">the tmpfs file system for NetBSD&lt;/a> and my development process went something like this on an iBook G3:&lt;/p>
&lt;ol>
&lt;li>Modify the kernel.&lt;/li>
&lt;li>Build and install the updated kernel.&lt;/li>
&lt;li>Reboot the laptop.&lt;/li>
&lt;li>&lt;strong>Mount the file system.&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Manually validate new functionality and bug fixes.&lt;/strong>&lt;/li>
&lt;li>Witness the machine crash.&lt;/li>
&lt;li>Force-reboot.&lt;/li>
&lt;li>Rinse and repeat.&lt;/li>
&lt;/ol>
&lt;p>Needless to say, this was a painful process. Using kernel modules lowered the pain a tiny little when the changes I made didn&amp;rsquo;t crash but, in general, they &lt;em>did&lt;/em> crash. Documentation was scarce and I didn&amp;rsquo;t know what I was doing!&lt;/p>
&lt;p>In an attempt to make the validation step less painful and to ensure that each change I made was a &amp;ldquo;net positive&amp;rdquo; instead of regressing functionality, I got what-I-think-was my first real exposure to automated testing. I &lt;em>had&lt;/em> to automate steps 4 and 5 in the flow above, and so I wrote ad-hoc scripts to do so.&lt;/p>
&lt;p>My GSoC mentors, Luke Mewburn and Bill Studenmund, pointed me at the &lt;code>src/tests&lt;/code> directory of the NetBSD source tree which contained a bunch of tests for the system. Some of these were for user space tools and others exercised portions of the kernel. The existing machinery could probably be reused to automate my manual regression testing steps, so I had to try to integrate with it.&lt;/p>
&lt;p>&lt;em>Machinery&lt;/em> is&amp;hellip; a strong word given how rudimentary &lt;code>src/tests&lt;/code> was. The main issue with this infrastructure was that the tests didn&amp;rsquo;t have a real runner: they were all small programs with unique command-line interfaces glued together by a collection of &lt;code>Makefile&lt;/code>s. There was no guarantee that these tests built nor that running a &lt;code>make test&lt;/code> would run them all because many tests were known to be broken. And what&amp;rsquo;s worse: some of the existing tests &lt;em>crashed&lt;/em> the kernel due to known bugs.&lt;/p>
&lt;p>I started envisioning the need for a principled test framework and wanted to create one, but I needed the time to do so and school didn&amp;rsquo;t give me much of that. So, in 2006, I applied once again to GSoC but, while I got in, I chose to take a detour and help write the beginnings of the Boost.Process library. The reason for this choice was primarily to learn Win32 internals but a good side-effect was that I got to experience a more advanced test framework.&lt;/p>
&lt;p>And then, in 2007, I wanted to create this automation project as my bachelor&amp;rsquo;s degree graduation project&amp;hellip; but I couldn&amp;rsquo;t find a professor that would back the idea. (A pity because I believe it would have been a great fit for the operating systems group.) So I did what I knew how to do: I applied and got into GSoC once again&amp;mdash;this time &lt;a href="http://web.archive.org/web/20071225031924/http://netbsd-soc.sourceforge.net/projects/atf/">to develop a testing framework for NetBSD&lt;/a>.&lt;/p>
&lt;p>The idea was to create libraries to write tests with a unified CLI interface in either C, C++, or shell, and then provide a unified runner that would execute these tests in an isolated manner &lt;em>and&lt;/em> without access to the source tree. And thus the &lt;em>Automated Testing Framework (ATF)&lt;/em> was born.&lt;/p>
&lt;h1 id="problems-with-atf">Problems with ATF&lt;/h1>
&lt;p>ATF was fine for a while. We got a large portion of the old tests in NetBSD resurrected, fixed, and hooked into the ATF infrastructure. A major contribution was Andreas Gustafsson&amp;rsquo;s &lt;a href="https://www.gson.org/netbsd/anita/">Anita&lt;/a>: an automation system to create a VM, install NetBSD on it via the interactive installer, and run the whole battery of tests inside of it.&lt;/p>
&lt;p>Anita and ATF were the beginnings of something akin to a CI system for a full operating system in the open source world. As far as I know, Linux did &lt;em>not&lt;/em> have anything similar&amp;mdash;and it &lt;em>couldn&amp;rsquo;t&lt;/em> have had anything similar due to the distributed nature of its development&amp;mdash;nor did the other BSDs. Windows &lt;em>did&lt;/em> have this, of course, as I could glance from the many articles in &lt;a href="https://devblogs.microsoft.com/oldnewthing/">The Old New Thing&lt;/a> describing backwards compatibility testing.&lt;/p>
&lt;p>This was great but, over time, ATF showed signs of suboptimal design. It had at least two problems that were really hard to fix, namely:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Its design (remember running &lt;code>atf-run | atf-report&lt;/code>?) prevented the execution of tests in parallel, and concurrent test execution is crucial for performance. The Anita test runs took just too long for a good CI turnaround.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ATF expected tests to be written using the &lt;code>atf-c&lt;/code>, &lt;code>atf-c++&lt;/code>, and &lt;code>atf-sh&lt;/code> libraries&amp;mdash;but developers wanted to implement and reuse tests without porting them to these libraries. In particular, people wanted to create simpler test programs and wanted to support programs that emitted the more-widespread &lt;a href="https://testanything.org/">Test Anything Protocol (TAP)&lt;/a> popularized by Perl test harnesses.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Aside from these problems, ATF also lacked a critical feature that Anita had to supply on its own: a data store to track historical test reports and an interface to browse through them. This feature was a necessity for a CI system so, ideally, it had to live within ATF to not be stuck inside a web server. (&amp;ldquo;Web apps everywhere&amp;rdquo; was not a thing pre-2010.) To compound the problem, I got an internship at Google in 2008 and joined full-time in 2009, which let me experience &lt;a href="https://bazel.build/">Blaze&lt;/a> and its ecosystem for test results tracking and reporting first-hand&amp;mdash;and I wanted those features for NetBSD.&lt;/p>
&lt;p>I concluded, &lt;a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/">maybe wrongly&lt;/a>, that ATF needed a rewrite.&lt;/p>
&lt;h1 id="kyua-arrives">Kyua arrives&lt;/h1>
&lt;p>So, in 2010, I set to create a brand new execution engine. At that point, the target was to rewrite just the portion of ATF that executed tests: that is, the &lt;code>atf-run | atf-report&lt;/code> pipeline. I did not want to recreate the ATF &lt;em>libraries&lt;/em>. The idea was to:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Create a generic test runner that could integrate multiple test protocols, with the ATF-based tests being the first use case. The unwritten goal was to let the ATF libraries dwindle at some point in the future in favor of better libraries (like &lt;a href="https://github.com/google/googletest">GoogleTest&lt;/a> for C++ tests or my own &lt;a href="/2023/10/unit-testing-with-shtk.html">shtk for shell tests&lt;/a>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Support a &amp;ldquo;tests database&amp;rdquo; to store the results of all test executions. This database was going to allow generating reports in multiple formats and permit running queries to look at historical trends.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Support parallel test execution via a more modular runner design. (If you know the history behind Kyua, you&amp;rsquo;ll notice that this feature only appeared very late in the game. It was a mistake to delay it and I&amp;rsquo;m not sure why I did that.)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Thus, in November of 2010, Kyua was born as a &amp;ldquo;testing framework for infrastructure software&amp;rdquo;. It took some work to get to the point where Kyua could run most of the NetBSD test suite unmodified, but by July 2011, I reached this milestone and launched Kyua 0.1.&lt;/p>
&lt;p>Which led to frustration: ATF was deeply ingrained in NetBSD and Kyua wasn&amp;rsquo;t well-received for various reasons. One was that Kyua was a larger C++ project than ATF and C++ was (and still is) frowned upon in that community. Another was the fact that Kyua&amp;rsquo;s reports were harder to manage and required significantly more disk space, and Andreas wasn&amp;rsquo;t ready to bubble up his CI hardware costs. There were other concerns, all pretty reasonable.&lt;/p>
&lt;p>I kept working on Kyua to try to resolve most of the concerns raised and I believe I addressed all of those that could be addressed. Unfortunately, this took time, and I had grown demotivated to pursue another &amp;ldquo;sell&amp;rdquo; of the project. But then&amp;hellip;&lt;/p>
&lt;h1 id="kyua-meets-freebsd">Kyua meets FreeBSD&lt;/h1>
&lt;p>The FreeBSD community had been paying attention. They needed to improve their CI story as well, and they did have much more interest in building a test suite due to the needs of a few corporate-backed projects. I&amp;rsquo;m not exactly sure who I talked to other than Enji Cooper, George N. Neville, and Ed Maste, but by 2013, I had &lt;a href="/2013/11/joining-freebsd-committer-ranks.html">gotten a FreeBSD commit bit&lt;/a> and I was set to integrate Kyua&amp;mdash;not ATF!&amp;mdash;into FreeBSD.&lt;/p>
&lt;p>One thing I remember was how the FreeBSD project offered me three dedicated beast machines in a colo so that I could set them up for CI runs. This felt very different from the NetBSD approach: FreeBSD had resources&amp;mdash;understandably so, as NetBSD has always been underrated&amp;mdash;and they were ready to invest them into the shiny-new thing. I took the bait and worked on that for a while. (Fun fact: I used a PowerMac G5 for this work.)&lt;/p>
&lt;p>By December of 2013, I shipped &lt;a href="/2013/12/introducing-freebsd-test-suite.html">the initial CI system&lt;/a> for FreeBSD. This was a really exciting moment because all efforts I had put behind Kyua finally paid off: Kyua was &lt;em>the&lt;/em> driver for what I think is the most important operating system project in the open source community outside the Linux monoculture.&lt;/p>
&lt;p>The future of the project was tainted though: I was still employed by Google&amp;hellip; which made me keep experiencing Blaze and its ecosystem day after day. You see: Blaze is often touted as a &lt;em>build&lt;/em> system, but its secret sauce lies in it being a &lt;em>test&lt;/em> system. Blaze is a test runner that knows how to precisely build and run &lt;em>the minimum set of tests impacted by a code change&lt;/em>. And this matters.&lt;/p>
&lt;p>Precise test selection doesn&amp;rsquo;t make a big difference for the run-off-the-mill projects you see in GitHub, but for monorepo-style projects&amp;mdash;and the BSDs are such projects&amp;mdash;this becomes a requirement to run a CI system with quick turnaround times. By this point, I saw Kyua as a &amp;ldquo;local maximum&amp;rdquo; because I did not see how to make this possible in combination with the native BSDs&amp;rsquo; build systems.&lt;/p>
&lt;h1 id="stagnation">Stagnation&lt;/h1>
&lt;p>No matter what I secretly thought, FreeBSD was bought into ATF and Kyua, and they&amp;mdash;Enji in particular&amp;mdash;had larger plans for the system. The FreeBSD developers really wanted to integrate TAP-style tests, needed to set up a full CI system with many more configurations and architectures than I had set up, and needed better reporting capabilities for failures.&lt;/p>
&lt;p>The problem was that I owned the critical software pieces of this infrastructure, and this was a problem for various reasons: I was too busy with work and two little kids; I had stopped using any BSD on a daily basis; I did not see a bright future for Kyua as it was designed nor implemented; and to make things worse, contributing to Kyua required signing the Google CLA because I still worked for Google (which never made any sense to me but I didn&amp;rsquo;t have a say in the matter).&lt;/p>
&lt;p>So, for years, Kyua went pretty much unmaintained. FreeBSD kept building local patches and occasionally sent PRs to my upstream project, but I dreaded dealing with them. Maintaining open source projects is stressful if only because of this guilt that you end up with when you don&amp;rsquo;t have the time to deal with contributions in a timely manner. The more time the situation lasts, the worse it gets.&lt;/p>
&lt;p>It took until 2020 to convince myself that ATF and Kyua needed attention and that I was not fit to support them. I had to let go off these projects and I needed to clear my mind &lt;a href="https://www.endbasic.dev/2020/04/hello-endbasic.html">for other cool stuff&lt;/a>. So&amp;hellip; I emailed the FreeBSD project owners and offered them to take ownership. If they forked the project and owned it, any FreeBSD committer could easily gain access to the project to do what was best for FreeBSD. And if they owned the fork, they wouldn&amp;rsquo;t have to worry about the CLA part of the situation because I wouldn&amp;rsquo;t be involved any longer.&lt;/p>
&lt;p>Things didn&amp;rsquo;t make the progress I desired though. The FreeBSD project owners had concerns about officially forking these projects into FreeBSD because it cloud give the wrong impression and alienate other consumers like NetBSD. Kyua and ATF were not FreeBSD-only projects so it was probably better for them to stand alone. The discussions died off and stayed like that&amp;hellip;&lt;/p>
&lt;h1 id="rebirth">Rebirth&lt;/h1>
&lt;p>&amp;hellip; until the end of 2023 when Li-Wen Hsu resurrected the email thread I had started. Kyua had become a critical piece of the FreeBSD quality infrastructure and it being almost-abandoned was starting to be problematic. He asked if we could move forward with the project ownership transfer idea&amp;mdash;and I was at the ready.&lt;/p>
&lt;p>And so that&amp;rsquo;s what happened!&lt;/p>
&lt;p>On January 17th, 2024, the right buttons were pushed and&amp;hellip; &lt;a href="https://github.com/freebsd/kyua/">Kyua&lt;/a> (along with &lt;a href="https://github.com/freebsd/atf/">ATF&lt;/a> and &lt;a href="https://github.com/freebsd/lutok/">Lutok&lt;/a>) all became owned by the &lt;a href="https://github.com/freebsd/">FreeBSD GitHub organization&lt;/a>. Since then, I&amp;rsquo;ve seen a flurry of PRs reviewed and merged into the project, which makes me happy, really, because the people that care now have the chance to do what&amp;rsquo;s best for the project.&lt;/p>
&lt;p>Plus, you know what? Even if Kyua has some &amp;ldquo;design flaws&amp;rdquo; because it was not designed to be exactly like Blaze, it does have some &lt;em>other&lt;/em> interesting ideas that make it a good-enough fit for FreeBSD. In particular, one design principle behind ATF and Kyua was to allow running a test suite &lt;em>without&lt;/em> having access to the source code.&lt;/p>
&lt;p>Actually, I think this was a killer idea in the original design of ATF. The motivation for &amp;ldquo;running tests without the source&amp;rdquo; was to allow installing a system from scratch and then let &lt;em>the end user&lt;/em> run the tests to verify that the system worked correctly &lt;em>on their specific software/hardware combination&lt;/em>. The BSDs have limited resources and cannot do the sorts of testing that e.g. Microsoft does with Windows so, by pushing the tests &amp;ldquo;to the edge&amp;rdquo;, users could fill that gap.&lt;/p>
&lt;p>And to illustrate this novel-at-the-time idea, let&amp;rsquo;s take a tour.&lt;/p>
&lt;h1 id="kyua-in-freebsd-14">Kyua in FreeBSD 14&lt;/h1>
&lt;p>Kyua is part of the FreeBSD base system and, as such, it is installed under &lt;code>/usr/bin/kyua&lt;/code> and is available out of the box.&lt;/p>
&lt;p>Similarly, the test suite is available under &lt;code>/usr/tests&lt;/code> if you choose to unpack &lt;code>tests.txz&lt;/code> during the system installation. Peeking under it, we find:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">/usr/tests$ ls -F
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Kyuafile conftest.py lib/ sys/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">README etc/ libexec/ usr.bin/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">__init__.py examples/ local@ usr.sbin/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">atf_python/ games/ sbin/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/ gnu/ secure/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">cddl/ include/ share/
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/tests$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can use &lt;code>kyua list&lt;/code> to see what kind of test coverage we have. This command prints one test case name per line in its default output format, so we can easily count how many tests there are:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">/usr/tests$ kyua list | wc -l
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 8246
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/tests$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>8000+ tests. Not bad! Let&amp;rsquo;s peek into a tiny sample by focusing on the tests for &lt;code>dd(1)&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">/usr/tests$ kyua list bin/dd
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd2_test:max_seek
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd2_test:seek_overflow
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd2_test:sigint_open
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd2_test:sigint_read
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd_test:io
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd_test:length
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd_test:seek
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/tests$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And let&amp;rsquo;s run those!&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">/usr/tests$ time kyua -v parallelism=$(nproc) test bin/dd
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd2_test:seek_overflow -&amp;gt; passed [0.042s]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd2_test:max_seek -&amp;gt; skipped: tmpfs can&amp;#39;t create arbitrarily large sparse files [0.044s]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd_test:length -&amp;gt; skipped: fdescfs is not mounted on /dev/fd [0.024s]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd_test:seek -&amp;gt; passed [0.048s]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd_test:io -&amp;gt; passed [0.071s]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd2_test:sigint_read -&amp;gt; passed [3.077s]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bin/dd/dd2_test:sigint_open -&amp;gt; passed [3.079s]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Results file id is usr_tests.20240728-161942-793100
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Results saved to /home/jmmv/.kyua/store/results.usr_tests.20240728-161942-793100.db
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">7/7 passed (0 failed)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kyua -v parallelism=$(nproc) test bin/dd 0.17s user 0.33s system 15% cpu 3.198 total
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/usr/tests$ █
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Easy peasy. We can run tests for any part of the system right from the &lt;code>/usr/tests&lt;/code> hierarchy without having had to build anything nor needing access to the source tree. As mentioned earlier, you can use this feature to validate that FreeBSD works on your specific machine given that device drivers are of varying quality (something that&amp;rsquo;s true on &lt;em>any&lt;/em> operating system) and may result in different behavior. And you can use this feature to validate that the system &lt;em>continues to work&lt;/em> as you perform upgrades over time.&lt;/p>
&lt;p>Outside of your machine, though, we can also look at the CI system based on Jenkins. For that, head to &lt;a href="https://ci.freebsd.org/">https://ci.freebsd.org/&lt;/a> and peek around. Notice the tens of different jobs configured to build and test FreeBSD under various architectures, releases, and build settings.&lt;/p>
&lt;p>As an example, look at the recent &lt;a href="https://ci.freebsd.org/job/FreeBSD-main-amd64-test/25361/testReport/">run 25361 of &lt;code>FreeBSD-main-amd64-test&lt;/code>&lt;/a>, and notice how we can inspect the same test results we saw on the command line. Do so by clicking on &lt;a href="https://ci.freebsd.org/job/FreeBSD-main-amd64-test/25361/testReport/bin.dd/">&lt;code>bin.dd&lt;/code>&lt;/a> and finding the results for the same tests we ran earlier. This showcases Kyua&amp;rsquo;s ability to generate a &lt;code>junit.xml&lt;/code> test report&amp;mdash;an output format that was precisely written to integrate with Jenkins for the FreeBSD CI system.&lt;/p>
&lt;hr>
&lt;p>And that&amp;rsquo;s it.&lt;/p>
&lt;p>The morale of the story is two-fold. First, free software can remain alive and kicking even if the original authors lose interest. And second&amp;hellip; do you see how powerful it is to have a CI system for &lt;em>a whole OS&lt;/em>? This is something Linux can only dream of because of its inherent distributed development nature. Yes, there is the &lt;a href="https://github.com/linux-test-project/ltp">Linux Test Project&lt;/a>, but because Linux is &amp;ldquo;just a kernel&amp;rdquo;, this is what this project can test. There is nothing that can validate that the kernel works in combination with all other pieces that form a Linux distribution in a uniform and cohesive manner.&lt;/p>
&lt;p>In any case, congrats grads. Kyua has a bright future ahead.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-08-02-kyua-graduates.jpg" length="367311" type="image/jpeg"/></item><item><title>Rust doesn't solve the CrowdStrike outage</title><link>https://jmmv.dev/2024/07/crowdstrike-and-rust.html</link><pubDate>Tue, 23 Jul 2024 07:10:00 -0700</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/07/crowdstrike-and-rust.html</guid><description>&lt;p>Look, I like Rust. I really, really do, and I agree with the premise that memory-unsafe languages like C++ should not be used anymore. But claiming that Rust would have &lt;em>prevented&lt;/em> the massive outage that the world went through last Friday is misleading and actively harmful to Rust&amp;rsquo;s evangelism.&lt;/p>
&lt;p>Having CrowdStrike written in Rust would have &lt;em>minimized&lt;/em> the chances of the outage happening, but not resolved the root cause that allowed the outage to happen in the first place. Thus, it irks me to see various folks blanket-claiming that Rust is the answer. It&amp;rsquo;s not, and pushing this agenda hurts Rust&amp;rsquo;s adoption more than it helps: C++ experts can understand the root cause and see that this claim is misleading, causing further divide in the systems programming world.&lt;/p>
&lt;p>So, why won&amp;rsquo;t Rust help? Let me try to answer that question, but while we are at it, let&amp;rsquo;s also delve deeper into the causes of the outage. In a way, let me put my SRE hat on and write my own version of the postmortem.&lt;/p>
&lt;h1 id="the-outage">The outage&lt;/h1>
&lt;p>Here is what &lt;a href="https://www.crowdstrike.com/blog/falcon-update-for-windows-hosts-technical-details/">CrowdStrike&amp;rsquo;s official &amp;ldquo;postmortem&amp;rdquo;&lt;/a> has to say about the problem that the industry faced:&lt;/p>
&lt;blockquote>
&lt;p>On July 19, 2024 at 04:09 UTC, as part of ongoing operations, CrowdStrike released a sensor configuration update to Windows systems. Sensor configuration updates are an ongoing part of the protection mechanisms of the Falcon platform. This configuration update triggered a logic error resulting in a system crash and blue screen (BSOD) on impacted systems.&lt;/p>
&lt;p>The sensor configuration update that caused the system crash was remediated on Friday, July 19, 2024 05:27 UTC.&lt;/p>
&lt;/blockquote>
&lt;p>Paraphrasing:&lt;/p>
&lt;ol>
&lt;li>CrowdStrike (the company) pushed a configuration change.&lt;/li>
&lt;li>The change tickled a latent bug in &amp;ldquo;the Falcon platform&amp;rdquo; (the product).&lt;/li>
&lt;li>This bug in Falcon resulted in a crash that brought down Windows.&lt;/li>
&lt;/ol>
&lt;p>The first two points are not too strange: configuration changes are &amp;ldquo;business as usual&amp;rdquo; for any online system and having changes tickle bugs in code is unfortunately common. In fact, the majority of the outages (citation needed) are caused by human-initiated configuration changes.&lt;/p>
&lt;p>Obviously, we should ask why the bug existed and how it could be remediated in order to increase the robustness of the product. But we must also question the third point: why was the bug able to bring down the whole machine? And, much more importantly, why did this bug bring down &lt;em>so many systems&lt;/em> across the world?&lt;/p>
&lt;h1 id="the-memory-bug">The memory bug&lt;/h1>
&lt;p>Let&amp;rsquo;s start with the first question: what was the nature of the bug in Falcon?&lt;/p>
&lt;p>Easy: there was a logic bug in the &amp;ldquo;Channel Files&amp;rdquo; (aka configuration files) parser that, given some invalid input, made the code try to access an invalid memory position. The details are really not interesting: this could be have been a null pointer dereference, a general protection fault, or whatever. The point is: the crash was triggered by an invalid memory access issue.&lt;/p>
&lt;p>And this is where some Rust enthusiasts will zero in and say &amp;ldquo;Ah-HAH! We got you, fools. If the code had been written in Rust, this bug would not have existed!&amp;rdquo; And, you know what, that&amp;rsquo;s literally true: this specific bug would not have happened.&lt;/p>
&lt;p>But so what? Avoiding this specific type of error would just have delayed this outage until another time when a different class of error that Rust doesn&amp;rsquo;t protect against happened. (This is the exact same argument I raised in a &lt;a href="/2018/07/forbidden-assertions-fallacy.html">critique of forbidding assertions in Go&lt;/a> back in 2018 by the way.) Focusing on the memory bug is missing the forest for the trees because of the nature of what Falcon is.&lt;/p>
&lt;p>OK so, what is Falcon?&lt;/p>
&lt;h1 id="the-kernel-crash">The kernel crash&lt;/h1>
&lt;p>Falcon is &amp;ldquo;malware&amp;hellip; but for the good guys&amp;rdquo;. Oops, I mean: Falcon is an endpoint security system. Falcon is a product typically installed on corporate machines so that the security team can detect and mitigate threats in real time (while monitoring the actions of their employees). There &lt;em>is&lt;/em> some value in this: most cyberattacks (citation needed again) start by compromising corporate machines, often via social engineering practices.&lt;/p>
&lt;p>This type of product must have control over the machine. It must be able to intercept all user file and network operations to scan their content. And it must be tamper-proof so that &amp;ldquo;savvy&amp;rdquo; corporate users don&amp;rsquo;t disable it when they read sketchy online instructions to fix their broken WiFi in an attempt to (&lt;em>shudder&lt;/em>) not have to create IT tickets.&lt;/p>
&lt;p>How can you &lt;em>implement&lt;/em> a product like Falcon? The easiest approach, and the approach that Windows encourages, is to write a kernel module. It easily follows that Falcon is a kernel module and, as such, it runs in kernel space. This means that any mess up in Falcon&amp;rsquo;s code can damage the running kernel and, in turn, bring the whole system down.&lt;/p>
&lt;p>And when I say &amp;ldquo;any mess up&amp;rdquo;, I really mean it. The kernel is not only brought down by memory errors, and you don&amp;rsquo;t have to &amp;ldquo;crash the kernel&amp;rdquo; to make a machine unusable. Think of a deadlock preventing the kernel from making forward progress. Think of a logic error in the &lt;code>open(2)&lt;/code> system call handler preventing user space from opening any file later on. Think of mistakenly mapping file system code as pageable when it should never be paged out. Think of writing an unbounded recursive algorithm that exhausts the kernel&amp;rsquo;s stack. Think of&amp;hellip; an innocent buggy &lt;code>unwrap()&lt;/code> call if the code actually used Rust.&lt;/p>
&lt;p>There are just too many ways to destroy the kernel&amp;rsquo;s stability, which is why claiming that Rust would have prevented this incident irks me. Rust&amp;rsquo;s memory-safety only addresses one type of crash. It is true, though, that the focus on correctness in the Rust ecosystem via strong types could minimize the chances of other types of logic bugs. But&amp;hellip; as much as we want to reach perfection, we must accept that bugs happen, and asserting that Rust is &lt;em>the only answer&lt;/em> to the problem is as negligent as sticking to C++.&lt;/p>
&lt;p>And, you know, there are many more C++ developers that work in kernel space than Rust developers know kernel internals (oops, another citation need). So, naturally, a large portion of C++ developers can smell the rubbish in this claim. Which is unfortunate because this increases animosity between the two communities, which goes against the goal of converting folks to safe languages. Rust folks &lt;em>know&lt;/em> that Rust can definitely help make the situation better, but C++ folks cannot get bought into it because the arguments they hear don&amp;rsquo;t resonate with them.&lt;/p>
&lt;h1 id="from-kernel-space-to-user-space">From kernel space to user space&lt;/h1>
&lt;p>There have &lt;em>also&lt;/em> been other claims saying that this would not have happened at all if Falcon was &lt;em>not&lt;/em> running in the kernel. OK, that&amp;rsquo;s a better take, but&amp;hellip; it is not crystal-clear that this alone would help either.&lt;/p>
&lt;p>As I mentioned earlier, Falcon needs to be as tamper-proof as possible to prevent malware from interfering with it and compromised users from trying to disable it. And if malware or humans were able to easily do that, then the product would be as useless as nothing.&lt;/p>
&lt;p>Now, the Windows kernel could definitely forbid kernel modules for anything similar to Falcon. Instead, the kernel could expose a bunch of APIs so that user space applications could hook into them to expose similar functionality. And you know what? Microsoft &lt;a href="https://news.microsoft.com/2009/12/16/microsoft-statement-on-european-commission-decision/">&lt;em>did&lt;/em> try to move Windows in that direction&lt;/a>, but the antivirus mob threatened to sue on antitrust grounds and the whole thing went nowhere. So we are stuck with a less secure system because antivirus companies need to be able to sell their obnoxious products.&lt;/p>
&lt;p>But let&amp;rsquo;s step aside from that dumpster fire for a moment. Even if Falcon ran in user space and communicated with the kernel via controlled APIs&amp;hellip; would that be sufficient to prevent a system malfunction? Note that these APIs would need to be tamper-proof too. Imagine if, say, you wanted this user space driver to validate every binary before it&amp;rsquo;s executed by the kernel. If you make the kernel &lt;em>require&lt;/em> an answer from the user space driver on every execution, and such driver is faulty, the system won&amp;rsquo;t be able to execute any program anymore. And if you make it &lt;em>optional&lt;/em> for the kernel to communicate with the driver so that the kernel can tolerate a crashing driver, then you open up a path for malware to try to crash the driver first and then infiltrate the system.&lt;/p>
&lt;p>Thus it is not obvious that &amp;ldquo;just moving to user space&amp;rdquo; is the answer here either. Oh, by the way, Apple has been moving macOS in the direction of disallowing kernel modules for years and providing alternative APIs to implement things like file systems and antivirus-type software without kernel privileges. They also have this kind of security daemon running in user space in the default system which validates process executions, and &lt;a href="/2017/10/fighting-execs-sandboxfs-macos.html">drove me crazy a while back&lt;/a>.&lt;/p>
&lt;h1 id="the-bug-is-in-the-deployment">The bug is in the deployment&lt;/h1>
&lt;p>If we must accept that bugs exist, that memory-related bugs are not the only ones that can kill a system, and that moving the driver to user space isn&amp;rsquo;t an obvious fix either&amp;hellip; are we doomed? Is there nothing we could do to prevent this from happening?&lt;/p>
&lt;p>The above are all things that could (and should!) be done to reduce the chances of a misbehavior happening, but we must accept that the code bug was just &lt;em>the specific trigger this time around&lt;/em> and a different trigger could have had similarly nefarious consequences. The root cause behind the outage lies in the &lt;em>process&lt;/em> to get the configuration change shipped to the world.&lt;/p>
&lt;p>Now, SRE 101 (or DevOps or whatever you want to call it) says that configuration changes must be staged for slow and controlled deployment, and validated at every step. Those changes should first be validated in a very small scale before being pushed to the world later on, and every push should be incremental.&lt;/p>
&lt;p>I find it quite hard to believe that CrowdStrike has nothing to validate deployments given the criticality of Falcon and the massive impact that a bug can have (and has had). Maybe they don&amp;rsquo;t have any process whatsoever as the external evidence seems to suggest, which would be incredibly negligent, &lt;del>but let&amp;rsquo;s give them the benefit of the doubt&lt;/del>. (&lt;strong>Update July 24th:&lt;/strong> According to CrowdStrike&amp;rsquo;s &lt;a href="https://www.crowdstrike.com/falcon-content-update-remediation-and-guidance-hub/">updated postmortem&lt;/a>, they actually do not have any sort of testing nor canarying. So it is indeed incredibly negligent.)&lt;/p>
&lt;p>Part of why I say this is due to new information from Microsoft with an assessment of how many machines were impacted by the outage. Here is &lt;a href="https://blogs.microsoft.com/blog/2024/07/20/helping-our-customers-through-the-crowdstrike-outage/">what Microsoft has to say&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>While software updates may occasionally cause disturbances, significant incidents like the CrowdStrike event are infrequent. We currently estimate that CrowdStrike’s update affected 8.5 million Windows devices, or less than one percent of all Windows machines. While the percentage was small, the broad economic and societal impacts reflect the use of CrowdStrike by enterprises that run many critical services.&lt;/p>
&lt;/blockquote>
&lt;p>Read that: &amp;ldquo;less than 1% of the machines were impacted&amp;rdquo;. Does this mean that CrowdStrike &lt;em>does&lt;/em> have a staged rollout in which they push configuration changes to just a subset of 1% of the machines worldwide? That doesn&amp;rsquo;t seem crazy actually and is in line with how many services are deployed, but if that&amp;rsquo;s their &lt;em>first&lt;/em> step in the deployment process, it&amp;rsquo;s time to reevaluate their approach because this 1% of machines is way too many and has proven to be catastrophic. (This is leaving aside the question of whether 1% refers to the total number of Windows machines in the world or 1% of Windows machines with CrowdStrike on them.)&lt;/p>
&lt;p>The question that remains answering, then, is what sort of testing happened between &amp;ldquo;the configuration change is done&amp;rdquo; to &amp;ldquo;let&amp;rsquo;s roll it out to 1% of the world&amp;rdquo;. &lt;em>Did any testing happen at all?&lt;/em> That&amp;rsquo;s where it gets concerning and where we may not get any official information on the matter from CrowdStrike. (&lt;strong>Update July 24th:&lt;/strong> We did get &lt;a href="https://www.crowdstrike.com/falcon-content-update-remediation-and-guidance-hub/">an update&lt;/a> and it confirms that they actually do not have any sort of rollout progressive deployment.)&lt;/p>
&lt;p>So, yes, CrowdStrike&amp;rsquo;s deployment practices are definitely to blame for the incident. This outage &lt;em>was a process problem&lt;/em>, not &lt;em>a code/technology problem&lt;/em>.&lt;/p>
&lt;h1 id="downstream-companies">Downstream companies&lt;/h1>
&lt;p>Before concluding, let&amp;rsquo;s shift away from CrowdStrike and look at the downstream companies impacted by this bad configuration push. Did you notice how many companies were absolving themselves from problems by pointing out that &amp;ldquo;due to a third-party IT incident&amp;rdquo; their service was down? OK, yes, that is literally true once again, but pointing fingers is not helpful.&lt;/p>
&lt;p>The problem in this situation is the monoculture around CrowdStrike. Certain security certifications require &amp;ldquo;endpoint protection&amp;rdquo; as a line item and it seems perfectly plausible that most IT departments just deploy Falcon due to aggressive marketing from CrowdStrike&amp;rsquo;s part and call it a day without putting any more thought into it. It&amp;rsquo;s just not interesting to them to spend any extra time on the issue.&lt;/p>
&lt;p>But this raises a problem: dependencies are always a liability, and when you choose to take a dependency on another vendor or another piece of code, you must own your choice. You must model how your own system will fail when the dependency fails, and then, if the risk warrants it, engineer a solution around the potential problem.&lt;/p>
&lt;p>I do not know how Falcon works, so I cannot tell if CrowdStrike offered sufficient clarity on how upgrades work to customers or even if their product offers a way to control how the deployment of configuration changes happens within an organization. These are things that will have to be built so that the few customers that care can increase the reliability of their systems.&lt;/p>
&lt;h1 id="is-there-an-attack-vector">Is there an attack vector?&lt;/h1>
&lt;p>And finally, let&amp;rsquo;s talk about the last part of CrowdStrike&amp;rsquo;s official statement:&lt;/p>
&lt;blockquote>
&lt;p>This issue is not the result of or related to a cyberattack.&lt;/p>
&lt;/blockquote>
&lt;p>Is that true? We may never know. It is unlikely that this &lt;em>is&lt;/em> a cyberattack because crashing systems left and right doesn&amp;rsquo;t seem to serve a clear purpose. I guess you could claim that an attacker may have wanted to hide something &lt;em>else&lt;/em> while the world was scrambling, and that might be possible, but&amp;hellip; well, you can imagine all sorts of conspiracy theories.&lt;/p>
&lt;p>What &lt;em>is&lt;/em> interesting is this part of CrowdStrike&amp;rsquo;s official postmortem:&lt;/p>
&lt;blockquote>
&lt;p>Although Channel Files end with the SYS extension, &lt;em>they are not kernel drivers&lt;/em>.&lt;/p>
&lt;/blockquote>
&lt;p>Emphasis theirs. Funny, huh? They seem to be wanting to hide the fact that Falcon doesn&amp;rsquo;t run in the kernel. But it does. Knowing that these files are not drivers is not comforting: the kernel module is reacting to changes to these files and thus these files influence the behavior of the kernel. As a result, it seems plausible that malformed Channel Files could tamper with the kernel in more subtle ways than just a blatant crash.&lt;/p>
&lt;p>And &lt;em>this&lt;/em> situation, my friend, is precisely where Rust would definitely help. Rust&amp;rsquo;s memory safety would minimize the chances that a malformed configuration file could exploit bugs like buffer overflows to escalate privileges within the kernel, resulting in much more subtle, but dangerous, attacks.&lt;/p>
&lt;p>But that&amp;rsquo;s not what happened this time around.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-07-23-rusty-crow.jpg" length="343660" type="image/jpeg"/></item><item><title>20 years of blogging</title><link>https://jmmv.dev/2024/06/20-years-of-blogging.html</link><pubDate>Sat, 22 Jun 2024 09:00:00 -0700</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/06/20-years-of-blogging.html</guid><description>&lt;p>&lt;a href="https://blogsystem5.substack.com/">Blog System/5&lt;/a> hasn&amp;rsquo;t always been called this way and it hasn&amp;rsquo;t been my first experience with blogging either. In fact, today marks the 20th anniversary of this publication in its various incarnations so it&amp;rsquo;s time for a bit of reflection.&lt;/p>
&lt;p>Just to set context for when 20 years ago was: Windows XP was almost 3 years old, Ubuntu had just debuted, Apple computers were still PowerPC-based, Half Life 2 was about to launch, and Slashdot was the place to be instead of the yet-to-be-created Hacker News. As for myself, I was still in college, had copious amounts of free time, and was a really active contributor to NetBSD.&lt;/p>
&lt;p>So let&amp;rsquo;s start with a trip down memory lane before getting into the retrospective and the analysis of how &lt;a href="/2023/10/hello-blog-system5.html">my Substack experiment&lt;/a>, which started late last year, has turned out. Beware that because this is an introspection post, all of the generalizations that follow are backed by feelings, not hard data, so take them with a grain of salt.&lt;/p>
&lt;h1 id="history-episode-one">History: Episode one&lt;/h1>
&lt;p>Looking back at &lt;a href="/2004/06/welcome-to-my-new-journal.html">my very first post from June 22nd, 2004&lt;/a>&amp;mdash;oh wow, my English was &lt;em>bad&lt;/em>&amp;mdash;I had an earlier experiment of a blog a few months prior that didn&amp;rsquo;t last more than three entries. But the newer iteration, started during college finals of course, did stick. I originally chose the LiveJournal platform because it had more features than Blogger, and thus &amp;ldquo;jmmv&amp;rsquo;s weblog&amp;rdquo; was born. Why &amp;ldquo;weblog&amp;rdquo; and not &amp;ldquo;blog&amp;rdquo;? Because the &amp;ldquo;blog&amp;rdquo; term wasn&amp;rsquo;t yet the popular one.&lt;/p>
&lt;figure>
&lt;a href="https://web.archive.org/web/20041126220647/http://www.livejournal.com:80/users/jmmv/">&lt;img src="/images/2024-06-22-livejournal-20041126.png" class="with-border">&lt;/a>
&lt;figcaption>Earliest image available of the "jmmv's weblog" site on LiveJournal via the Wayback Machine. Scraped on November 26th, 2004.&lt;/figcaption>
&lt;/figure>
&lt;p>My choice of LiveJournal didn&amp;rsquo;t last very long though. I migrated to Blogger on &lt;a href="/2005/10/blog-migrated-to-blogger-welcome.html">October 22nd, 2005&lt;/a> because it had improved significantly by then and it was growing in popularity. As part of that transition, I renamed the blog to &amp;ldquo;The Julipedia&amp;rdquo;&amp;mdash;a nickname that a friend gave me because I usually had answers to all of his Linux-related questions. And yes, Wikipedia was a pretty new thing back then too, having just launched in 2001 and still being mocked as &amp;ldquo;the thing that will never compete with real encyclopaedias&amp;rdquo;. Ha, ha.&lt;/p>
&lt;figure>
&lt;a href="https://web.archive.org/web/20060412020801/http://julipedia.blogspot.com/">&lt;img src="/images/2024-06-22-blogger-20060412.png" class="with-border">&lt;/a>
&lt;figcaption>Earliest image available of "The Julipedia" site on Blogger via the Wayback Machine. Scraped on April 12th, 2006.&lt;/figcaption>
&lt;/figure>
&lt;p>Blogger was just fine for a long time&amp;hellip; until Google+ messed things up in 2011 or so. During that era, Google was shoehorning their social platform everywhere, and part of that process was optionally injecting the commenting system of Google+ into Blogger. I took the bait and adopted the new commenting experience&amp;mdash;which, in other words, meant that I killed engagement overnight.&lt;/p>
&lt;p>In any case, I continued to hold onto Blogger until about 2015. At that point, Medium was the new hot thing and, having grown disillusioned with Blogger&amp;rsquo;s staleness&amp;mdash;the platform didn&amp;rsquo;t receive any love because all development and product resources had gone into the dwindling Google+&amp;mdash;&lt;a href="/2015/05/hello-medium.html">I gave it a try&lt;/a>.&lt;/p>
&lt;figure>
&lt;a href="https://web.archive.org/web/20160701041554/https://medium.com/@jmmv">&lt;img src="/images/2024-06-22-medium-20160701.png" class="with-border">&lt;/a>
&lt;figcaption>Earliest image available of my Medium publication via the Wayback Machine. Scraped on July 1st, 2016.&lt;/figcaption>
&lt;/figure>
&lt;p>Medium was great: its UI was &lt;em>neat&lt;/em> and their value proposition was that their network would help me increase the reach of my articles while offering a delightful publishing experience. But things went downhill pretty quickly too: the desire to monetize Medium made it gain aggressive subscription popups and paywalls everywhere, and it started filling up with &amp;ldquo;Top N things to XYZ&amp;rdquo; spam. AI-generated garbage? Not yet, but close. Yikes SEO.&lt;/p>
&lt;h1 id="history-episode-two">History: Episode two&lt;/h1>
&lt;p>My Medium experiment &lt;a href="/2016/01/medium-experiment-wrapup.html">didn&amp;rsquo;t last very long&lt;/a>. I published 8 posts over the course of 5 months but I knew I had to move elsewhere. Part of this move involved exporting those 8 posts and reimporting them into Blogger, which was easier said than done. Medium offered an &amp;ldquo;export&amp;rdquo; feature indeed, which sold me in the first place to try it out, but the &lt;em>content&lt;/em> of the export was &lt;em>unusable&lt;/em>. It took a lot of effort to clean up the tainted HTML that the platform produced to reimport it anywhere else. And this is when it hit me: I was not in control of my content, which was validated by seeing that Blogger&amp;rsquo;s export suffered from the same issues.&lt;/p>
&lt;p>Which drove me to static hosting. &lt;a href="/2016/05/homepage-v3.html">I extended the trivial homepage I already had&lt;/a> on the web with a blog section, built with Jekyll first and &lt;a href="/2018/02/from-jekyll-to-hugo.html">then with Hugo&lt;/a>, and managed my blog in there for about 8 years. I dabbled with going back to a CMS a few times because the authoring experience in Markdown+Git+Hugo isn&amp;rsquo;t particularly&amp;hellip; amenable to posting. But every time I prototyped an alternative, I ended up hitting the same roadblocks around content ownership. I wanted to own the originals in a future-proof format.&lt;/p>
&lt;figure>
&lt;a href="https://web.archive.org/web/20161224052720/http://julio.meroh.net/">&lt;img src="/images/2024-06-22-static-20161224.png" class="with-border">&lt;/a>
&lt;figcaption>Earliest image available of my static blog via the Wayback Machine. Scraped on December 24th, 2016.&lt;/figcaption>
&lt;/figure>
&lt;p>But wait&amp;hellip; you are reading this in Substack, aren&amp;rsquo;t you? &amp;ldquo;WTH!&amp;rdquo; you must be thinking. How can I reconcile what I just said with publishing on this platform? Well&amp;hellip; over the 8 years of static hosting, it was almost impossible for me to grow my blog&amp;rsquo;s readership. I tried various things to grow a subscribers base, but nothing worked. And Substack seemed to be the new cool kid on the block where various folks I follow post content, so I had to try it out. This experiment is &lt;a href="/2023/10/hello-blog-system5.html">what brought you Blog System/5 in the first place&lt;/a>. And you know what? It has been pretty cool.&lt;/p>
&lt;p>The key difference between the Substack experiment and the Medium experiment is that I&amp;rsquo;m authoring the content in Markdown first, using the exact same platform I had previously set up for my static blog. Only when the posts are ready to publish, I literally copy/paste them into Substack and then set up a redirect in my static blog to point to Substack. It&amp;rsquo;s painful indeed, but I feel at peace because I can pull the rug at any time and be back in my happy static blog.&lt;/p>
&lt;h1 id="evolution-writing-approach">Evolution: Writing approach&lt;/h1>
&lt;p>Anyhow, enough talk about the history of the blog. Let&amp;rsquo;s talk about how its content has changed form over the years and &lt;em>why&lt;/em> it has had to change. And to talk about content changes, we must talk about the evolution of audience behaviors: if you care about being read at all, you must consider who you are writing for.&lt;/p>
&lt;p>With that in mind, the obvious question is: do people care about blogs these days? I&amp;rsquo;d say no, not really. Proof of this is when you hear people say &amp;ldquo;so and so wrote a blog&amp;rdquo; (cringe) when they actually mean &amp;ldquo;a blog &lt;em>post&lt;/em>&amp;rdquo;. The concept of a blog as a personal publication seems to have been lost in favor of individual posts. I&amp;rsquo;m sure you can think of some recent popular articles that came your way, but, question: can you remember &lt;em>where&lt;/em> you read them or &lt;em>who&lt;/em> the author was? I bet not.&lt;/p>
&lt;p>As a consequence, the type of content for a long-running blog like this one has had to evolve. Take a moment to skim &lt;a href="/archive.html">through the archives&lt;/a> and look for patterns. You might notice that, back when I started blogging, my usual article was about 350 words long and not very polished. This was pretty common in the blogosphere: most blogs had similar short articles, often just describing the author&amp;rsquo;s &amp;ldquo;daily doings&amp;rdquo;.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-06-22-blog-words-per-post.png" class="with-border">
&lt;figcaption>Median words per post by year for all articles that I've written in this blog and other publications.&lt;/figcaption>
&lt;/figure>
&lt;p>Fast-forward to today and you can clearly see a change in the shape of the articles you read here and elsewhere: they are &lt;em>much&lt;/em> longer now, even if attention spans are infinitesimally small. One reason is that we used to use blogs like a social network, to publish our &amp;ldquo;daily doings&amp;rdquo; for our committed followers to see. Contrast that to today: there is almost zero chance anyone will come across a bunch of short articles frequently enough for them to become an avid reader. The other reason, closely related to this one, is that it&amp;rsquo;s easier for long-form posts to &amp;ldquo;go viral&amp;rdquo;&amp;mdash;and while you might not care about reach, some of us do.&lt;/p>
&lt;p>Writing longer-form posts has had a direct inverse consequence on the frequency of the posts though, for obvious reasons:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-06-22-blog-posts-per-year.png" class="with-border">
&lt;figcaption>Total number of posts per year in this blog, but also including occasional articles in other publications.&lt;/figcaption>
&lt;/figure>
&lt;p>Having to dedicate many, &lt;em>many&lt;/em>, &lt;strong>&lt;em>many&lt;/em>&lt;/strong> hours to every long-form post necessarily means that the frequency of the articles has to decrease. For this blog, you can clearly see that I started strong with a post about every two or three days and now I consider myself lucky if I am able to average two posts per month per year. I also have much less free time available, so whenever I write an article, I want it to be of higher quality than before.&lt;/p>
&lt;p>Fewer articles is not a bad metric per se though. Investing time to write long-form prose is beneficial. Long articles require much more work to put together indeed, but I find that spending such time helps me understand each topic in a better way. Furthermore, one only gets better at writing by, surprise, writing: if you skim through the oldest articles, you&amp;rsquo;ll notice that they were, ehem, mediocre&amp;mdash;yet I made the conscious decision to blog and to do so in English. As a result, I think that my contemporary copy is pretty decent, and this has helped me immensely in my professional environment. I couldn&amp;rsquo;t have gotten here without going through this long journey.&lt;/p>
&lt;p>The other big difference in how the content has evolved is the presence of pictures:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-06-22-images-per-year.png" class="with-border">
&lt;figcaption>Evolution of the number of posts that include images over time.&lt;/figcaption>
&lt;/figure>
&lt;p>In the past, I rarely included pictures in any article: these were not necessary because the articles were shorter and because resharing articles in social media didn&amp;rsquo;t require pictures to fight for the small chance of being seen. These days, however, you must have pictures in order to make articles stand out when they are shared. And just due to the nature of the longer text, it&amp;rsquo;s useful to include pictures, diagrams or code snippets to break the wall of text and make the copy easier to digest. Shout out to &lt;a href="https://draw.io/">draw.io&lt;/a>, which has been awesome to create diagrams in some recent posts.&lt;/p>
&lt;h1 id="evolution-consumption-and-audience">Evolution: Consumption and audience&lt;/h1>
&lt;p>Shifting gears a little bit, blogs were the true decentralized platform. Any individual could easily start a blog in the service of their choice, possibly self-hosting it if they didn&amp;rsquo;t want to use a commercial service, and anyone could trivially subscribe to those blogs via one of many RSS aggregators. And, yes, I know, this is true today as well: you can &lt;em>still&lt;/em> create a blog and you can &lt;em>still&lt;/em> subscribe to (most) blogs via RSS; however, the way blogs are &amp;ldquo;consumed&amp;rdquo; has changed.&lt;/p>
&lt;p>One thing that was useful in the past were the so-called &amp;ldquo;Planets&amp;rdquo;: websites that acted as curated aggregators of blogs by topic. Some of these still exist, such as &lt;a href="https://planet.kde.org">Planet KDE&lt;/a> or &lt;a href="http://fedoraplanet.org/">Planet Fedora&lt;/a>, but they don&amp;rsquo;t &lt;em>feel&lt;/em> like they used to. The longer posts and the (ab)use of images has made Planets unwieldy: their front pages are extremely long and contain lots of irregularly-formatted pictures, offering little content cohesion. Years ago, because articles were shorter, more fit in one page, and because Planets were popular, authors often replied to each other&amp;rsquo;s &lt;del>controversial&lt;/del> interesting thoughts by posting on &lt;em>their&lt;/em> platform and seeing their articles bubble up through the Planets. Much like a social media timeline these days but without a central authority.&lt;/p>
&lt;p>So, without RSS aggregators or Planets, how do people find blogs now and &lt;em>stay engaged&lt;/em>? Well&amp;hellip; they don&amp;rsquo;t for the most part? Organic traffic is pretty much useless at building any kind of following: people come in, read the very few words they were looking for, and vanish right away. Gathering traffic from self-promotion in social media is almost a futile experiment. So the best bet is to have articles turn into &amp;ldquo;one-off hits&amp;rdquo; on sites like Hacker News. But even then, the majority of the traffic from those sources ends up being noise as well because it doesn&amp;rsquo;t lead to conversions into frequent readers. Getting content seen by a reasonable number of people is a fight with every single post.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-06-22-blog-views.png" class="with-border">
&lt;figcaption>Traffic to my blog during the second half of 2022 as captured by &lt;a href="https://endtracker.azurewebsites.net/">my own analytics platform&lt;/a>. I selected this period to avoid conflating the data with the creation of Blog System/5 and to avoid a humongous traffic spike driven by the &lt;a href="/2023/06/fast-machines-slow-machines.html">Fast machines, slow machines&lt;/a> post that made the other lines impossible to read. Note how there is no growth: the peaks are driven purely by Hacker News and the like.&lt;/figcaption>
&lt;/figure>
&lt;p>What am I basing this on? Well, for starters, my blog has had email subscriptions for more than 10 years, first powered by FeedBurner and then by &lt;a href="/2023/06/in-house-email-subscriptions.html">my own mechanism&lt;/a>. In that period of time, I only saw the subscribers count rise to about 170, which feels pretty pathetic to be honest. I resisted the urge to add &amp;ldquo;on-your-face&amp;rdquo; subscription popups, but the lack of those, the fact that some people are &amp;ldquo;invisible&amp;rdquo; because of RSS aggregators, and the fact that readers had to subscribe to the blog via obscure platforms&amp;hellip; well, there have few incentives to opt into potential email spam.&lt;/p>
&lt;p>Now compare this past multi-year effort of a self-hosted blog with custom email subscriptions to what I&amp;rsquo;ve experienced on Substack since the launch of Blog System/5:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-06-22-substack-subscribers.png" class="with-border">
&lt;figcaption>Subscribers growth during the first 6 months of Blog System/5's existence. I pretty much paused publishing in March to focus on other side projects and thus haven't seen any major growth since.&lt;/figcaption>
&lt;/figure>
&lt;p>I created Blog System/5 in late October 2023 and by March 2024 I had already amassed 700+ subscribers. It&amp;rsquo;s also true that I was lucky to have a few articles spread through Hacker News and other sites, and that I put a lot more effort in crafting these than what I wrote in the past, but still: using a platform that people know, that is focused on writing, and in which &lt;em>readers already have accounts&lt;/em> helps with growth. Yes, I&amp;rsquo;m sure there is a lot of fake accounts in there given that the typical &amp;ldquo;open count&amp;rdquo; for any article hovers around 50%, but that feels impressive regardless compared to what I was able to achieve in the past.&lt;/p>
&lt;h1 id="evolution-monetization">Evolution: Monetization&lt;/h1>
&lt;p>And before parting, let&amp;rsquo;s talk about the taboo topic, should we? Every hobby has to become a side hustle these days, right, but God forbid you try to charge for it.&lt;/p>
&lt;p>I have to confess that I&amp;rsquo;ve dabbled with monetizing my writing and done some experiments. These included: &lt;a href="/2009/06/trying-adsense.html">enabling Google AdSense&lt;/a> back when I used Blogger and then &lt;a href="/2010/05/ads-gone.html">running away from it&lt;/a>; writing paid-for &lt;a href="/2019/10/onlamp-articles.html">articles for OnLamp&lt;/a>, which weren&amp;rsquo;t really part of this blog originally but were written in the same &amp;ldquo;spirit&amp;rdquo;; enabling optional paid subscriptions in Substack, which has generated a handful of leads; and using Amazon affiliate links for certain product-focused articles, which has also generated some sales.&lt;/p>
&lt;p>None of these have generated any meaningful amount of income, but that doesn&amp;rsquo;t mean it isn&amp;rsquo;t exciting to see pennies flow in! In any case, I still struggle with the thought of pursuing monetization. It&amp;rsquo;d feel great to be paid for the effort I put in writing, but in the grand scheme of things, I would not be able to make it my primary income&amp;mdash;and I&amp;rsquo;d probably not want it to be either&amp;mdash;without putting in &lt;em>a ton more&lt;/em> effort. Plus I like my content to reach as far as possible, which paywalls would prevent.&lt;/p>
&lt;p>Does that mean I will &lt;em>never&lt;/em> try to more aggressively monetize this type of content? Never say never, but I have no plans for now. That said, I will still gladly take and infinitely appreciate any donations you might send; don&amp;rsquo;t be shy!&lt;/p>
&lt;h1 id="from-blogging-to-newslettering">From blogging to&amp;hellip; newslettering?&lt;/h1>
&lt;p>Why do I keep writing a blog after 20 years? Because I enjoy the &lt;em>process&lt;/em> and the reward of seeing a complete piece be appreciated by many. I know, &amp;ldquo;views&amp;rdquo; are a vanity metric, but they do translate into tangible real-world benefits. I&amp;rsquo;m convinced that I had the chance to land a Google internship in 2008 &lt;em>because&lt;/em> of what I wrote in my blog; I&amp;rsquo;m also pretty certain that my recent job changes have been aided by my writing; and during the last few months, I&amp;rsquo;ve met some folks I hadn&amp;rsquo;t seen for a long time and several have exclaimed &amp;ldquo;hey, I enjoy your blog!&amp;rdquo; which, let&amp;rsquo;s be honest, is flattering to hear.&lt;/p>
&lt;p>Are blogs truly dead? The answer sounds like &amp;ldquo;yes&amp;rdquo;. But what if I asked you if newsletters are truly dead? The answer sounds like &amp;ldquo;of course not&amp;rdquo;, right? And, to me, these two look like essentially the same thing. Maybe the differences lie in the fact that blogs &lt;em>used to&lt;/em> be made of much shorter posts and used to be part of a larger community, whereas newsletters focus on longer pieces distributed primarily via email&amp;hellip; but considering how blogging itself has evolved for some (including me), the two are identical.&lt;/p>
&lt;p>Which brings me to the final thought of the day: is &amp;ldquo;Blog System/5&amp;rdquo; the same as the &amp;ldquo;jmmv&amp;rsquo;s weblog&amp;rdquo; that launched 20 years ago? No, not really. They are quite different beasts at this point, but they still come from the same&amp;mdash;even if older&amp;mdash;person, they still scratch the same itch, and they are still my main window to showcase ideas and projects to the outside world. So, to me, they are indeed the same.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-06-22-livejournal-20041126.png" length="140932" type="image/jpeg"/></item><item><title>Porting the EndBASIC console to an LCD</title><link>https://jmmv.dev/2024/04/endbasic-st7735s.html</link><pubDate>Fri, 26 Apr 2024 13:30:00 -0700</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/04/endbasic-st7735s.html</guid><description>&lt;p>Hello again Blog System/5 and sorry for the radio silence for the last couple of months. I had been writing too much in here and neglecting my side projects so I &lt;em>needed&lt;/em> to get back to them. And now that I&amp;rsquo;ve made significant progress on cool new features for &lt;a href="https://www.endbasic.dev/">EndBASIC&lt;/a>, it&amp;rsquo;s time to write about them a little!&lt;/p>
&lt;p>One of the defining characteristics of EndBASIC is its hybrid console: what looks like a simple text terminal at first glance can actually render overlapping graphics and text &lt;em>at the same time&lt;/em>. This is a feature that I believe is critical to simplify learning and it first appeared with &lt;a href="/2021/11/endbasic-0.8.html">the 0.8 release&lt;/a> back in 2021.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-04-26-endbasic-hybrid-console.png" width="100%" />
&lt;figcaption>EndBASIC's hybrid console running in the web browser, rendering text and overlapping graphics at once, all controlled from the built-in command prompt.&lt;/figcaption>
&lt;/figure>
&lt;p>A few months before that, I had added support for simple GPIO manipulation in &lt;a href="/2021/02/endbasic-0.6.html">the 0.6 release&lt;/a> and, around that same time, I impulse-bought a tiny LCD for the Raspberry Pi with the hope of making the console work on it. But life happened and I lost momentum on the project&amp;hellip; until recently.&lt;/p>
&lt;p>In this post, I&amp;rsquo;ll guide you through the process of porting the EndBASIC hybrid console to the ST7735s 1.44&amp;quot; LCD shown below, which sports a resolution of 128x128 pixels, a D-pad, and 3 other buttons. I will cover the prerequisite work to make the port possible, dig into the GPIO and SPI interface of an LCD, outline the design for a fast rendering engine, and conclude with pointers for you to build your own &amp;ldquo;developer kit&amp;rdquo;. Let&amp;rsquo;s go!&lt;/p>
&lt;figure>
&lt;a href="https://www.amazon.com/gp/product/B077Z7DWW1?ie=UTF8&amp;psc=1&amp;linkCode=ll1&amp;tag=&amp;linkId=7a97236b22ca32377d308871eedde6fc&amp;language=en_US&amp;ref_=as_li_ss_tl">&lt;img src="/images/2024-04-26-st7735s-box.jpg" width="100%" />&lt;/a>
&lt;figcaption>The &lt;a href="https://www.amazon.com/gp/product/B077Z7DWW1?ie=UTF8&amp;psc=1&amp;linkCode=ll1&amp;tag=&amp;linkId=7a97236b22ca32377d308871eedde6fc&amp;language=en_US&amp;ref_=as_li_ss_tl">ST7735s 1.44" LCD&lt;/a>. Photo from back when I first unboxed this little device... on February 7th, 2021.&lt;/figcaption>
&lt;/figure>
&lt;h1 id="previous-implementation">Previous implementation&lt;/h1>
&lt;p>Even though I had plans to support graphics in the EndBASIC console from the very beginning, the first versions of the product did &lt;em>not&lt;/em> support graphics. That fact didn&amp;rsquo;t prevent me from defining a &lt;code>Console&lt;/code> abstraction anyway because I needed to support various operating systems and, more importantly, write &lt;a href="/2020/12/unit-testing-a-console-app.html">unit tests for the console&lt;/a>. Such abstraction was a straightforward trait representing text-only operations, like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="line">&lt;span class="cl">&lt;span class="k">trait&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Console&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">clear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">how&lt;/span>: &lt;span class="nc">ClearType&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">io&lt;/span>::&lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">color&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="p">(&lt;/span>&lt;span class="nb">Option&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">u8&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">Option&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">u8&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">text&lt;/span>: &lt;span class="kp">&amp;amp;&lt;/span>&lt;span class="kt">str&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">io&lt;/span>::&lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c1">// ... and more text operations ...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Later on, when the time came to add graphics support, I extended the &lt;code>Console&lt;/code> trait with additional rendering primitives, like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="line">&lt;span class="cl">&lt;span class="k">trait&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Console&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c1">// ... same as previous ...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">draw_circle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">_center&lt;/span>: &lt;span class="nc">PixelsXY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">_radius&lt;/span>: &lt;span class="kt">u16&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">io&lt;/span>::&lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nb">Err&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">io&lt;/span>::&lt;span class="n">Error&lt;/span>::&lt;span class="n">new&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">io&lt;/span>::&lt;span class="n">ErrorKind&lt;/span>::&lt;span class="n">Other&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34;No graphics support&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">draw_pixel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">_xy&lt;/span>: &lt;span class="nc">PixelsXY&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">io&lt;/span>::&lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nb">Err&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">io&lt;/span>::&lt;span class="n">Error&lt;/span>::&lt;span class="n">new&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">io&lt;/span>::&lt;span class="n">ErrorKind&lt;/span>::&lt;span class="n">Other&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s">&amp;#34;No graphics support&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c1">// ... and more graphics operations ...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>What constitutes a primitive or not depends on the graphics device. In the snippet above, it&amp;rsquo;s fairly easy to understand why &lt;code>draw_pixel&lt;/code> exists. But why is &lt;code>draw_circle&lt;/code> there? SDL2, for example, does not provide a circle drawing primitive and forces you to implement your own algorithm on top of individual pixel plotting. However, the HTML &lt;code>canvas&lt;/code> element &lt;em>does&lt;/em> provide such a primitive and thus it is important to expose access to it for performance reasons.&lt;/p>
&lt;p>A reasonable design, right?&lt;/p>
&lt;p>Not quite. The problem is that mixing the textual and graphical operations under just one abstraction bundles two concepts together. Take, for example, the innocent-looking &lt;code>print&lt;/code> function: writing text to the console requires knowing the current cursor position, writing text at that location, wrapping long lines, and then updating the cursor position and moving its glyph. This sequence of operations is given to us &amp;ldquo;for free&amp;rdquo; when the &lt;code>Console&lt;/code> is backed by a terminal&amp;mdash;the kernel or terminal emulator handle all details&amp;mdash;but none of this exists when you stare at a blank graphical canvas.&lt;/p>
&lt;p>As a result, the SDL2 and the HTML canvas console implementations had to supply their own code to represent a textual console on top of their backing graphical canvases. But due to tight timelines&amp;mdash;I wanted to release graphics support &lt;a href="https://vimeo.com/641791002">in time for a conference&lt;/a>&amp;mdash;I ended up with &lt;em>a lot&lt;/em> of copy/pasted code between the two. Unfortunately, it wasn&amp;rsquo;t just copy/pasted code. Oh no. It was copy/pasted code with minor variations throughout. For example: SDL2 coordinates are &lt;code>i32&lt;/code> whereas HTML canvas coordinates are &lt;code>float&lt;/code>s.&lt;/p>
&lt;p>This duplication with minor differences required unification before attempting to write a console driver for the LCD because I could not afford to introduce yet another duplicate of the same code.&lt;/p>
&lt;h1 id="past-netbsd-experience-to-the-rescue">Past NetBSD experience to the rescue&lt;/h1>
&lt;p>The first step to resolving this conundrum was to unify the implementation of the two graphical consoles as much as possible. Having worked on &lt;a href="https://www.netbsd.org/docs/guide/en/chap-cons.html">NetBSD&amp;rsquo;s &lt;code>wscons&lt;/code> console framework&lt;/a> eons ago, I knew how this had to be done.&lt;/p>
&lt;p>The idea was to separate the text console manipulation from the underlying graphics primitives. As you can imagine, the way to do this was via a new abstraction, &lt;code>RasterOps&lt;/code>, providing direct access to the &amp;ldquo;raster operations&amp;rdquo; of the graphics device. Take a look:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="line">&lt;span class="cl">&lt;span class="k">trait&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">RasterOps&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">draw_circle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">center&lt;/span>: &lt;span class="nc">PixelsXY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">radius&lt;/span>: &lt;span class="kt">u16&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">io&lt;/span>::&lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">draw_pixel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">xy&lt;/span>: &lt;span class="nc">PixelsXY&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">io&lt;/span>::&lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">move_pixels&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">x1y1&lt;/span>: &lt;span class="nc">PixelsXY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">x2y2&lt;/span>: &lt;span class="nc">PixelsXY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">size&lt;/span>: &lt;span class="nc">SizeInPixels&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">io&lt;/span>::&lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">write_text&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">xy&lt;/span>: &lt;span class="nc">PixelsXY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">text&lt;/span>: &lt;span class="kp">&amp;amp;&lt;/span>&lt;span class="kt">str&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">io&lt;/span>::&lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c1">// ... and more operations ...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As you can see in this snippet, the earlier operations &lt;code>draw_circle&lt;/code> and &lt;code>draw_pixel&lt;/code> exist as raster operations because some devices may provide them as primitives. But we now see other functions like &lt;code>move_pixels&lt;/code> and &lt;code>write_text&lt;/code> (which differs from &lt;code>print&lt;/code> because it writes text at a specific pixel location, not at the &amp;ldquo;current cursor position&amp;rdquo;) and these are meant to expose other primitives of the device.&lt;/p>
&lt;p>Having done this, I added a new &lt;code>GraphicsConsole&lt;/code> type that encapsulates the logic to implement a terminal backed by an arbitrary graphical display, and then relies on a specific implementation of the &lt;code>RasterOps&lt;/code> trait to do the screen manipulation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="line">&lt;span class="cl">&lt;span class="k">impl&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="no">RO&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Console&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">for&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">GraphicsConsole&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="no">RO&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">where&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="no">RO&lt;/span>: &lt;span class="nc">RasterOps&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c1">// ... implement Console in terms of RasterOps ...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Most details of this refactor are uninteresting at this point, except for the fact that it eliminated all of the tricky duplicate code between the SDL2 and HTML canvas variants. A side-effect was that these changes increased my confidence in the HTML canvas variant because most code was now unit-tested indirectly via the SDL2 implementation. Here is how the simplification looked like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">&lt;span class="line">&lt;span class="cl">$ wc -l {before,after}/sdl/src/host.rs {before,after}/web/src/canvas.rs
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1245 before/sdl/src/host.rs
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 887 after/sdl/src/host.rs # 358 fewer lines
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 760 before/web/src/canvas.rs
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 444 after/web/src/canvas.rs # 320 fewer lines
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I was now ready to reuse the &lt;code>GraphicsConsole&lt;/code> to quickly provide a new backend on top of the LCD.&lt;/p>
&lt;h1 id="playing-with-the-c-interface">Playing with the C interface&lt;/h1>
&lt;p>With the foundational abstractions in place, it was time to prove that my idea of rendering the EndBASIC console on the tiny LCD was &lt;del>feasible&lt;/del> cool enough to pursue. But I knew absolutely nothing about the LCD so, before diving right into the &amp;ldquo;correct&amp;rdquo; implementation, I had to create a prototype to get used to its API.&lt;/p>
&lt;p>Fortunately, I found an &amp;ldquo;SDK&amp;rdquo;&amp;mdash;and I put it in quotes because SDK is quite an overstatement&amp;mdash;for the LCD somewhere online. The SDK consists of a pair of C and Python libraries that provide basic graphics drawing primitives, text rendering, and access to the D-pad and the buttons. This was a &amp;ldquo;perfect&amp;rdquo; finding because the code was simple enough to understand, functional enough to build a prototype, and allowed me to skip reading through the data sheet.&lt;/p>
&lt;p>So I chose to do the easiest thing: I folded the C library into EndBASIC using Rust&amp;rsquo;s C FFI and quickly implemented a &lt;code>RasterOps&lt;/code> on it. With that, I could instantiate a &lt;code>GraphicsConsole&lt;/code> backed by the LCD and demonstrate, in just a couple of hours before the crack of dawn, that the idea was feasible. And it was more than feasible: it was super-exciting! Here, witness the very first results:&lt;/p>
&lt;figure>
&lt;video width="100%" controls>
&lt;source src="/images/2024-04-26-endbasic-st7735s-prototype.mov" type="video/mp4">
&lt;/video>
&lt;figcaption>First prototype of the EndBASIC console on the ST7735s LCD display, reusing the vendor-provided C library from Rust with lots of hacks.&lt;/figcaption>
&lt;/figure>
&lt;p>But performance was also awful. Using the C library was enough to show that this idea was cool and worth pursuing, but it was not the best path forward. On the one hand, the code in the C library was rather crappy and fragile to integrate into Rust via &lt;code>build.rs&lt;/code>; and, on the other hand, I needed precise control of the hardware primitives to achieve good rendering performance levels.&lt;/p>
&lt;h1 id="the-lcd-hardware-interface">The LCD hardware interface&lt;/h1>
&lt;p>Before diving into the solution, let&amp;rsquo;s take a quick peek at the hardware interface of the LCD. Note that this is just a very simplified picture of what the LCD offers and is derived from my reverse-engineering of the code in the SDK. I&amp;rsquo;m sure there is more to it, but I do not have the drive to go through the aforementioned data sheet.&lt;/p>
&lt;p>The LCD is a matrix of pixels where each position contains an integer representing the color of the pixel. The representation of each color value depends on the selected pixel format, but what the SDK uses by default is RGB565: a 16-bit value decomposed into two 5-bit quantities for red and blue and one 6-bit quantity for green.&lt;/p>
&lt;p>This is pretty standard stuff for a framebuffer-style device but the mechanism to write to the LCD is different. Unlike most framebuffer devices backed by video cards, the LCD matrix is not memory-mapped: we have to issue specific reads and writes via the GPIO and SPI busses. The GPIO bus is used to control hardware registers and the SPI bus is used to transfer large data sequences.&lt;/p>
&lt;p>Simplifying, the LCD offers just two primitives. The first is a &amp;ldquo;set window&amp;rdquo; operation to define the window for rendering. This tells the LCD what rectangular region to update with new pixel values on the &lt;em>subsequent&lt;/em> &amp;ldquo;set data&amp;rdquo; operation. Executing this operation requires a GPIO write to select the operation followed by an SPI write to set the window position and dimensions.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-04-26-rpi-lcd-set-window.png" width="100%" />
&lt;figcaption>Representation of an LCD's "set window" operation.&lt;/figcaption>
&lt;/figure>
&lt;p>The second is a &amp;ldquo;set data&amp;rdquo; primitive to set the contents of the previously-defined window. This primitive just sends a stream of pixel color values that will be written on the window, top-left to bottom-right. Executing this operation requires a GPIO write to select the operation followed by a &amp;ldquo;very large&amp;rdquo; SPI write to write the pixel data.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-04-26-rpi-lcd-set-data.png" width="100%" />
&lt;figcaption>Representation of an LCD's "set data" operation.&lt;/figcaption>
&lt;/figure>
&lt;p>By default, SPI writes on Linux are limited to 4KB in size and the full LCD content is 128x128 pixels * 2 bytes per pixel = 32KB, which means drawing the whole LCD requires 8 separate SPI writes. (It is possible to raise the default via a kernel setting, which I did too, but let&amp;rsquo;s ignore that for now.)&lt;/p>
&lt;p>Understanding that the LCD hardware interface is as simple and constrained as this is super-important to devise an efficient higher-level API, which is what we are going to do next.&lt;/p>
&lt;h1 id="double-buffering">Double-buffering&lt;/h1>
&lt;p>Having figured out the native interface, I faced two problems.&lt;/p>
&lt;p>The first one was performance. Issuing individual &amp;ldquo;set window&amp;rdquo; and &amp;ldquo;set data&amp;rdquo; commands works great to draw on large portions of the LCD, but sending these commands on a pixel basis as would be necessary to render letters or shapes like circles would be prohibitively slow.&lt;/p>
&lt;figure>
&lt;video width="100%" controls>
&lt;source src="/images/2024-04-26-endbasic-st7735s-demo-ffi.mov" type="video/mp4">
&lt;/video>
&lt;figcaption>An EndBASIC demo program bouncing a bunch of rectangles on the screen running with the prototype console driver that relied on the C library without double-buffering or damage tracking.&lt;/figcaption>
&lt;/figure>
&lt;p>The second one was the inability to &lt;em>read&lt;/em> the contents of the LCD. Certain operations of the console, like moving the cursor or entering/exiting the builtin editor, require saving portions of the screen aside for later restoration. The LCD does not offer an interface to read what it currently displays. (Actually I don&amp;rsquo;t know because I did not read the data sheet&amp;hellip; but even if it does, the reads would involve slow SPI bus traffic which we should avoid.)&lt;/p>
&lt;p>To address these two problems, I decided to implement double-buffering for the LCD: the console driver reserves a portion of main memory to track the content of the LCD and all operations that update the LCD first write to this buffer. This allows the code to decide &lt;em>when&lt;/em> to send the buffer to the LCD, allowing it to compose complex patterns in memory&amp;mdash;e.g. plotting letters pixel by pixel&amp;mdash;before ever touching the GPIO and SPI busses. And this also allows trivially reading portions of the LCD.&lt;/p>
&lt;figure>
&lt;img src="/images/2024-04-26-rpi-lcd-buffer-damage.png" width="100%" />
&lt;figcaption>Representation of the double-buffering and damage tracking techniques.&lt;/figcaption>
&lt;/figure>
&lt;p>Which brings me to the next point. Part of this solution required adding damage tracking as well. Flushing the full in-memory buffer to the LCD is expensive&amp;mdash;remember, with the default SPI configuration, we need to issue 8 separate writes to flush 32KB of data&amp;mdash;and many times, like when moving the cursor around in the editor, this is not necessary. Therefore, it is important to keep track of the portion of the display that has been &amp;ldquo;damaged&amp;rdquo; by drawing operations and then only flush that portion of the buffer to the LCD. This maps perfectly well to the hardware primitives offered by the LCD.&lt;/p>
&lt;p>With that in mind, and after a bit of iteration, I ended up splitting the implementation of the LCD &lt;code>RasterOps&lt;/code> into two pieces. One is the &lt;code>Lcd&lt;/code> trait, which exposes the basic &amp;ldquo;set window&amp;rdquo; and &amp;ldquo;set data&amp;rdquo; primitives of an LCD as a single &lt;code>set_data&lt;/code> operation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="line">&lt;span class="cl">&lt;span class="k">trait&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Lcd&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">set_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">x1y1&lt;/span>: &lt;span class="nc">LcdXY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">x2y2&lt;/span>: &lt;span class="nc">LcdXY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">data&lt;/span>: &lt;span class="kp">&amp;amp;&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="kt">u8&lt;/span>&lt;span class="p">])&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">io&lt;/span>::&lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c1">// ... some more stuff ...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The other is the &lt;code>BufferedLcd&lt;/code> type, which implements the general idea of buffering and damage tracking on top of a generic &lt;code>Lcd&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="line">&lt;span class="cl">&lt;span class="k">impl&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">L&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">RasterOps&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">for&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">BufferedLcd&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">L&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">where&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">L&lt;/span>: &lt;span class="nc">Lcd&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c1">// ... implementation of RasterOps in terms of an Lcd ...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This design keeps all of the complex logic in one place and, as was the case with the &lt;code>Console&lt;/code> abstraction, allowed me to unit-test the tricky corner cases of the double-buffering and damage tracking by supplying a test-only &lt;code>Lcd&lt;/code> implementation. This was critical during development because I wrote most of this code while I was on a trip without access to the device, and when I came back home, the &amp;ldquo;real thing&amp;rdquo; worked on the first try.&lt;/p>
&lt;figure>
&lt;video width="100%" controls>
&lt;source src="/images/2024-04-26-endbasic-st7735s-demo-native.mov" type="video/mp4">
&lt;/video>
&lt;figcaption>Same EndBASIC demo program as before but running with the improved console that implements double-buffering and damage tracking. No more flickering.&lt;/figcaption>
&lt;/figure>
&lt;p>Invest in Rust and exhaustive unit testing! They really are worth it to deliver working software quickly. But I digress&amp;hellip;&lt;/p>
&lt;h1 id="font-rendering">Font rendering&lt;/h1>
&lt;p>The last interesting part of the puzzle was rendering text. You see: for the previous graphical consoles I wrote based on SDL2 and the HTML &lt;code>canvas&lt;/code> element, I was able to use &lt;a href="https://www.ibm.com/plex/">a nice TTF font&lt;/a>. But with the LCD&amp;hellip; well, the LCD has no builtin mechanism to render text and the very restrictive 128x128 resolution means that most fonts would render poorly at small sizes anyway.&lt;/p>
&lt;p>To solve this problem, I had to implement my own text rendering code, which sounds scary at first but isn&amp;rsquo;t as hard as it sounds. One issue was coming up with the font data, but the SDK for the LCD came with a free reusable 5x8 font that I took over. Another issue was coming up with a mechanism to efficiently render the letters because plotting them pixel by pixel on the LCD would be unfeasible, but this was solved by the previous buffering and damage tracking techniques.&lt;/p>
&lt;p>Having my own font rendering code is interesting. If you used a BASIC machine back in the 1980s, the computer-supplied manual would show you the glyphs used by the machine like this page demonstrates:&lt;/p>
&lt;figure>
&lt;img src="/images/2024-04-26-amstrad-cpc-c7p9.png" width="100%" class="with-border"/>
&lt;figcaption>Page 9 of Chapter 7 of the Amstrad CPC 6128 user manual showing the glyphs corresponding to characters 32&amp;ndash;48.&lt;/figcaption>
&lt;/figure>
&lt;p>In some machines like the Amstrad CPC 6128, you could use the &lt;code>SYMBOL&lt;/code> command to define your own glyphs. And in some others, you could &lt;code>POKE&lt;/code> the memory locations where the glyphs were stored to (re)define them. So I&amp;rsquo;m thinking that&amp;hellip; maybe I should forget about the TTF font and instead expose a custom bitmap font in all EndBASIC consoles to allow for these neat tricks.&lt;/p>
&lt;h1 id="putting-it-all-together">Putting it all together&lt;/h1>
&lt;p>And that&amp;rsquo;s all folks! Having gone through all the pieces involved in providing support for the LCD, we can now connect them all. Here is how the code on the &lt;code>main&lt;/code> branch looks like with additional inline commentary:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">// Instantiate access to the GPIO bus, required to control the LCD and
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">// to read the status of the D-pad and buttons.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kd">let&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">gpio&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Gpio&lt;/span>::&lt;span class="n">new&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="n">map_err&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gpio_error_to_io_error&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">?&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">// Build the struct used to interface with the LCD.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kd">let&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">lcd&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ST7735SLcd&lt;/span>::&lt;span class="n">new&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">gpio&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">?&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">// Wrap the low-level LCD with the double-buffering and damage tracking
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">// features for high speed.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kd">let&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">lcd&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">BufferedLcd&lt;/span>::&lt;span class="n">new&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lcd&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">// Instantiate the input controller. I haven&amp;#39;t covered this in the post
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">// because it&amp;#39;s uninteresting.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kd">let&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">input&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ST7735SInput&lt;/span>::&lt;span class="n">new&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="k">mut&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">gpio&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">signals_tx&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">?&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">// Wrap the display and the buttons with the generic graphical console
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">// logic -- the same one used by the SDL2 and HTML canvas variants.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kd">let&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">inner&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">GraphicsConsole&lt;/span>::&lt;span class="n">new&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">lcd&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">?&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Even though this looks complex, this composition of three different types keeps the responsibilities of each layer separate. The layers help make the logic simpler to understand, allow the possibility of supporting more LCDs with ease, and make unit-testing a reality. And thanks to Rust&amp;rsquo;s static dispatch, the overhead of the separate types is minimal.&lt;/p>
&lt;p>Witness the finished result:&lt;/p>
&lt;figure>
&lt;video width="100%" controls>
&lt;source src="/images/2024-04-26-endbasic-st7735s-snake.mov" type="video/mp4">
&lt;/video>
&lt;figcaption>EndBASIC running the snake game on the Raspberry Pi with the ST7735s console, showing the final graphics support as well as interaction with the physical buttons.&lt;/figcaption>
&lt;/figure>
&lt;h1 id="build-your-own-developer-kit">Build your own Developer Kit&lt;/h1>
&lt;p>Does the above sound cool? Do you want to play with it? Here are the parts you&amp;rsquo;ll need to build you own:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>&lt;a href="https://www.amazon.com/ELEMENT-Element14-Raspberry-Pi-Motherboard/dp/B07BDR5PDW?&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=cab45a45b3e43934d489486d416be7a0&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">Raspberry Pi 3 B+&lt;/a>:&lt;/strong> I suppose a newer model will work but I don&amp;rsquo;t have one to try it out; let me know if it does! I also have my eyes on the &lt;a href="https://www.amazon.com/gp/product/B074P6BNGZ?th=1&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=1f5bdad0f78d4512685cda89db5c6d87&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">Libre Computer Board - Le Potato&lt;/a>, but if you go this route, know that it will &lt;em>definitely&lt;/em> require extra work in EndBASIC to be functional. Hardware donations welcome if you want me to give it a try 😉.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;a href="https://www.amazon.com/CanaKit-Raspberry-Supply-Adapter-Listed/dp/B00MARDJZ4?&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=ef82e298644b68298cd9bf5559b74aaa&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">CanaKit 5V 2.5A Power Supply&lt;/a>:&lt;/strong> Yes, the Raspberry Pi is USB-powered but you need a lot of power for it to run properly&amp;mdash;particularly if you are going to attach any USB devices like hard disks. Don&amp;rsquo;t skimp on the power supply. This is the one I have and works well.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;a href="https://www.amazon.com/PNY-Elite-microSDHC-Memory-P-SDU32GU185GW-GE/dp/B07R8GVGN9?th=1&amp;amp;linkCode=ll1&amp;amp;tag=blogsystem5-20&amp;amp;linkId=33a25c49f236d8bd6e8f2259a6c57148&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">PNY 32GB microSD card&lt;/a>:&lt;/strong> Buy any microSD card you like. The SD bay is incredibly slow no matter what and 32GB should be plenty for experimentation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;a href="https://www.amazon.com/gp/product/B077Z7DWW1?ie=UTF8&amp;amp;psc=1&amp;amp;linkCode=ll1&amp;amp;tag=&amp;amp;linkId=7a97236b22ca32377d308871eedde6fc&amp;amp;language=en_US&amp;amp;ref_=as_li_ss_tl">waveshare 1.44inch LCD Display HAT 128x128&lt;/a>:&lt;/strong> The star product of this whole article! You may find other LCD hats and I&amp;rsquo;m sure they can be made to work, but they&amp;rsquo;ll require code changes. Hopefully the abstractions I implemented make it easy to support other hats, but I can&amp;rsquo;t tell yet. Again, hardware donations welcome if you want to keep me busy 😉.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Once you have those pieces, get either Raspbian or Ubuntu, flash the image to the microSD, and boot the machine. After that, you have to build and run EndBASIC from unreleased sources until I publish 0.11:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">$ cargo install --git&lt;span class="o">=&lt;/span>https://github.com/endbasic/endbasic.git --features&lt;span class="o">=&lt;/span>rpi
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ ~/.cargo/bin/endbasic --console&lt;span class="o">=&lt;/span>st7735s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I know, I know, this is all quite convoluted. You have to manually go through the process of setting up Linux, then installing Rust, and then building EndBASIC from source. All of this is super-slow too.&lt;/p>
&lt;p>Which means&amp;hellip; what I want to do next is to build a complete &amp;ldquo;Developer Kit&amp;rdquo;, including a lightweight prebuilt SD image that gives you access to EndBASIC out of the box in just a few seconds. Right now I&amp;rsquo;m playing with downsizing a NetBSD/evbarm build so that the machine can boot quickly and, once I have that ready, I&amp;rsquo;ll have to port EndBASIC&amp;rsquo;s hardware-specific features to work on it. I don&amp;rsquo;t think I&amp;rsquo;ll postpone 0.11 until this is done, but we&amp;rsquo;ll see.&lt;/p>
&lt;p>Interested in any of this? Please leave a note! And if you have tried the above at all with your own hardware, post your story too!&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-04-26-endbasic-st7735s-editor.jpg" length="2750893" type="image/jpeg"/></item><item><title>How "new type" helps avoid production outages</title><link>https://jmmv.dev/2024/03/new-type-and-production-outages.html</link><pubDate>Sat, 09 Mar 2024 09:50:00 -0700</pubDate><author>julio@meroh.net (Julio Merino)</author><guid>https://jmmv.dev/2024/03/new-type-and-production-outages.html</guid><description>&lt;p>My &lt;a href="/2024/01/links-january-2024-edition.html">January links recap&lt;/a> included the &lt;a href="https://experimentalworks.net/posts/2024-01-22-simple-phantom-types/">&amp;ldquo;Phantom Types&amp;rdquo;&lt;/a> article by David Soria Parra. In it, the author briefly touches upon the &amp;ldquo;new type&amp;rdquo; idiom, its typical implementation in Rust, and then proceeds to propose a better alternative. But the question arises: why should you care?&lt;/p>
&lt;p>To answer why this idiom is useful, I want to present you with a real production problem we faced in the Storage Infrastructure team at Google circa 2010. That issue made me a convert and I&amp;rsquo;ve kept it in mind when designing APIs since then.&lt;/p>
&lt;h1 id="what-is-new-type-anyway">What is &amp;ldquo;new type&amp;rdquo; anyway?&lt;/h1>
&lt;p>The &lt;a href="https://rust-unofficial.github.io/patterns/patterns/behavioural/newtype.html">&amp;ldquo;new type&amp;rdquo; idiom&lt;/a> is a programming pattern whereby you define domain-specific types to wrap native types. Then, you use those types in your code and only &amp;ldquo;lower&amp;rdquo; them to their underlying types when passing the values to any external APIs.&lt;/p>
&lt;p>More specifically, your public interfaces&amp;mdash;but also private methods and functions&amp;mdash;should never receive primitive types like integers or strings. A function like &lt;code>allow_write(username: String, size: usize)&lt;/code> should not exist in your code base.&lt;/p>
&lt;p>Instead, what you&amp;rsquo;d do is define two new types, &lt;code>Username&lt;/code> and &lt;code>Size&lt;/code>, that wrap the string and integer values. These types are trivial wrappers over the native types (e.g. &lt;code>struct Username(String)&lt;/code> and &lt;code>struct Size(usize)&lt;/code>) with the goal of making them zero-cost abstractions.&lt;/p>
&lt;p>However, as David&amp;rsquo;s article describes, this simplistic way to implement the pattern lends itself to code duplication for every type. And, as you know, code duplication leads to slight inconsistencies over time, which may increase maintenance costs.&lt;/p>
&lt;p>But this is not relevant now. &amp;ldquo;New type&amp;rdquo; is &amp;ldquo;new type&amp;rdquo; no matter how you implement it and no matter which language you choose. What I want to showcase here is &lt;em>why&lt;/em> adopting this pattern helps at all. And, for that, it&amp;rsquo;s time to dive into the story.&lt;/p>
&lt;h1 id="quota-management-outage">Quota management outage&lt;/h1>
&lt;p>Back in the early days of the Storage Infrastructure at Google, we the SRE team managed tens of shared file system deployments (aka &lt;em>cells&lt;/em>). Those file systems were multi-user and had support for quotas&amp;mdash;like pretty much any file system worthy of consideration.&lt;/p>
&lt;p>Storage quotas are typically expressed as two quantities: a &lt;em>bytes quota&lt;/em>, which says how much disk space a user can use; and a &lt;em>files quota&lt;/em>, which says how many files a user can store. Tracking byte usage limits disk usage and tracking file usage limits metadata overhead.&lt;/p>
&lt;p>Now, even with the replicated and zonal system that Google had, it was convenient to &lt;em>manage&lt;/em> user quotas in a centralized way. So that&amp;rsquo;s what we also had: the equivalent of a MySQL database tracking how much quota each user had allocated in which cell, world-wide.&lt;/p>
&lt;p>So far so good. But how do you keep the decentralized storage cells decoupled from the quota database? Easy! Via a cron job that reads the quotas and pushes them to cluster-local &amp;ldquo;configuration&amp;rdquo; files so that each cell doesn&amp;rsquo;t need to know about the central database.&lt;/p>
&lt;p>Of course, this cron job is code. And code has bugs. So here is what happened: there was some function somewhere that looked like this: &lt;code>set_quota(user: String, bytes: usize, files: usize)&lt;/code>. And, for whatever reason, a caller invoked it as &lt;code>set_quota(user, files, bytes)&lt;/code>.&lt;/p>
&lt;p>Notice the problem? The caller swapped the arguments but the language did not flag this issue and&amp;hellip; the tests didn&amp;rsquo;t catch it either! It&amp;rsquo;s unfortunately too common for people to write the same sentinel values (&lt;code>set_quota(&amp;quot;foo&amp;quot;, 100, 100)&lt;/code>) for different test arguments.&lt;/p>
&lt;p>Soon after this change was checked in, the code rolled out to production and&amp;hellip; of course, the moment it touched the first canary deployment, things went south. Users started receiving out of quota alerts even when they should have had, in theory, plenty of available quota.&lt;/p>
&lt;div class="container action-highlight p-4 my-4 d-md-none">
&lt;div class="row text-center">
&lt;p>A blog on operating systems, programming languages, testing, build systems, my own software
projects and even personal productivity. Specifics include FreeBSD, Linux, Rust, Bazel and
EndBASIC.&lt;/p>
&lt;/div>
&lt;div class="row">
&lt;div class="col">
&lt;div class="form-group">
&lt;form action="https://endtracker.azurewebsites.net/api/sites/e8da9f62-b7ac-4fe9-bf20-7c527199a376/subscribers/add" method="post">
&lt;input type="text" name="email"
placeholder="Enter your email"
class="form-control input-sm text-center my-1"/>
&lt;button type="submit" class="btn btn-primary btn-block my-1">Subscribe&lt;/button>
&lt;/form>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="row px-2">
&lt;div class="col col-sm-5 text-left">
&lt;small>&lt;span class="subscriber-count">0&lt;/span> subscribers&lt;/small>
&lt;/div>
&lt;div class="col col-sm-7 text-right">
&lt;p>
&lt;a rel="me" href="https://mastodon.online/@jmmv">
&lt;img src="/images/badges/mastodon-logo.svg" width="32px" height="32px" alt="Follow @jmmv on Mastodon">
&lt;/a>
&lt;a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fjmmv.dev%2F&amp;amp;screen_name=jmmv">
&lt;img src="/images/badges/Twitter_logo_blue.svg" width="32px" height="32px" alt="Follow @jmmv on Twitter">
&lt;/a>
&lt;a href="/feed.xml">&lt;img src="/images/badges/feed-icon-28x28.png" alt="RSS feed">&lt;/a>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h1 id="the-aftermath">The aftermath&lt;/h1>
&lt;p>The fix to the above was trivial: swap the arguments in the problematic caller and call it a day. But that&amp;rsquo;s a naïve fix. It just corrects the immediate problem but does not minimize the chances of it happening again. That is: this change is a &lt;em>mitigation&lt;/em>, not a &lt;em>fix&lt;/em>.&lt;/p>
&lt;p>In good SRE fashion, the team went a bit further to prevent this issue from ever recurring. They adopted the &amp;ldquo;new type&amp;rdquo; pattern, created two separate types&amp;mdash;&lt;code>BytesQuota&lt;/code> and &lt;code>FilesQuota&lt;/code>&amp;mdash;and plumbed them through the whole program. And note: Rust wasn&amp;rsquo;t really a thing in 2010.&lt;/p>
&lt;p>Furthermore, the team contributed an entry to the &lt;a href="https://testing.googleblog.com/2007/01/introducing-testing-on-toilet.html">Testing on the Toilet blog&lt;/a>, describing the issue and how this same problem had &lt;em>just&lt;/em> bitten a major financial company and had made them lose millions of dollars.&lt;/p>
&lt;p>And that&amp;rsquo;s the simple story of the day. Please adopt the new type pattern. Future-you will be happy that you did when the compiler or language interpreter yells at you about something that&amp;rsquo;s obviously wrong.&lt;/p></description><enclosure url="https://jmmv.dev/images/2024-03-09-new-types-store.jpg" length="552435" type="image/jpeg"/></item></channel></rss>