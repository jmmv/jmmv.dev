<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Julio Merino (jmmv.dev)</title><link>https://jmmv.dev/</link><description>Recent content on Julio Merino (jmmv.dev)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 07 Jul 2022 07:00:00 -0700</lastBuildDate><atom:link href="https://jmmv.dev/feed.xml" rel="self" type="application/rss+xml"/><item><title>Tips on formatting Markdown lists</title><link>https://jmmv.dev/2022/07/markdown-lists.html</link><pubDate>Thu, 07 Jul 2022 07:00:00 -0700</pubDate><guid>https://jmmv.dev/2022/07/markdown-lists.html</guid><description>&lt;p>Lists are a very common construct in technical documents, which is the kind of material I most often write and &lt;em>review&lt;/em>. But getting complex lists to look right is tricky, especially when authoring them in Markdown.&lt;/p>
&lt;p>The problem with lists starts early on. All Markdown training material will teach you that lists are written like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">This is an unordered list:
&lt;span class="k">*&lt;/span> First.
&lt;span class="k">*&lt;/span> Second.
&lt;span class="k">*&lt;/span> Third.
And this is an ordered list:
&lt;span class="k">1.&lt;/span> First.
&lt;span class="k">2.&lt;/span> Second.
&lt;span class="k">3.&lt;/span> Third.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Looks trivial and easily readable in its textual form, right? Unfortunately, this style quickly breaks down as soon as you have multiple people editing the same document or you start nesting lists and code blocks.&lt;/p>
&lt;p>From experience, I&amp;rsquo;ve come up with a style of writing lists that ensures they are always correctly and consistently formatted, and that mistakes are trivial to spot at review time. I find myself repeating these tips and their rationale during PR reviews, so here is a full-fledged explanation for posterity&amp;rsquo;s sake.&lt;/p>
&lt;h1 id="ordered-list-numbers">Ordered list numbers&lt;/h1>
&lt;p>When working with a large ordered list, keeping items properly numbered is a task that seems impossible. Oftentimes, I&amp;rsquo;ve gotten PRs &lt;em>for new content&lt;/em> where the item numbers were wrong from the get go. Things like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="k">1.&lt;/span> First.
&lt;span class="k">3.&lt;/span> Third already? I guess the item was moved at the last minute.
&lt;span class="k">2.&lt;/span> Second.
&lt;span class="k">2.&lt;/span> Second again. Copy/paste error?
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Markdown will happily accept this and will render the list with correct sequential numbering (note: it will fix the numbering, not reorder your items!). But&amp;hellip; do you think this is good? In my opinion, this looks sloppy&amp;mdash;and sloppiness of this kind during a PR review is an indicator that the rest of your change is suspect of more serious problems and deserves thorough scrutiny.&lt;/p>
&lt;p>The problems don&amp;rsquo;t end with the author having to be more careful though. If you &lt;em>try&lt;/em> to do the right thing and try to keep the list numbers sequential, you&amp;rsquo;ll have cases where a trivial addition to a document will result in a large diff because you had to renumber all subsequent entries. Take a look at this change and try to answer the question of &amp;ldquo;what is this doing?&amp;quot;:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-diff" data-lang="diff">&lt;span class="gd">--- before 2022-07-07 05:50:28.439334000 -0700
&lt;/span>&lt;span class="gd">&lt;/span>&lt;span class="gi">+++ after 2022-07-07 05:50:48.189334000 -0700
&lt;/span>&lt;span class="gi">&lt;/span>&lt;span class="gu">@@ -1,3 +1,4 @@
&lt;/span>&lt;span class="gu">&lt;/span> 1. This item comes first.
&lt;span class="gd">-2. This item is in the middle.
&lt;/span>&lt;span class="gd">-3. This item comes last.
&lt;/span>&lt;span class="gd">&lt;/span>&lt;span class="gi">+2. This item is also in the middle.
&lt;/span>&lt;span class="gi">+3. This item is in the middle.
&lt;/span>&lt;span class="gi">+4. This item comes last.
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This patch is simply adding a new item in position 2, but the change had to end up touching two extra unrelated lines. This is hard to review and pollutes the output of &lt;code>git blame&lt;/code> (or whichever equivalent annotation command your VCS provides).&lt;/p>
&lt;p>The easiest way out of this problem is to &lt;em>not&lt;/em> number your lists sequentially. Instead, always prefix them with &lt;code>1.&lt;/code>, like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="k">1.&lt;/span> First.
&lt;span class="k">1.&lt;/span> Second, really.
&lt;span class="k">1.&lt;/span> Third. Yes, this works!
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Doing so will free you from having to maintain sequential list numbers and will also keep diffs clean. Going back to our previous example, adding an item in position 2 would look like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-diff" data-lang="diff">&lt;span class="gd">--- before 2022-07-07 05:53:31.549334000 -0700
&lt;/span>&lt;span class="gd">&lt;/span>&lt;span class="gi">+++ after 2022-07-07 05:53:33.429334000 -0700
&lt;/span>&lt;span class="gi">&lt;/span>&lt;span class="gu">@@ -1,3 +1,4 @@
&lt;/span>&lt;span class="gu">&lt;/span> 1. This item comes first.
&lt;span class="gi">+1. This item is also in the middle.
&lt;/span>&lt;span class="gi">&lt;/span> 1. This item is in the middle.
1. This item comes last.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Clean and to the point.&lt;/p>
&lt;h1 id="nested-elements">Nested elements&lt;/h1>
&lt;p>The other problem when formatting lists comes from complex lists that have more than just one paragraph per item or higher-level blocks within them. Inevitably, people new to Markdown will write things like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="k">*&lt;/span> This is the first bullet point.
We try to add a second paragraph, but it may not be within the item.
&lt;span class="k">*&lt;/span> This is the second bullet point.
And we want a code block within it, but this is not a code block!
Neither is this!
&lt;span class="k">*&lt;/span> This is the third bullet point.
&lt;span class="k">
&lt;/span>&lt;span class="k"> &amp;gt; &lt;/span>&lt;span class="ge">And we try to add a blockquote like we added a second paragraph, but fail.
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above will not be formatted as you expect. The second paragraph in the first list item may or may not end up rendered as a paragraph within the list item. The code blocks in the second list item may or may not be rendered as a code block, but if they are, they will &lt;em>not&lt;/em> be nested. And if you try to nest lists, all bets are off.&lt;/p>
&lt;p>These problems don&amp;rsquo;t look apparent in the raw Markdown text and they are difficult to spot in document previews unless you pay close attention to the rendering. Most people don&amp;rsquo;t seem to notice when code blocks are wider than they are supposed to be, for example, or when paragraphs are not correctly aligned within their container list items.&lt;/p>
&lt;p>Plus it&amp;rsquo;s not only about the looks. These formatting mistakes break the &lt;em>semantics&lt;/em> of the document: instead of having a single list with longer individual elements, you end up with &lt;em>two&lt;/em> disjoint lists. I don&amp;rsquo;t have experience with screen readers, but I&amp;rsquo;m pretty sure this poses an accessibility problem.&lt;/p>
&lt;p>The above problems are fixable, obviously. If we look at the &lt;a href="https://www.markdownguide.org/basic-syntax#adding-elements-in-lists">Adding Elements in Lists&lt;/a> section of the &lt;a href="https://www.markdownguide.org/">Markdown Guide&lt;/a> website, we&amp;rsquo;ll find examples on how to correctly nest elements. Their suggestion is this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="k">*&lt;/span> This is the first bullet point.
This is a second paragraph within the first item.
&lt;span class="k">*&lt;/span> This is the second bullet point.
This is a code block within it.
&lt;span class="k">*&lt;/span> This is the third bullet point.
&lt;span class="k">
&lt;/span>&lt;span class="k"> &amp;gt; &lt;/span>&lt;span class="ge">This is a blockquote within it.
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>These work as intended, but to me, they look like a huge mess. If I were reviewing the above, I would have a hard time understanding if the nesting is correct, and to the untrained eye, this just looks wrong. I wouldn&amp;rsquo;t fault anyone coming up with a PR to &amp;ldquo;realign&amp;rdquo; the lines above so that they &amp;ldquo;look&amp;rdquo; correct in their textual form.&lt;/p>
&lt;p>My suggestion to fix the above is to always indent list item content to multiples of 4 columns, like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="k">*&lt;/span> This is the first bullet point.
This is a continuation paragraph, and it works.
&lt;span class="k">*&lt;/span> This is the second bullet point.
With a nested code block.
&lt;span class="k">*&lt;/span> This is the third bullet point.
&lt;span class="k">
&lt;/span>&lt;span class="k"> &amp;gt; &lt;/span>&lt;span class="ge">With a nested blockquote.
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Which works equally well for ordered lists &lt;em>and&lt;/em> lists with multiple levels:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="k">*&lt;/span> First bullet point.
&lt;span class="k">1.&lt;/span> Nested ordered list.
&lt;span class="k">1.&lt;/span> With multiple items.
And a code block.
&lt;span class="k">*&lt;/span> Second bullet point.
&lt;span class="k">
&lt;/span>&lt;span class="k"> &amp;gt; &lt;/span>&lt;span class="ge">And a blockquote.
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note how there are 3 spaces after the &lt;code>*&lt;/code> prefixes and 2 spaces after the &lt;code>1.&lt;/code> prefixes. I&amp;rsquo;ll agree that this doesn&amp;rsquo;t look super neat in textual form, but it is &lt;em>consistent&lt;/em>, easy to understand, and works all the time.&lt;/p>
&lt;h1 id="markdownlint-configuration-for-vscode">markdownlint configuration for VSCode&lt;/h1>
&lt;p>I strongly recommend everyone that authors Markdown content:&lt;/p>
&lt;ol>
&lt;li>to use Visual Studio Code;&lt;/li>
&lt;li>to install the &lt;a href="https://marketplace.visualstudio.com/items?itemName=DavidAnson.vscode-markdownlint">markdownlint extension&lt;/a>, and&lt;/li>
&lt;li>to address all squiggly lines that indicate Markdown formatting mistakes.&lt;/li>
&lt;/ol>
&lt;p>If you use these tools, you can apply the following configuration entries to enforce the style suggestions given in this post:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="s2">&amp;#34;markdownlint.config&amp;#34;&lt;/span>&lt;span class="err">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="nt">&amp;#34;MD007&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="c1">// Unordered list indentation.
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="nt">&amp;#34;indent&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="nt">&amp;#34;MD029&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="c1">// Ordered list item prefix.
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="nt">&amp;#34;style&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;one&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">},&lt;/span>
&lt;span class="nt">&amp;#34;MD030&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="c1">// Spaces after list markers.
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="nt">&amp;#34;ol_single&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;ol_multi&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;ul_single&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;ul_multi&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">3&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>I have yet to set up PR validation automation to run &lt;a href="https://github.com/DavidAnson/markdownlint">markdownlint&lt;/a> directly on the content and fail PR merges on mistakes&amp;hellip; but this is an idea I&amp;rsquo;ve just had as I was writing this ðŸ˜‰&lt;/p></description></item><item><title>Speeding up autoconf with caching</title><link>https://jmmv.dev/2022/06/autoconf-caching.html</link><pubDate>Fri, 17 Jun 2022 06:30:00 -0700</pubDate><guid>https://jmmv.dev/2022/06/autoconf-caching.html</guid><description>&lt;p>In the recent &lt;a href="/2022/05/remembering-buildtool.html">Remembering Buildtool&lt;/a> post, I described how setting up a cache of configuration checks was an important step in Buildtool&amp;rsquo;s installation process. The goal was to avoid pointless repetitive work on every build by performing such common checks once.&lt;/p>
&lt;p>&lt;a href="https://www.bsdnow.tv/457">Episode 457 of BSD Now&lt;/a> featured my post and Allan Jude wondered how much time would be saved in a bulk build of all FreeBSD packages if we could just do that same kind of caching with GNU Autoconf. And, you know what? It is indeed possible to do so. I had mentioned it en passing in my post but I guess I wasn&amp;rsquo;t clear enough, so let&amp;rsquo;s elaborate!&lt;/p>
&lt;h1 id="the-problem-autoconfs-slowness">The problem: Autoconf&amp;rsquo;s slowness&lt;/h1>
&lt;p>The &lt;code>configure&lt;/code> scripts generated by GNU Autoconf are slow, &lt;em>very&lt;/em> slow, to the point where sometimes their execution time is longer than the time it takes to build the package they configure. This is especially true on multi-core systems where these scripts make builds drag along.&lt;/p>
&lt;p>Here, take a look at some package build times on an 8-core machine from 2011:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Package&lt;/th>
&lt;th>Type&lt;/th>
&lt;th style="text-align:right">&lt;code>configure&lt;/code>&lt;/th>
&lt;th style="text-align:right">&lt;code>make -j8&lt;/code>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>bmake&lt;/td>
&lt;td>Small C package&lt;/td>
&lt;td style="text-align:right">8s&lt;/td>
&lt;td style="text-align:right">7s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>coreutils&lt;/td>
&lt;td>Medium C package&lt;/td>
&lt;td style="text-align:right">62s&lt;/td>
&lt;td style="text-align:right">96s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>m4&lt;/td>
&lt;td>Small C package&lt;/td>
&lt;td style="text-align:right">36s&lt;/td>
&lt;td style="text-align:right">9s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pkgconf&lt;/td>
&lt;td>Small C package&lt;/td>
&lt;td style="text-align:right">3s&lt;/td>
&lt;td style="text-align:right">2s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kyua&lt;/td>
&lt;td>Small C++ package&lt;/td>
&lt;td style="text-align:right">6s&lt;/td>
&lt;td style="text-align:right">91s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>tmux&lt;/td>
&lt;td>Small C package&lt;/td>
&lt;td style="text-align:right">7s&lt;/td>
&lt;td style="text-align:right">8s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For comparison, here are two of the builds above&amp;mdash;I did not have the patience to run them all&amp;mdash;on an even older single-core PowerBook G4 from 2005:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Package&lt;/th>
&lt;th>Type&lt;/th>
&lt;th style="text-align:right">&lt;code>configure&lt;/code>&lt;/th>
&lt;th style="text-align:right">&lt;code>make -j1&lt;/code>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>bmake&lt;/td>
&lt;td>Small C package&lt;/td>
&lt;td style="text-align:right">44s&lt;/td>
&lt;td style="text-align:right">60s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>tmux&lt;/td>
&lt;td>Small C package&lt;/td>
&lt;td style="text-align:right">46s&lt;/td>
&lt;td style="text-align:right">217s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Note the huge cost of the &lt;code>configure&lt;/code> run times &lt;em>relative to&lt;/em> &lt;code>make&lt;/code>.&lt;/p>
&lt;p>You might think that slow &lt;code>configure&lt;/code> scripts aren&amp;rsquo;t a big deal, but pause for a second to realize that these scripts plague the entire Unix ecosystem. Almost every package in your standard Linux distribution or BSD system has a &lt;code>configure&lt;/code> script of its own, and this script has to run before the package can be built. Considering that this ecosystem favors small source packages, each with its own &lt;code>configure&lt;/code> script, the costs add up quickly.&lt;/p>
&lt;p>But wait, it gets even worse. All BSD systems and some Linux distributions have some form of &lt;strong>bulk build&lt;/strong>: a long-running process where they rebuild their entire collection of binary packages at once from source. These binary packages are the ones you can later trivially install via, say, &lt;code>pkg&lt;/code> on FreeBSD or &lt;code>dnf&lt;/code> on Fedora. These bulk builds take several hours &lt;em>at best&lt;/em> on the most powerful machines and several &lt;em>weeks&lt;/em> (or is it months?) at worst on legacy platforms. I don&amp;rsquo;t have hard numbers, but based on the simple data presented above, I think it&amp;rsquo;s fair to assume that a large percentage of the total build time is wasted on &lt;code>configure&lt;/code> scripts&amp;mdash;and most of this time is stupidly spent doing the same work over and over and over again.&lt;/p>
&lt;p>Can we do anything to make these runs faster? Yes, it turns out we can. But before getting into that, lets explore why these scripts are so slow and why they are still a big problem even on modern multi-core machines.&lt;/p>
&lt;h1 id="why-are-configure-scripts-slow">Why are configure scripts slow?&lt;/h1>
&lt;p>The reasons &lt;code>configure&lt;/code> scripts are slow are varied:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>They are huge shell scripts. &lt;code>bmake&lt;/code>&amp;rsquo;s &lt;code>configure&lt;/code> to take just one example is about 210kb with a total of 7500 lines. The shell is a language that doesn&amp;rsquo;t win any speed tests.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>They are creating, compiling and running&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> small programs to verify features of the host system.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>They are sequential and mostly I/O-bound (again, they are shell scripts), which is the worst kind of sequential.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&amp;lsquo;nuf said.&lt;/p>
&lt;h1 id="parallel-builds-to-the-rescue">Parallel builds to the rescue?&lt;/h1>
&lt;p>&amp;ldquo;Ah, but it doesn&amp;rsquo;t matter&amp;rdquo;, you say. &amp;ldquo;While the &lt;code>configure&lt;/code> script of one package may be slow, we are building thousands of packages in a bulk build. Therefore, we can make use of parallelism to hide the costs!&amp;rdquo; Yeah&amp;hellip; not really.&lt;/p>
&lt;p>You see, the end-to-end build of a package tends to be bimodal: the &lt;code>configure&lt;/code> script is slow, I/O-bound, and sequential, while the build itself is typically reasonably parallel and CPU-bound. (Actually, the end-to-end process is trimodal if we account for the I/O-bound installation step, but let&amp;rsquo;s ignore that in this post.)&lt;/p>
&lt;p>These different kinds of resource consumption at different stages pose problems when trying to parallelize the build of independent packages.&lt;/p>
&lt;p>Suppose we have a machine with 8 CPUs and that every package&amp;rsquo;s build stage is parallel enough to consume up to 4 CPUs at any given time. If we try to build 8 of these packages in parallel to paper over the fact that &lt;code>configure&lt;/code> is sequential, we&amp;rsquo;ll have good cases where we are running the 8 scripts at once and making an OK use of resources. Unfortunately, we&amp;rsquo;ll also have bad cases where the 8 packages are in their build stage trying to use 4 CPUs each, which means we&amp;rsquo;ll have 32 CPU-hungry processes scheduling on 8 CPUs. The latter scenario is more likely than the former so this is not great.&lt;/p>
&lt;p>To &amp;ldquo;fix&amp;rdquo; this under bulk build scenarios, we could say that we don&amp;rsquo;t want to allow parallel builds within a package (i.e. we restrict each build to &lt;code>make -j1&lt;/code>) to keep every package limited to one CPU at most. But if we do that, we&amp;rsquo;ll introduce major choke points in the bulk build because some packages, like &lt;code>clang&lt;/code>, are depended on by almost everything and take forever to build without parallelism.&lt;/p>
&lt;h1 id="repeated-work">Repeated work&lt;/h1>
&lt;p>The worst part of all is that a lot of the work that &lt;code>configure&lt;/code> scripts do is pure waste. How many times do you &lt;em>really&lt;/em> need to check during the build of multiple packages that your system has a C compiler? And &lt;code>stdlib.h&lt;/code>? And &lt;code>uint8_t&lt;/code>? And a &lt;em>Fortran&lt;/em> compiler? FFS. Most of these checks are useless in most packages (&lt;code>configure&lt;/code> scripts are cargoculted and almost nobody understands them), and for those that are useful, their answers aren&amp;rsquo;t going to change for the duration of the build. Heck, the answers are likely not going to change for the lifetime of the entire system either.&lt;/p>
&lt;p>This is particularly frustrating when you want to revive an old machine&amp;mdash;like the PowerBook G4 I mentioned above&amp;mdash;where the only option to get modern software is to build it yourself. Doing so is exasperating because you spend most of your time witnessing &lt;code>configure&lt;/code> scripts doing repeated work and very little time building the code you want to run.&lt;/p>
&lt;p>All hope is not lost though. I&amp;rsquo;m sure you have occasionally noticed this: when you run &lt;code>configure&lt;/code> in a large project that has recursive &lt;code>configure&lt;/code> invocations, you&amp;rsquo;ll often see &lt;code>(cached)&lt;/code> next to individual checks. In other words, these scripts do know how to reuse results from previous invocations.&lt;/p>
&lt;p>So, wouldn&amp;rsquo;t it be great if we convinced &lt;code>configure&lt;/code> to avoid doing repeated work across packages? Couldn&amp;rsquo;t we check for these details just &lt;em>once&lt;/em> and reuse cached results later? Well, yes, yes we can!&lt;/p>
&lt;h1 id="say-hello-to-gnu-autoconf-caching">Say hello to GNU Autoconf caching&lt;/h1>
&lt;p>GNU Autoconf does have first-class caching features. Using them within a single package is trivial. All we have to do is pass the &lt;code>--config-cache&lt;/code> flag to the script as described in the &lt;a href="https://www.gnu.org/savannah-checkouts/gnu/autoconf/manual/autoconf-2.71/html_node/Cache-Files.html">Cache Files section of the manual&lt;/a> and it will maintain a &lt;code>config.cache&lt;/code> file with the results of the invocation. You can see the impact of a perfect cache here:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">.../m4-1.4.19$ time ./configure --config-cache &amp;gt;/dev/null
./configure &amp;gt; /dev/null 16.32s user 12.18s system 90% cpu 31.660 total
.../m4-1.4.19$ time ./configure --config-cache &amp;gt;/dev/null
./configure --config-cache &amp;gt; /dev/null 1.45s user 0.77s system 108% cpu 2.045 total
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In other words, &lt;code>configure&lt;/code>&amp;rsquo;s time went from 31 seconds to just 2 by saving and reusing the previous results. (Note that this is &lt;em>different&lt;/em> than running &lt;code>config.status&lt;/code>, which just recreates output files, but let&amp;rsquo;s leave that aside.)&lt;/p>
&lt;p>This is nice for a single package, but as it turns out, the &lt;code>--config-cache&lt;/code> flag takes an optional parameter to specify the path to the cache file. There is nothing preventing us from passing a path to a central location and reusing the same cache for various packages!&lt;/p>
&lt;p>In fact, the GNU Autoconf developers have thought about this problem. On the one hand, the tool supports setting up a system-wide configuration file (known as &lt;code>config.site&lt;/code>) as described in the &lt;a href="https://www.gnu.org/savannah-checkouts/gnu/autoconf/manual/autoconf-2.71/html_node/Site-Defaults.html">Setting Site Defaults section of the manual&lt;/a>. And on the other hand, the default code snippet that they show in the manual has an explicit mention of using a system-wide cache:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text"># Give Autoconf 2.x generated configure scripts a shared default
# cache file for feature test results, architecture-specific.
if test &amp;#34;$cache_file&amp;#34; = /dev/null; then
cache_file=&amp;#34;$prefix/var/config.cache&amp;#34;
# A cache file is only valid for one C compiler.
CC=gcc
fi
&lt;/code>&lt;/pre>&lt;/div>&lt;p>It wouldn&amp;rsquo;t be easier, really, to cache results, right? But then&amp;hellip; why aren&amp;rsquo;t we collectively using this feature more widely? Well, caching &lt;code>configure&lt;/code> results willy-nilly can cause random build failures because the checks performed by one package aren&amp;rsquo;t necessarily equivalent to similar-looking checks in another. An obvious case is when the results of a check depend on the results of a previous check: for cache correctness, any two scripts need to run these two checks in the same order, but there is no guarantee that they&amp;rsquo;ll do so.&lt;/p>
&lt;p>If we want to have system-wide caching of reasonable safety, we need to do better than simply pointing all &lt;code>configure&lt;/code> runs to a central cache file. And this is where &lt;code>autoswc&lt;/code> enters the picture.&lt;/p>
&lt;h1 id="enter-autoswc">Enter autoswc&lt;/h1>
&lt;p>&lt;a href="https://pkgsrc.se/pkgtools/autoswc">&lt;code>autoswc&lt;/code>&lt;/a>, whose name stands for &lt;em>Automatic System-Wide Configuration&lt;/em> and was brought to you by yours truly in 2004, is a little tool that exposes GNU Autoconf system-wide caching facilities in a safe manner.&lt;/p>
&lt;p>The key idea behind &lt;code>autoswc&lt;/code> is that you (the administrator) create a system-wide &lt;code>configure&lt;/code> script with the list of checks you want to cache and then run &lt;code>autoswc&lt;/code> to refresh the cache at specific points in time (say before performing a bulk build). Then, any build you perform from within &lt;code>pkgsrc&lt;/code> (the tool is specific to this packaging system), will reuse those checks&amp;mdash;but these arbitrary builds won&amp;rsquo;t contaminate the central cache.&lt;/p>
&lt;p>Put it another way: &lt;code>autoswc&lt;/code> helps define a cache of safe checks and automates the process of using those during bulk builds minimizing the risks of bad things happening due to cache contamination.&lt;/p>
&lt;p>Using this tool is easy. I had not used it in &lt;em>years&lt;/em>, but installing it from pkgsrc and setting it up only required these steps. Just like with Buildtool, I&amp;rsquo;m surprised such old code of mine still works:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Install the &lt;code>pkgtools/autoswc&lt;/code> package.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Optionally copy &lt;code>/usr/pkg/share/autoswc/configure.ac&lt;/code> to &lt;code>/usr/pkg/etc/autoswc/configure.ac&lt;/code> and extend the sample script with the checks you want to cache.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Append &lt;code>.sinclude &amp;quot;/usr/pkg/share/autoswc/autoswc.mk&amp;quot;&lt;/code> to &lt;code>/etc/mk.conf&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Occasionally run &lt;code>autoswc&lt;/code> from the command line to update the cache.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Voila. All package builds done through pkgsrc now benefit from the cached configuration results generated by the files in step 2.&lt;/p>
&lt;p>Unfortunately, as good as this may seem, &lt;code>autoswc&lt;/code>&amp;rsquo;s results aren&amp;rsquo;t impressive. The main problem is that it&amp;rsquo;s on you (the administrator) to curate the list of checks to cache. This is a very difficult task as it requires looking at what &lt;code>configure&lt;/code> scripts are doing throughout a bulk build and determining which checks are safe to cache and which aren&amp;rsquo;t, and ain&amp;rsquo;t nobody have time for that.&lt;/p>
&lt;p>I think my hope when I created this tool was that we&amp;rsquo;d get a swarm of people with pkgsrc expertise to curate the predefined list of checks in the sample &lt;code>configure.ac&lt;/code> file and then we&amp;rsquo;d all benefit from the results on our own machines and on the bulk build clusters&amp;hellip; but this obviously did not happen. But the feature in GNU Autoconf exists, &lt;code>autoswc&lt;/code> is still functional and trivially configurable, and with some effort it could potentially bring some tangible speed improvements to builds&amp;mdash;especially on old hardware.&lt;/p>
&lt;p>Anyhow, now you know about one more &amp;ldquo;hidden&amp;rdquo; feature that GNU Autoconf has and that can potentially speed things up in repeated package builds massively.&lt;/p>
&lt;p>Thanks for reading and enjoy the weekend!&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Avid readers will note that another consequence of &lt;em>running&lt;/em> the test programs that &lt;code>configure&lt;/code> creates is that &lt;code>configure&lt;/code> scripts are often terrible when trying to cross-build software for other platforms. The test programs must be built for the target system in order to provide correct results, but that means that they cannot be run on the host.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>EndBASIC 0.9: Run shared demos easily</title><link>https://jmmv.dev/2022/06/endbasic-0.9.html</link><pubDate>Sun, 05 Jun 2022 17:20:00 -0700</pubDate><guid>https://jmmv.dev/2022/06/endbasic-0.9.html</guid><description>&lt;p>It is with great pleasure that I announce the release of EndBASIC 0.9.0 ðŸ˜Žï¸.&lt;/p>
&lt;p>The major feature in this new release is the ability to &lt;strong>launch publicly-shared files via a click of a URL&lt;/strong> without having to create an account first. Here, try running my &lt;a href="https://repl.endbasic.dev/?run=jmmv/bounce.bas">&lt;code>jmmv/bounce.bas&lt;/code>&lt;/a> or &lt;a href="https://repl.endbasic.dev/?run=jmmv/paint.bas">&lt;code>jmmv/paint.bas&lt;/code>&lt;/a> demos in your browser, now!&lt;/p>
&lt;p>This may seem like a small feature not worthy of a new release, but it is a game-changer for two reasons. First, the backend has undergone massive changes to support this, which in turn have allowed me to fulfill my original vision for the cloud service. And, second, this lets passersby trivially play with anything you create. The barrier to entry is now much lower for new developers as well as observers.&lt;/p>
&lt;p>This release includes other changes as well, primarily around fixing UTF8-related crashes in the command line and the editor. These are special and worth calling out because they are the first substantial code contributions from a user, &lt;a href="https://github.com/zenria">@zenria&lt;/a>. Thanks!&lt;/p>
&lt;p>Finally, &lt;strong>the website now includes a &lt;a href="https://www.endbasic.dev/docs.html">User&amp;rsquo;s manual&lt;/a>&lt;/strong>. This manual is intended to provide an overview of the features in EndBASIC and how they all tie together, and I hope can help get an overview of what&amp;rsquo;s supported before even diving into the language. Note that this is not meant as a replacement for the built-in reference documentation, whose primary source is still within the interpreter.&lt;/p>
&lt;p>Here are the links you need to get started:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://repl.endbasic.dev/">Launch the online interpreter&lt;/a>&lt;/strong>&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://www.endbasic.dev/">Visit the EndBASIC website&lt;/a>&lt;/strong>&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://github.com/endbasic/endbasic/releases/tag/endbasic-0.9.0">Read the release notes&lt;/a>&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Please &lt;a href="mailto:support@endbasic.dev">let me know&lt;/a> if you encounter any issues.&lt;/p>
&lt;p>Enjoy and keep reading if you want to know more about what has happened behind the scenes, as this release has taken about 2 months of early morning and weekend work.&lt;/p>
&lt;h1 id="refactoring-authentication">Refactoring authentication&lt;/h1>
&lt;p>Modifying the EndBASIC cloud service to support unauthenticated access to publicly shared files has been&amp;hellip; very difficult.&lt;/p>
&lt;p>To set some context, the cloud service grew as an experiment of mine to learn more about Azure. As such, I ended up using Azure Active Directory (AAD) for authentication and Azure Functions as the runtime for the service. This combination works nicely if you choose to enable authentication in the Azure Functions configuration: the runtime will handle integration with AAD for you and only allow authenticated calls to go through.&lt;/p>
&lt;p>Unfortunately, setting up authentication on an Azure Functions instance is an all-or-nothing option. Either you protect the whole service with authentication or you don&amp;rsquo;t. It&amp;rsquo;s not possible to serve some authenticated and some unauthenticated endpoints at once from the same deployment. Well, it &lt;em>is&lt;/em> possible if you roll your own bearer token validation, but that requires a significant amount of code. I suppose that the Azure SDKs for officially-supported languages such as C# let you do this with ease, but the Rust SDK is&amp;hellip; well&amp;hellip; lacking.&lt;/p>
&lt;p>I had various options here:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>I could have come up with a humongous hack by deployed two separate Azure Functions instances: one with authentication as before, and one without authentication to support this new use case. This would have required adding a simple knob in the code to configure the latter deployment to only allow access to public content. But&amp;hellip; this is how operational complexity is made when launch schedules dictate cutting corners.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I could have implemented my own bearer token validation. In fact, I did research this quite a bit and had it almost working, but I stopped before implementing signature validation because this required fetching and caching service-specific public keys with certain refresh periods&amp;hellip; and I was lazy to code it all.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I could have dropped AAD altogether and implemented my own account management and authentication mechanism.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>And option 3 is, of course, the one I chose.&lt;/p>
&lt;p>Now, before you call me reckless, there is a good reason I went for option 3 and implemented my own authentication flow. The reason was to avoid OAuth&amp;mdash;there are &lt;a href="https://www.ory.sh/oauth2-openid-connect-do-you-need-use-cases-examples/">good reasons to do so&lt;/a>!&amp;mdash;which had prevented me from implementing the signup flow I wanted to have from the very beginning.&lt;/p>
&lt;h1 id="account-creation">Account creation&lt;/h1>
&lt;p>EndBASIC 0.9 implements the original vision I had for account creation, which is made possible by the cloud service &lt;em>not&lt;/em> using OAuth any longer.&lt;/p>
&lt;p>Before this release, you had to visit the EndBASIC web site to start an account creation flow in AAD, go through that bland process, and then come back to the interpreter to log in. What I really wanted from the very beginning was to have a &lt;code>SIGNUP&lt;/code> command within the interpreter that acted like &lt;code>adduser&lt;/code> on FreeBSD, and this is now what we have:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">Ready
SIGNUP
Let&amp;#39;s gather some information to create your cloud account.
You can abort this process at any time by hitting Ctrl+C and you will
be given a chance to review your inputs before creating the account.
Username: demo
Password: *********
Retype password: *********
[...]
Continue (y/N)? y
Your account has been created and is pending activation.
Check your email now and look for a message from the EndBASIC Service.
Follow the instructions in it to activate your account. Make sure to
check your spam folder.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The client aspects of this were trivial to implement, but the service side wasn&amp;rsquo;t. To make this work, I had to implement the account signup flow with email validation, which means I had to generate and track account activation codes, deal with secure password hashing, and integrate with SendGrid to send service-related emails. Not inherently difficult code to be honest, but getting the tests updated was painful.&lt;/p>
&lt;h1 id="lost-accounts">Lost accounts&lt;/h1>
&lt;p>Everything sounds great but there are some bad news, unfortunately. Rolling my own account management means that I had to migrate user accounts from AAD to my own accounts system. Which was doable&amp;hellip; except for passwords. For obvious reasons, AAD doesn&amp;rsquo;t store raw passwords, which means it&amp;rsquo;s just impossible to fetch them and put them in a different database. But I could have migrated the accounts by hand and emailed all account owners to reset their password, right?&lt;/p>
&lt;p>Well&amp;hellip; you see, while I had configured the AAD signup flow to perform email validation and I had code in the service to save the email claim in the OAuth token in the shadow accounts table&amp;hellip; I had misconfigured the OAuth login flow by forgetting to enable email claim propagation. This means that my service never saw the email addresses and they were never stored in AAD. Simply put, I have no way of contacting previous account owners.&lt;/p>
&lt;p>Considering that there were only a few accounts and that they are mostly inactive, wiping everything didn&amp;rsquo;t seem like a big deal and is what I did. Apologies for the hassle.&lt;/p>
&lt;p>Fear not, though: I have a backup of all previous data so if you want me to recover anything for you, just let me know.&lt;/p>
&lt;h1 id="whats-next">What&amp;rsquo;s next?&lt;/h1>
&lt;p>I had bigger plans for this release. On top of public file sharing, I planned to finally add support for user-defined functions, &lt;code>GOTO&lt;/code> and &lt;code>GOSUB&lt;/code>, file system directories, and maybe even automatic numeric type promotion. But because the changes to the cloud service turned out to be so intrusive and required a database wipeout, I preferred to cut scope and get this launched ASAP to get the new code exercised and avoid losing even more data.&lt;/p>
&lt;p>The aforementioned features should come in 0.10 though, so stay tuned for a much better language! A great way to do this is to &lt;a href="https://repl.endbasic.dev/">launch the interpreter&lt;/a>, create an account with &lt;code>SIGNUP&lt;/code>, and opt into receiving promotional (release announcements) emails! Or you can subscribe to this blog with the buttons below.&lt;/p>
&lt;p>Thanks for reading, and have fun ðŸ˜ï¸.&lt;/p></description></item><item><title>Remembering Buildtool</title><link>https://jmmv.dev/2022/05/remembering-buildtool.html</link><pubDate>Fri, 13 May 2022 09:50:00 -0700</pubDate><guid>https://jmmv.dev/2022/05/remembering-buildtool.html</guid><description>&lt;p>Build systems are one of my favorite topics in software engineering. If I recall correctly, my interest in this area started when I got into NetBSD in 2002&amp;mdash;20 years ago&amp;mdash;and became a pkgsrc contributor. Packaging software for NetBSD made me &lt;a href="/2005/03/making-packager-friendly-software-1.html">fight various build systems&lt;/a> and, in particular, experience the pains of debugging the GNU Autotools.&lt;/p>
&lt;p>Around that same time, I was also writing &lt;a href="/software/menu2wm.html">small&lt;/a> &lt;a href="/software/vcsme.html">tools&lt;/a> &lt;a href="/software/xmlcatmgr.html">here&lt;/a> &lt;a href="/software/sysbuild.html">and&lt;/a> &lt;a href="/software/pkg_comp.html">there&lt;/a>. Out of inertia, I used the GNU Autotools for these and, the more I used them, the more I saw an opportunity for improvement. The GNU Autotools were slow, hard to deal with, and they bloated every package. Why did you have to ship heavy &lt;code>configure&lt;/code>, &lt;code>Makefile.in&lt;/code> and &lt;code>libtool&lt;/code> scripts in every single distribution file when you could instead rely on a few system-wide scripts? And thus &lt;a href="/software/buildtool.html">Buildtool&lt;/a> was born in the summer of 2002, just before I started college, and I worked on it for about two years.&lt;/p>
&lt;p>The Buildtool project recently came to mind and I noticed that &lt;a href="http://buildtool.sourceforge.net/">its website is still up and running&lt;/a> (kudos to SourceForge for that), so I poked around a bit. Just by looking at the &lt;a href="http://buildtool.sourceforge.net/docs/manual/index.html">User&amp;rsquo;s Manual&lt;/a>, I&amp;rsquo;m amazed by how comprehensive the tool is and makes me jealous of how much free time I had back then. Since noticing this, I had been meaning to try the tool again and write a post, and finally got to it just yesterday. So let&amp;rsquo;s take a tour of what Buildtool was and what it achieved.&lt;/p>
&lt;h1 id="history">History&lt;/h1>
&lt;p>Buildtool was originally inspired by the FreeBSD and NetBSD build systems. The idea was to leverage system-wide generic build logic files to build libraries and binaries&amp;mdash;just like &lt;code>bsd.lib.mk&lt;/code> and &lt;code>bsd.prog.mk&lt;/code> provide&amp;mdash;and have arbitrary packages rely on those installed files. As an end user, you would have to install Buildtool first before you could build any other package, but you would only pay the cost of the build infrastructure once. While this paradigm is accepted today, it was quite a departure from the traditions of Unix systems in the early 2000s.&lt;/p>
&lt;p>Buildtool&amp;rsquo;s first version in 2002 was precisely what the NetBSD build system was. The 0.1 release shipped with a copy of NetBSD&amp;rsquo;s &lt;code>make&lt;/code> tool, renamed as &lt;code>bt_make&lt;/code>, along with a few &lt;code>mk&lt;/code> files to build common targets. That release also included a rudimentary GNU Autoconf-like tool.&lt;/p>
&lt;p>Later on, Buildtool grew a &lt;code>bt_wrap&lt;/code> helper utility to deal with platform incompatibilities when invoking common tools such as the compiler. This idea was inspired by pkgsrc&amp;rsquo;s buildlink3 and wrappers infrastructure, which to this day still wrap the compiler and linker to paper over platform-specific oddities. Reading through the release notes, I can see how this helped make Buildtool work on Cygwin and Mac OS X systems of the day.&lt;/p>
&lt;p>Towards the latest releases of the project by 2004, things took a significant turn: &lt;code>bt_make&lt;/code> and &lt;code>bt_wrap&lt;/code> were removed in favor of &lt;code>bt_logic&lt;/code>, a custom-made build system purely based on shell scripts. If I recall correctly, the first version of &lt;code>bt_logic&lt;/code> was written in Perl, but it never shipped because having Buildtool rely on the gigantic Perl dependency was a non-starter.&lt;/p>
&lt;p>By the beginning of 2005, I cancelled the project for the reasons I&amp;rsquo;ll cover at the end of this post.&lt;/p>
&lt;h1 id="installing-buildtool">Installing Buildtool&lt;/h1>
&lt;p>The last release of Buildtool was 0.16, which was published on July 4th, 2004 (about 18 years ago) and is still available in the &lt;a href="http://buildtool.sourceforge.net/download.html">Downloads&lt;/a> page if you want to try it out.&lt;/p>
&lt;p>Buildtool itself was written in about 10,000 lines shell, but the distribution includes a slimmed down version of NetBSD&amp;rsquo;s &lt;code>/bin/sh&lt;/code> named &lt;code>bt_sh&lt;/code>. The raison d&amp;rsquo;Ãªtre for &lt;code>bt_sh&lt;/code> was pretty much the same as Debian&amp;rsquo;s &lt;a href="https://manpages.debian.org/buster/dash/dash.1.en.html">&lt;code>dash&lt;/code>&lt;/a>: a portable, high-performance and standards-compliant shell interpreter to run scripts. &lt;code>bt_sh&lt;/code> helped keep Buildtool&amp;rsquo;s implementation simpler as it did not have to deal with &lt;code>sh&lt;/code> platform incompatibilities (Solaris' version was a pain). Furthermore, &lt;code>bt_sh&lt;/code> also ensured that the users of Buildtool writing their own build scripts wouldn&amp;rsquo;t have to &lt;a href="https://www.gnu.org/savannah-checkouts/gnu/autoconf/manual/autoconf-2.71/html_node/Portable-Shell.html">care about shell portability&lt;/a>, as is the case when writing &lt;code>configure.ac&lt;/code> scripts.&lt;/p>
&lt;p>The reason I&amp;rsquo;m mentioning &lt;code>bt_sh&lt;/code> is because trying to build Buildtool 0.16 on a FreeBSD 13 system today fails due to a couple of bugs in the C code. Fixing those bugs is a matter of avoiding trivial conflicts among repeated symbols in different modules. After those simple fixes, Buildtool installs successfully:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/buildtool-0.16$ ./configure -p /tmp/local
configuring for buildtool-0.16
checking for machine type: amd64
checking for program gcc: not found
checking for program cc: /usr/bin/cc
checking whether /usr/bin/cc is GNU C: yes
[...]
===========================================================================
BUILDTOOL 0.16 CONFIGURATION SUMMARY
Installation prefix: /tmp/local
Configuration directory: /tmp/local/etc/buildtool
Type `make&amp;#39; to start the build.
Type `make install&amp;#39; to install, only _after_ the build has finished.
===========================================================================
/tmp/buildtool-0.16$ make &amp;amp;&amp;amp; make install
[...]
===========================================================================
Buildtool 0.16 has been successfully installed!
Please take some time to read the `Testing&amp;#39; section in the README file
to easily provide useful feedback. This will not take more than ten
minutes, but you will be contributing to the project.
As part of the post installation stage, you should now create a system
configuration file for the bt_config module, containing cached results
for several common checks run by many configure scripts.
For an automated setup, issue the following commands:
mkdir -p /tmp/local/etc/buildtool
cp /tmp/local/share/buildtool/templates/bt_config.conf.in \
/tmp/local/etc/buildtool/bt_config.conf.in
/tmp/local/bin/buildtool swcgen
Thanks for choosing Buildtool.
===========================================================================
&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="post-installation-configuration">Post-installation configuration&lt;/h1>
&lt;p>The installation process quoted above tells us that we should create a system-wide configuration file and then run &lt;code>buildtool swcgen&lt;/code>, which stands for &amp;ldquo;System-wide configuration generator&amp;rdquo;. Let&amp;rsquo;s poke at that file and try to follow the given steps.&lt;/p>
&lt;p>Here is what &lt;code>/tmp/local/share/buildtool/templates/bt_config.conf.in&lt;/code> has to say:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># $Id: bt_config.conf.in,v 1.8 2004/06/26 18:42:05 jmmv Exp $&lt;/span>
&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># bt_config.conf - System wide configuration&lt;/span>
&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># File automatically generated by @BT_PKG_NAME@, version @BT_PKG_VERSION@.&lt;/span>
&lt;span class="c1"># Timestamp: @TIMESTAMP@&lt;/span>
&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># bt_config will open this file from @BT_DIR_ETC@.&lt;/span>
&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># C/C++ environments (leave both for now, all other macros rely on them).&lt;/span>
bt_cache_env c cxx
&lt;span class="c1"># Basic programs (C/C++ utilities handled by environments).&lt;/span>
bt_cache_prog info lex m4 sh yacc
&lt;span class="c1"># Header files&lt;/span>
bt_cache_hdr sys/cdefs.h sys/utsname.h err.h poll.h stdarg.h stdbool.h ulimit.h
&lt;span class="c1"># C++ only header files&lt;/span>
bt_cache_hdr bitset deque fstream iostream list map queue &lt;span class="nb">set&lt;/span> stack string &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> vector
&lt;span class="c1"># Basic libraries&lt;/span>
bt_cache_lib m
&lt;span class="c1"># System specific functions&lt;/span>
bt_cache_func setenv strerror stricmp strlcat strlcpy strncat strncpy strndup &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> strftime poll readdir_r &lt;span class="nb">ulimit&lt;/span> uname vfork vsnprintf
&lt;span class="c1"># Types&lt;/span>
bt_cache_type gid_t int8_t int16_t int32_t int64_t size_t uid_t uint8_t &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> uint16_t uint32_t uint64_t u_int8_t u_int16_t u_int32_t u_int64_t
&lt;span class="c1"># Type sizes&lt;/span>
bt_cache_sizeof char short int long &lt;span class="s2">&amp;#34;long long&amp;#34;&lt;/span>
&lt;span class="c1"># Miscellaneous results&lt;/span>
bt_cache_attribute
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Interesting. This configuration file is all about &amp;ldquo;caching&amp;rdquo;. But caching what?&lt;/p>
&lt;p>You see, one of the problems I had with the GNU Autotools is how every single package I built and installed on my machine had to go through a very costly &lt;code>configure&lt;/code> invocation. It was mind-blowing to me (and still is &lt;em>today&lt;/em>) how many CPU hours the world burns on a day-to-day basis checking if a system has standard headers and functions. Couldn&amp;rsquo;t we check just &lt;em>once&lt;/em> and have all packages reuse the results? If the system has &lt;code>vfork&lt;/code>, for example, it will continue to have that function for the foreseeable future; there is no need to test for it over and over again.&lt;/p>
&lt;p>That&amp;rsquo;s precisely what the &lt;code>buildtool swcgen&lt;/code> command addresses: it executes all of the checks specified in the configuration file and installs those cached results for later reuse. Let&amp;rsquo;s see what the invocation looks like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">$ cp /tmp/local/share/buildtool/templates/bt_config.conf.in \
/tmp/local/etc/buildtool/bt_config.conf.in
$ /tmp/local/bin/buildtool swcgen
Input: /tmp/local/etc/buildtool/bt_config.conf.in
Output: /tmp/local/etc/buildtool/bt_config.conf
bt_swcgen: running bt_wizard to create temporary project skeleton
Entering directory /tmp/bt_swcgen.1802... done.
Creating directories... src
Creating Generic.bt... done.
Creating README.bt... done.
Creating src/Logic.bt... done.
bt_swcgen: generating configuration script and cache
bt_config: starting configuration for bt_swcgen-0.16
checking for program gcc... not found.
checking for program cc... /usr/bin/cc
checking for C compiler name... gnu
checking for C compiler version... 13.0.0
checking for program cpp... /usr/bin/cpp
checking for program ld... /usr/bin/ld
[...]
checking for program sh... /bin/sh
checking for program yacc... /usr/bin/yacc
checking for c header sys/cdefs.h... yes
checking for cxx header sys/cdefs.h... yes
checking for c header sys/utsname.h... yes
checking for cxx header sys/utsname.h... yes
checking for c header err.h... yes
checking for cxx header err.h... yes
[...]
checking for c function vfork... yes
checking for cxx function vfork... no
checking for c function vsnprintf... yes
checking for cxx function vsnprintf... no
checking for c type gid_t... yes
checking for cxx type gid_t... yes
checking for c type int8_t... yes
checking for cxx type int8_t... yes
[...]
bt_config: creating bt_output
bt_config: generating configuration environment
bt_config: generating package dependent build logic
===========================================================================
Configuration summary for bt_swcgen-0.16:
Prefix is: /usr/local
Developer mode: yes
Install documentation: yes
Static libraries: no, Shared libraries: yes, rpath: yes
===========================================================================
bt_swcgen: creating system wide configuration file
bt_output: creating /tmp/bt_swcgen.1802/conf
bt_swcgen: /tmp/local/etc/buildtool/bt_config.conf created
===========================================================================
PLEASE NOTE THE FOLLOWING:
Installed: /tmp/local/etc/buildtool/bt_config.conf
By using a system wide configuration file for bt_config that stores
check results, you assume that they may get obsoleted with respect to
your system, specially after software updates. Be careful to only
store results that are unlikely to change with time. Anyway, you are
encouraged to re-run this program periodically to regenerate the file
with new results.
If a third party program fails to configure after a check that shows
the `(cached)&amp;#39; string in it, try to pass the `--ignore-sw-config&amp;#39;
flag to bt_config before thinking it is a bug in your system or in
the package. DO NOT DISTURB SOFTWARE AUTHORS BEFORE DOUBLE CHECKING
THAT THERE IS A PROBLEM IN THEIR SOFTWARE. YOU HAVE BEEN WARNED.
===========================================================================
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Pretty impressive that this 18-year old code still works today, and running this shows precisely the problem that this was supposed to solve: computing these results took about 15 seconds on my modest server, and those are 15 seconds you would pay to build most software packages. When you are bulk-building all binary packages for a source-bootstrapped system (the common case in the BSD world), those quickly add up.&lt;/p>
&lt;p>By the way: Autoconf supports this same feature although I did not discover it until later. &lt;a href="https://pkgsrc.se/pkgtools/autoswc">&lt;code>autoswc&lt;/code>&lt;/a> is a follow-up tool I created for pkgsrc to implement this same idea. Using autoswc proved to be a nice speedup for daily operations when I was still an active pkgsrc maintainer. I suspect that most people are still unaware of this Autoconf feature, unfortunately.&lt;/p>
&lt;h1 id="commands-summary">Commands summary&lt;/h1>
&lt;p>Now that Buildtool is installed and configured, let&amp;rsquo;s poke around to see what it has to offer:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">$ /tmp/local/bin/buildtool --help
buildtool version 0.16
usage: buildtool [options] target [target_options]
Copyright (c) 2002, 2003, 2004 Julio M. Merino Vidal.
This program is licensed under the terms of the BSD license.
Available options:
{-h,--help} Show detailed usage (this text).
{-v,--version} Show version number.
{-w,--warnings} Enable bt_sh warning messages.
Available targets:
build Build the package.
clean Soft clean the package (keeps configuration data).
cleandir Hard clean the package.
config Automatically configure a package.
dist Generate a distribution file.
deinstall Deinstalls the package.
doc Read build-time package documentation.
install Installs the package.
lint Validate the package according to standards.
pkgflags Show compilation flags for an installed package.
siteinfo Get site specific configuration details.
swcgen Aid with creation of system-wide configuration files.
test Run tests specific to the package after a successful build.
wizard Use an interactive wizard to create initial project files.
See buildtool(1) for more information.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Pretty standard output for a subcommand-based utility, but I can already see myself here. &lt;a href="/2013/08/cli-design-series-introduction.html">My style&lt;/a> in developing command-line tools hasn&amp;rsquo;t changed much since then.&lt;/p>
&lt;p>Anyhow. From the output above, we can distill the following command groups:&lt;/p>
&lt;ul>
&lt;li>&lt;code>config&lt;/code>: A configuration command to let each package discover system-wide features, obviously inspired by GNU Autoconf.&lt;/li>
&lt;li>&lt;code>build&lt;/code>, &lt;code>clean&lt;/code>, &lt;code>cleandir&lt;/code>, &lt;code>dist&lt;/code>, &lt;code>deinstall&lt;/code>, &lt;code>install&lt;/code>, &lt;code>test&lt;/code>: Build-related commands, some of which (like &lt;code>dist&lt;/code>) are inspired by GNU Automake.&lt;/li>
&lt;li>&lt;code>doc&lt;/code>: An interactive documentation viewer for a package&amp;rsquo;s &lt;code>README&lt;/code>, &lt;code>INSTALL&lt;/code>, etc. distribution documents. Interesting.&lt;/li>
&lt;li>&lt;code>pkgflags&lt;/code>: A replacement for the heavy-weight &lt;code>pkg-config&lt;/code>. These days we have &lt;a href="http://pkgconf.org/">pkgconf&lt;/a> instead, which is compatible with &lt;code>pkg-config&lt;/code>.&lt;/li>
&lt;li>&lt;code>wizard&lt;/code>: An interactive tool to create a new package, akin to what &lt;code>cargo init&lt;/code> would offer today.&lt;/li>
&lt;/ul>
&lt;h1 id="demo">Demo&lt;/h1>
&lt;p>Alright. Let&amp;rsquo;s now use Buildtool&amp;rsquo;s package creation wizard to initialize a new demo project. The wizard is interactive so I&amp;rsquo;m just going to post the completed output of what I typed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">$ mkdir /tmp/demo
$ cd /tmp/demo
$ /tmp/local/bin/buildtool wizard
Welcome to Buildtool&amp;#39;s Wizard
-----------------------------
This module will help you to set up a basic directory structure
for your project, based on your choices.
Each question has associated a default answer, shown inside the
brackets. If you hit [RETURN] leaving a question in blank, it
will take the default value.
Project definitions:
- Unix name [foobar]? demo
- Initial version [0.1]?
- License [bsd]?
- Maintainer&amp;#39;s email [foo@bar.net]? julio@meroh.net
- Homepage (if any) []?
- Short comment [Sample package]? Remembering Buildtool
Code details:
- Will this package provide one or more programs [y]?
- Will this package provide one or more libraries [n]? y
- Will you use the C language [y]?
- Will you use the C++ language [n]?
- Will you use CVS [y]? n
Dependancies:
- Do you need pkgconfig (not bt_pkgflags) [n]?
- Do you need threads [n]?
- Do you need an X Window System [n]?
- Do you need awk [n]?
- Do you need a lexical analyzer [n]?
- Do you need a LARL parser generator [n]?
Begin process:
- Where should files be created [.]?
Creating directories... src data lib
Creating Generic.bt... done.
Creating README.bt... done.
Creating src/Logic.bt... done.
Creating lib/Logic.bt... done.
Creating data/demo.bpf.in... done.
Creating data/Logic.bt... done.
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This asked many more questions than I expected. Looks like I implemented support for many different things.&lt;/p>
&lt;p>After completing the wizard, we are left with these files:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ find . -type f | sort
./Generic.bt
./README.bt
./data/Logic.bt
./data/demo.bpf.in
./lib/Logic.bt
./src/Logic.bt
&lt;/code>&lt;/pre>&lt;/div>&lt;p>From the looks of it, we have a top-level configuration file called &lt;code>Generic.bt&lt;/code>, a mysterious &lt;code>data&lt;/code> directory with a &lt;code>bpf&lt;/code> file, a &lt;code>lib&lt;/code> directory to contain the library we&amp;rsquo;ll write, and a &lt;code>src&lt;/code> directory to contain the code of the program we&amp;rsquo;ll write.&lt;/p>
&lt;h2 id="the-top-level-genericbt-file">The top-level Generic.bt file&lt;/h2>
&lt;p>First, let&amp;rsquo;s look at the top-level &lt;code>Generic.bt&lt;/code> file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># Buildtool Generic Script&lt;/span>
&lt;span class="c1">#&lt;/span>
defs&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nv">BT_REQUIRE&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;0.16&amp;#34;&lt;/span>
&lt;span class="nv">BT_PKG_NAME&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;demo&amp;#34;&lt;/span>
&lt;span class="nv">BT_PKG_VERSION&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;0.1&amp;#34;&lt;/span>
&lt;span class="nv">BT_PKG_LICENSE&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;bsd&amp;#34;&lt;/span>
&lt;span class="nv">BT_PKG_MAINTAINER&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;julio@meroh.net&amp;#34;&lt;/span>
&lt;span class="nv">BT_PKG_COMMENT&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Remembering Buildtool&amp;#34;&lt;/span>
&lt;span class="o">}&lt;/span>
config_init&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="o">}&lt;/span>
config&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
bt_check_env_c
bt_generate_output data/demo.bpf
bt_generate_configh
&lt;span class="o">}&lt;/span>
docs&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
bt_doc CHANGES &lt;span class="s2">&amp;#34;Major changes between package versions&amp;#34;&lt;/span>
bt_doc PEOPLE &lt;span class="s2">&amp;#34;Authors and contributors&amp;#34;&lt;/span>
bt_doc README &lt;span class="s2">&amp;#34;General documentation&amp;#34;&lt;/span>
bt_doc TODO &lt;span class="s2">&amp;#34;Missing things&amp;#34;&lt;/span>
&lt;span class="o">}&lt;/span>
logic&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
bt_target lib data src
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>Generic.bt&lt;/code> is the package-wide file where it all begins. This file is composed of a collection of user-defined functions that tell Buildtool what to do:&lt;/p>
&lt;ul>
&lt;li>&lt;code>defs&lt;/code>: Describes the package&amp;rsquo;s metadata, much like what &lt;code>Cargo.toml&lt;/code>, &lt;code>go.mod&lt;/code> or &lt;code>package.json&lt;/code> do today in popular ecosystems.&lt;/li>
&lt;li>&lt;code>config_init&lt;/code> and &lt;code>config&lt;/code>: Allow the package to request system feature checks. In other words, these contain what you would typically put in a &lt;code>configure.ac&lt;/code> file with GNU Autoconf.&lt;/li>
&lt;li>&lt;code>docs&lt;/code>: Configures documentation files, consumed both by the build process and by the &lt;code>buildtool doc&lt;/code> interactive viewer.&lt;/li>
&lt;li>&lt;code>logic&lt;/code>: The entry point to the build process, aka the equivalent of a top-level &lt;code>Makefile.am&lt;/code> file.&lt;/li>
&lt;/ul>
&lt;h2 id="creating-source-files">Creating source files&lt;/h2>
&lt;p>Before we can successfully build our demo, we have to perform a few edits to the source tree. The wizard didn&amp;rsquo;t tell us we had to do any of these, so I encountered various failures while I played with the tool. Fear not, it wasn&amp;rsquo;t too hard to get past these failures.&lt;/p>
&lt;p>First, let&amp;rsquo;s create the top-level documents referenced by the &lt;code>docs&lt;/code> function:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ touch CHANGES PEOPLE README TODO
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, let&amp;rsquo;s look at what building the &lt;code>lib&lt;/code> subdirectory will require by looking at the &lt;code>lib/Logic.bt&lt;/code> build script:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">logic&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
bt_target demo
target_demo&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nv">BT_LIB_MAJOR&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">0&lt;/span>
&lt;span class="nv">BT_LIB_MINOR&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span>
&lt;span class="nv">BT_MANPAGES&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;demo.1&amp;#34;&lt;/span>
&lt;span class="nv">BT_SOURCES&lt;/span>&lt;span class="o">=&lt;/span>func1.c
&lt;span class="nv">BT_TYPE&lt;/span>&lt;span class="o">=&lt;/span>library
&lt;span class="nv">BT_INCLUDESDIR&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BT_DIR_INC&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">/demo&amp;#34;&lt;/span>
&lt;span class="nv">BT_INCLUDESDIR_demo_h&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BT_DIR_INC&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>
&lt;span class="nv">BT_INCLUDES&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;demo.h&amp;#34;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Based on this, let&amp;rsquo;s create the files &lt;code>lib&lt;/code> expects:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ cat &amp;gt;lib/demo.h
int func1(void);
^D
/tmp/demo$ cat &amp;gt;lib/func1.c
int func1(void) { return 42; }
^D
/tmp/demo$ touch lib/demo.1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>After that, let&amp;rsquo;s do the same with the &lt;code>src&lt;/code> subdirectory. Here is what the &lt;code>src/Logic.bt&lt;/code> build script will require:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">logic&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
bt_target demo
target_demo&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nv">BT_MANPAGES&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;demo.1&amp;#34;&lt;/span>
&lt;span class="nv">BT_SOURCES&lt;/span>&lt;span class="o">=&lt;/span>main.c
&lt;span class="nv">BT_TYPE&lt;/span>&lt;span class="o">=&lt;/span>program
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And, as before, let&amp;rsquo;s create the files &lt;code>src&lt;/code> expects:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ cat &amp;gt;src/main.c
#include &amp;lt;stdio.h&amp;gt;
#include &amp;#34;lib/demo.h&amp;#34;
int main(void) {
printf(&amp;#34;Calling library: %d\n&amp;#34;, func1());
return 0;
}
^D
/tmp/demo$ touch src/demo.1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, because we have made the binary in &lt;code>src&lt;/code> depend on the library in &lt;code>lib&lt;/code>, we have to express this dependency in the &lt;code>src/Logic.bt&lt;/code> script. We can add these two lines to the &lt;code>target_demo&lt;/code> function in &lt;code>src/Logic.bt&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="nv">BT_FLAGS_LD&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BT_FLAGS_LD&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> -L../lib&amp;#34;&lt;/span>
&lt;span class="nv">BT_LIBS&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BT_LIBS&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> -ldemo&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="configuring-the-demo">Configuring the demo&lt;/h2>
&lt;p>We are ready to go. Let&amp;rsquo;s configure the demo project against our current system:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ /tmp/local/bin/buildtool config -p /tmp/root
bt_config: loading system-wide configuration
bt_config: starting configuration for demo-0.1
checking for program gcc cc bcc... /usr/bin/cc (cached)
checking for C compiler name... gnu (cached)
checking for C compiler version... 13.0.0 (cached)
checking for program cpp... /usr/bin/cpp (cached)
checking for program ld... /usr/bin/ld (cached)
checking for c header stdio.h... yes (cached)
checking for c header sys/types.h... yes (cached)
checking for c header sys/stat.h... yes (cached)
checking for c header stdlib.h... yes (cached)
checking for c header string.h... yes (cached)
checking for c header unistd.h... yes (cached)
checking for host system name... FreeBSD
checking for host system type... FreeBSD-13.1-STABLE-amd64
checking for program ar... /usr/bin/ar (cached)
checking for program ranlib... /usr/bin/ranlib (cached)
checking for program fastdep... not found.
bt_config: creating bt_output
bt_output: creating data/demo.bpf
bt_config: generating C include header
bt_config: generating configuration environment
bt_config: generating package dependent build logic
===========================================================================
Configuration summary for demo-0.1:
Prefix is: /tmp/root
Developer mode: no
Install documentation: yes
Static libraries: no, Shared libraries: yes, rpath: yes
===========================================================================
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Pretty standard &lt;code>configure&lt;/code>-like output.&lt;/p>
&lt;p>What I want you to notice is how pretty much all results were cached. These results come from the system-wide cache we populated with &lt;code>swcgen&lt;/code> after installing Buildtool, so running this step was almost instantaneous. Good time savings.&lt;/p>
&lt;h2 id="building-the-demo">Building the demo&lt;/h2>
&lt;p>With a successful configuration step, let&amp;rsquo;s try to build:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ /tmp/local/bin/buildtool build
bt_logic: entering directory `lib&amp;#39; for `build&amp;#39;
[depend] func1.c -&amp;gt; func1.dep
c++: warning: treating &amp;#39;c&amp;#39; input as &amp;#39;c++&amp;#39; when in C++ mode, this behavior is deprecated [-Wdeprecated]
[compile] /usr/bin/cc -I/tmp/demo/lib -I/tmp/demo -I/tmp/demo -fPIC -DPIC -shared -o func1.po -c /tmp/demo/lib/func1.c
cc: warning: argument unused during compilation: &amp;#39;-shared&amp;#39; [-Wunused-command-line-argument]
[link] /usr/bin/cc -Wl,-R/usr/local/lib -shared -Wl,-soname=libdemo.so.0 -o libdemo.so.0.1 func1.po
[exec] ln -fs libdemo.so.0.1 libdemo.so.0
[exec] ln -fs libdemo.so.0.1 libdemo.so
bt_logic: leaving directory `lib&amp;#39; for `build&amp;#39;
bt_logic: entering directory `data&amp;#39; for `build&amp;#39;
bt_logic: leaving directory `data&amp;#39; for `build&amp;#39;
bt_logic: entering directory `src&amp;#39; for `build&amp;#39;
[depend] main.c -&amp;gt; main.dep
c++: warning: treating &amp;#39;c&amp;#39; input as &amp;#39;c++&amp;#39; when in C++ mode, this behavior is deprecated [-Wdeprecated]
[compile] /usr/bin/cc -I/tmp/demo/src -I/tmp/demo -I/tmp/demo -fPIC -DPIC -shared -o main.po -c /tmp/demo/src/main.c
cc: warning: argument unused during compilation: &amp;#39;-shared&amp;#39; [-Wunused-command-line-argument]
[runscript] demo
[link] /usr/bin/cc -L/tmp/demo/lib -Wl,-R/usr/local/lib -o real.bt/demo main.po -ldemo
bt_logic: leaving directory `src&amp;#39; for `build&amp;#39;
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We get a few warnings around shared libraries, as you could imagine from build scripts written almost 20 years ago, but no failures. Neat.&lt;/p>
&lt;p>But does the built product work?&lt;/p>
&lt;h2 id="running-the-demo">Running the demo&lt;/h2>
&lt;p>Let&amp;rsquo;s try to run the final binary we got:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ ./src/demo
Calling library: 42
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Yay! It does work! But wait a moment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ file src/demo
src/demo: a /tmp/local/libexec/buildtool/bt_sh script, ASCII text executable
&lt;/code>&lt;/pre>&lt;/div>&lt;p>What is this about? Why is our &lt;code>demo&lt;/code> a script and not a proper binary given that we used C?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ cat src/demo
#!/tmp/local/libexec/buildtool/bt_sh
# File generated by bt_logic.
# Fri May 13 06:35:23 PDT 2022
LD_LIBRARY_PATH=:/tmp/demo/lib; export LD_LIBRARY_PATH
/tmp/demo//src/real.bt/demo
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Aha. Much like when using GNU Libtool, a binary cannot be run from the source tree if it links against a library, because the library has not been installed yet. The script is just a wrapper to configure the dynamic loader and make things work. After installation, the rpath functionality will kick in and make the dynamic loader find the library in the right place.&lt;/p>
&lt;h2 id="installing-the-demo">Installing the demo&lt;/h2>
&lt;p>Let&amp;rsquo;s install the demo:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ /tmp/local/bin/buildtool install
bt_logic: entering directory `lib&amp;#39; for `install&amp;#39;
[install] installing data file /tmp/root/lib/libdemo.so.0.1
[exec] ln -fs libdemo.so.0.1 libdemo.so.0
[exec] ln -fs libdemo.so.0.1 libdemo.so
[install] creating missing directory /demo
mkdir: /demo: Permission denied
bt_logic: process stopped; command exited with error status `1&amp;#39;
bt_logic: leaving directory `lib&amp;#39; for `install&amp;#39;
bt_logic: entering directory `data&amp;#39; for `install&amp;#39;
[install] installing data file /tmp/local/share/buildtool/pkgflags/demo.bpf
bt_logic: leaving directory `data&amp;#39; for `install&amp;#39;
bt_logic: entering directory `src&amp;#39; for `install&amp;#39;
[install] installing binary file /tmp/root/bin/demo
[install] installing data file /tmp/root/man/man1/demo.1
bt_logic: leaving directory `src&amp;#39; for `install&amp;#39;
[install] installing data file /tmp/root/share/doc/demo/CHANGES
[install] installing data file /tmp/root/share/doc/demo/PEOPLE
[install] installing data file /tmp/root/share/doc/demo/README
[install] installing data file /tmp/root/share/doc/demo/TODO
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Oops, a little &lt;code>mkdir&lt;/code> error, but it seems to have worked. If we inspect the installation prefix we provided during the configuration step (the &lt;code>-p /tmp/root&lt;/code> flag we specified):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ find /tmp/root
/tmp/root
/tmp/root/bin
/tmp/root/bin/demo
/tmp/root/lib
/tmp/root/lib/libdemo.so
/tmp/root/lib/libdemo.so.0.1
/tmp/root/lib/libdemo.so.0
/tmp/root/share
/tmp/root/share/doc
/tmp/root/share/doc/demo
/tmp/root/share/doc/demo/PEOPLE
/tmp/root/share/doc/demo/CHANGES
/tmp/root/share/doc/demo/TODO
/tmp/root/share/doc/demo/README
/tmp/root/man
/tmp/root/man/man1
/tmp/root/man/man1/demo.1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And, from these installed files, we can run the demo:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">$ /tmp/root/bin/demo
Calling library: 42
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, note how the installed demo is really a proper executable that uses the shared library that was installed along it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ file /tmp/root/bin/demo
/tmp/root/bin/demo: ELF 64-bit LSB executable, x86-64, version 1 (FreeBSD), dynamically linked, interpreter /libexec/ld-elf.so.1, for FreeBSD 13.0 (1300525), FreeBSD-style, with debug_info, not stripped
/tmp/demo$ ldd /tmp/root/bin/demo
/tmp/root/bin/demo:
libdemo.so.0 =&amp;gt; /tmp/root/lib/libdemo.so.0 (0x200000)
libc.so.7 =&amp;gt; /lib/libc.so.7 (0x200000)
[preloaded]
[vdso] (0x7ffffffff5d0)
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="distributing-the-demo">Distributing the demo&lt;/h2>
&lt;p>As a software package author, installing the program in one&amp;rsquo;s machine is nice, but what about giving the program to other people? This is where the &lt;code>dist&lt;/code> command comes into play, much like GNU Automake&amp;rsquo;s &lt;code>dist&lt;/code> target:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ /tmp/local/bin/buildtool dist
bt_dist: cleaning tree (cleandir)
bt_logic: entering directory `lib&amp;#39; for `clean&amp;#39;
[remove] func1.po
[remove] libdemo.so.0.1 libdemo.so.0 libdemo.so
bt_logic: leaving directory `lib&amp;#39; for `clean&amp;#39;
bt_logic: entering directory `lib&amp;#39; for `cleandir&amp;#39;
[remove] func1.dep
bt_logic: leaving directory `lib&amp;#39; for `cleandir&amp;#39;
bt_logic: entering directory `data&amp;#39; for `clean&amp;#39;
[remove] demo.bpf
bt_logic: leaving directory `data&amp;#39; for `clean&amp;#39;
bt_logic: entering directory `data&amp;#39; for `cleandir&amp;#39;
bt_logic: leaving directory `data&amp;#39; for `cleandir&amp;#39;
bt_logic: entering directory `src&amp;#39; for `clean&amp;#39;
[remove] main.po
[remove] demo
[remove] real.bt/demo
bt_logic: leaving directory `src&amp;#39; for `clean&amp;#39;
bt_logic: entering directory `src&amp;#39; for `cleandir&amp;#39;
[remove] main.dep
bt_logic: leaving directory `src&amp;#39; for `cleandir&amp;#39;
[remove] /tmp/demo/bt_config.h /tmp/demo/bt_config.env /tmp/demo/bt_logic.env /tmp/demo/bt_config.log /tmp/demo/bt_config.sed /tmp/demo/bt_output
bt_dist: validating package
=&amp;gt; Checking root files
WARN: COPYING not found; it is highly recommended
=&amp;gt; Checking definitions (/tmp/demo/Generic.bt)
WARN: no package homepage
=&amp;gt; Checking configuration script (/tmp/demo/Generic.bt)
=&amp;gt; Summary
bt_lint: package should be corrected; 2 warnings
bt_dist: building compressed dist, type tar.gz
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above invocation above generates a &lt;code>demo-0.1.tar.gz&lt;/code> distributable source package, which we can inspect:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">/tmp/demo$ tar tzf ../demo-0.1.tar.gz
demo-0.1/
demo-0.1/PEOPLE
demo-0.1/lib/
demo-0.1/CHANGES
demo-0.1/TODO
demo-0.1/README.bt
demo-0.1/data/
demo-0.1/README
demo-0.1/src/
demo-0.1/Generic.bt
demo-0.1/src/Logic.bt
demo-0.1/src/main.c
demo-0.1/src/demo.1
demo-0.1/data/Logic.bt
demo-0.1/data/demo.bpf.in
demo-0.1/lib/func1.c
demo-0.1/lib/Logic.bt
demo-0.1/lib/demo.1
demo-0.1/lib/demo.h
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="peeking-into-the-data-directory">Peeking into the data directory&lt;/h2>
&lt;p>I glanced over the &lt;code>data&lt;/code> directory earlier on, skipping that mysterious &lt;code>bpf&lt;/code> file. If we look at the files that were installed under Buildtool&amp;rsquo;s own prefix (a mistake) when we installed the demo, we can find that our package installed a &lt;code>share/buildtool/pkgflags/demo.bpf&lt;/code> file. Looking at its contents, we see:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># $Id: pkgflags,v 1.1 2003/04/26 21:48:17 jmmv Exp $&lt;/span>
&lt;span class="c1"># pkgflags file&lt;/span>
&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># This file is mostly useful for packages providing libraries.&lt;/span>
&lt;span class="c1"># If not needed (i.e., if the package is a program), remove it.&lt;/span>
&lt;span class="c1">#&lt;/span>
&lt;span class="nv">BT_PREFIX&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;/tmp/root&amp;#34;&lt;/span>
&lt;span class="nv">BT_DIR_LIB&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;/tmp/root/lib&amp;#34;&lt;/span>
&lt;span class="nv">BT_DIR_INCLUDE&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;/tmp/root/include&amp;#34;&lt;/span>
&lt;span class="nv">bpf_name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;demo&amp;#34;&lt;/span>
&lt;span class="nv">bpf_descr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Remembering Buildtool&amp;#34;&lt;/span>
&lt;span class="nv">bpf_version&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;0.1&amp;#34;&lt;/span>
&lt;span class="nv">bpf_libs&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;-L&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BT_DIR_LIB&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> -ldemo&amp;#34;&lt;/span>
&lt;span class="nv">bpf_cflags&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;-I&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">BT_DIR_INCLUDE&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is the equivalent of what a &lt;code>pkg-config&lt;/code> file would provide. And it works. We can query the file using Buildtool&amp;rsquo;s &lt;code>pkgflags&lt;/code> command and obtain the right compiler and linker flags for our demo library:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">$ /tmp/local/bin/buildtool pkgflags --cflags --libs demo
setting undefined variable `BT_PREFIX&amp;#39;
setting undefined variable `BT_DIR_LIB&amp;#39;
setting undefined variable `BT_DIR_INCLUDE&amp;#39;
-I/tmp/root/include -L/tmp/root/lib -ldemo
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And this concludes the demo.&lt;/p>
&lt;h1 id="what-happened-to-buildtool">What happened to Buildtool?&lt;/h1>
&lt;p>Buildtool as it was in its 0.16 release in 2004 seems fairly impressive. The user manual is comprehensive, the tool provides many more features than I remembered, and it still works to this day.&lt;/p>
&lt;p>So what happened to Buildtool? Why did I officially cancel it in 2005? There are a few reasons.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The first is that Buildtool collapsed under its own complexity. Shell is not the right language to write a build system in, and &lt;code>bt_logic&lt;/code> became an unmanageable mess to deal with. I&amp;rsquo;m surprised it still works on a modern system to be honest.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The second is that, even though Buildtool seems &amp;ldquo;complete&amp;rdquo;, it still lacks lots of functionality&amp;mdash;and the missing functionality wasn&amp;rsquo;t easy to implement. When I was writing the tool, I found myself leaning on the GNU Autotools manuals to understand how things worked across systems, and I relied on those details to implement my own versions. The more I did this, the more I learned about the GNU Autotools, and the more I realized how knowledgeable the GNU Autotools authors were and how far I was from providing something comparable. This was humbling. Furthermore, during this process, I had become so &amp;ldquo;fluent&amp;rdquo; in the GNU Autotools that it made no sense to continue developing my own thing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The third is that having to install a supporting build tool to compile fundamental system packages wasn&amp;rsquo;t well-seen at the time. Every time I had to deal with a similar system in pkgsrc (Boost.Jam or whatever Mozilla had, for example), it was a pain. Like it or not, the GNU Autotools are the de-facto standard, and despite all of their flaws, they are the ones that integrate well with packaging systems and system tools.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>My interest in build systems remains and what you have read here partially explains my &lt;a href="/2015/04/on-bazel-and-open-source.html">original critique of Bazel&lt;/a>. I ended up working in Bazel for a few years though because I like the topic. No matter my original comments, Bazel is a great build system from which many others should learn. But it&amp;rsquo;s still missing the kind of system-level integration that Buildtool intended to provide via the &lt;code>install&lt;/code> and &lt;code>pkgflags&lt;/code> commands.&lt;/p>
&lt;p>One thing I&amp;rsquo;ll note is how Buildtool already had a concept of high-level targets from a semantical perspective. You could make a single build file define various targets (libraries, programs, etc.) and each of those would know how to build, install, and clean itself. You can see that the same ideas have evolved in most of today&amp;rsquo;s build systems in some form or another, and that the file-level dependency tracking that &lt;code>make&lt;/code> provides is &amp;ldquo;a thing of the past&amp;rdquo;.&lt;/p>
&lt;p>To conclude, let me add that I still believe there is room for something like Buildtool in this day and age to support the foundations of our free Unix-like systems&amp;hellip; but it would need to be much better designed and implemented than Buildtool. I have ideas; I just wish I had the same amount of free time that I had when I was a student.&lt;/p></description></item><item><title>Rust is hard, yes, but does it matter?</title><link>https://jmmv.dev/2022/05/rust-is-hard-but-does-it-matter.html</link><pubDate>Fri, 06 May 2022 06:45:00 -0700</pubDate><guid>https://jmmv.dev/2022/05/rust-is-hard-but-does-it-matter.html</guid><description>&lt;p>Rust is infamous for having a steep learning curve. The borrow checker is the first boss you must defeat, but with a good mental model of how memory works, how objects move, and the rules that the borrow checker enforces, it becomes second nature rather quickly. These rules may sound complicated, but really, they are about understanding the fundamentals of how a computer works.&lt;/p>
&lt;p>That said&amp;hellip; the difficulties don&amp;rsquo;t stop there. Oh no. As you continue to learn about the language and start dealing with things like concurrency&amp;mdash;or, God forbid, Unix signals&amp;mdash;things can get tricky very quickly. To make matters worse, mastering &lt;a href="https://rust-unofficial.github.io/patterns/intro.html">idiomatic Rust&lt;/a> and &lt;a href="/2020/04/rust-into-trait.html">the purpose of core traits&lt;/a> takes a lot of time. I&amp;rsquo;ve had to throw my arms up in frustration a few times so far and, while I&amp;rsquo;ve emerged from those exercises as a better programmer, I have to concede that they were exhausting experiences. And I am certainly not an expert yet.&lt;/p>
&lt;p>So, yes, there is no denying in saying that Rust is harder than other languages. But&amp;hellip; does it matter in practical terms?&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Betteridge's_law_of_headlines">Betteridge&amp;rsquo;s law of headlines&lt;/a> says that we should conclude the post right here with a &amp;ldquo;no&amp;rdquo;&amp;mdash;and I think that&amp;rsquo;s the right answer. But let&amp;rsquo;s see why.&lt;/p>
&lt;hr>
&lt;p>To answer this question, I&amp;rsquo;d like to imagine what would happen if we were to use Rust in a large team (say tens of people) that deals with a large codebase (hundreds of thousands of SLoC).&lt;/p>
&lt;p>The reason I pick this scenario is &lt;em>totally&lt;/em> unrelated (wink, wink) to the work I do on a day-to-day basis in Azure Storage. Our current codebase is written in C++ and has its fair share of NPEs and concurrency bugs, so we have sometimes &lt;del>argued&lt;/del> fantasized with the idea of adopting Rust.&lt;/p>
&lt;p>An obvious concern that arises is that adding a new language to a large project is&amp;hellip; very difficult: fighting inertia, bringing up tooling, training people, porting code &lt;a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/">without a rewrite&lt;/a>&amp;hellip; these are all very hard work items. But there is a more subtle worry: even if we went through this whole endeavor, would our developer population be able to learn enough Rust to contribute to the codebase in a meaningful way? The language is, again, complex at first sight, and we should not expect everyone to master it.&lt;/p>
&lt;p>The first observation is that, in a sufficiently large team, we will have developers with various levels of expertise no matter the language. This is expected and intentional, but depending on the language, the consequences are different. For example: C++ is &lt;em>also&lt;/em> a very complex language. Sure, it may be easier to &lt;em>write&lt;/em> than Rust because the compiler is more forgiving, but it&amp;rsquo;s also much harder to guarantee its correctness. This comes back to bite the developer team at a later point, because now you need to call the experts to debug crashes and race conditions.&lt;/p>
&lt;p>The second observation comes from my writing of side projects in Rust. I&amp;rsquo;m finding that the majority of my time goes into writing straightforward business logic and refactoring tests, for which Rust doesn&amp;rsquo;t get in the way. It&amp;rsquo;s only during certain parts of the project&amp;rsquo;s lifetime that I&amp;rsquo;ve had to build the foundations (abstractions, layering, async constructs, etc.) or done large redesigns, and it&amp;rsquo;s only during those times that I&amp;rsquo;ve had my fights with Rust.&lt;/p>
&lt;p>In other words: given a sufficiently large project or team, and irrespective of the language, there will always be a set of core maintainers that design, write and maintain the foundations. This set of people knows the ins and outs of the project and should know the ins and outs of the language and its ecosystem. This set of people is &lt;em>necessary&lt;/em>. But once these foundations are in place, all other contributors can focus on the exciting aspects of building features. Rust&amp;rsquo;s complexities may still get in the way, but not much more than those of other languages.&lt;/p>
&lt;p>To conclude, I would like you to &lt;a href="/2022/04/do-rust-devs-hate-go-devs.html">consider again&lt;/a> that learning a language is not just a matter of learning its syntax. Mastering a programming language requires months of expertise with the language&amp;rsquo;s idiosyncrasies and its libraries, and one must go through these before making comparisons about long-term maintainability. But yes, Rust could be simpler, and there are &lt;a href="https://tim.mcnamara.nz/post/683022094467039232/easy-mode-for-rust">efforts to make it so&lt;/a>!&lt;/p>
&lt;p>Finally, a question for you. I haven&amp;rsquo;t had the fortune (?) of working in a large-scale Rust project yet, so all I&amp;rsquo;m doing is hypothesizing here based on experiences with other languages in large projects and teams. If you have converted a team into Rust, or if you were brought in to contribute to an existing Rust codebase, would you mind sharing your experience below? :)&lt;/p></description></item><item><title>Do Rust devs hate Go devs?</title><link>https://jmmv.dev/2022/04/do-rust-devs-hate-go-devs.html</link><pubDate>Fri, 29 Apr 2022 09:50:00 -0700</pubDate><guid>https://jmmv.dev/2022/04/do-rust-devs-hate-go-devs.html</guid><description>&lt;p>Earlier this week, a 2-year old post titled &lt;a href="https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride">I want off Mr. Golang&amp;rsquo;s wild ride&lt;/a> by &lt;a href="https://fasterthanli.me/">@fasterthanlime&lt;/a> made the news rounds &lt;em>again&lt;/em>. This post raises a bunch of concerns on the Go language and is posted from the perspective of someone who prefers Rust. And, just yesterday, I noticed &lt;a href="https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang">a comment on Twitter by @FiloSottile&lt;/a> that, paraphrased, reads &amp;ldquo;&lt;em>Why is there so much hatred towards Go, especially from Rust developers?&lt;/em>&amp;rdquo;.&lt;/p>
&lt;p>I wish I could answer this question with a &amp;ldquo;no, there isn&amp;rsquo;t&amp;rdquo;, but that would be a lie: in any large community, there will certainly be hateful people/opinions. If you have encountered such flamebait, I&amp;rsquo;m sorry, and I&amp;rsquo;m not here to defend it. What I&amp;rsquo;m here to do is look at the possible truth behind the claim that Rust developers dislike Go, and I wanted to elaborate on this based on my personal experience.&lt;/p>
&lt;hr>
&lt;p>I have taught myself both Go (in 2016) and Rust (in 2018). I have written the exact same project&amp;mdash;&lt;a href="/software/sandboxfs.html">sandboxfs&lt;/a>, a FUSE file system&amp;mdash;in both languages, which allowed me to experience pretty closely how these two languages compare for high-performance systems-level software development. I have written other code in both as well, using Go when I was at Google and Rust for &lt;a href="https://www.endbasic.dev/">a side project&lt;/a>.&lt;/p>
&lt;p>And, you know, I enjoy writing code in both languages. They are fun; they make me productive; they are better than the alternatives. You can read &lt;a href="/2016/03/golang-review.html">my review of Go in 2016&lt;/a>; &lt;a href="/2018/05/rust-review-introduction.html">my review of Rust in 2018&lt;/a>; and &lt;a href="/2018/07/rust-vs-go.html">my comparison of both languages in 2018&lt;/a>. So, I do have some &amp;ldquo;comparable&amp;rdquo; experience with these two languages and, based on this experience, I think I understand where some of this &amp;ldquo;Go hatred&amp;rdquo; comes from. And I would say that this hatred comes from &lt;em>sadness&lt;/em>.&lt;/p>
&lt;p>From the very beginning, Go has been touted as this systems-level language that can replace C and C++ in large-scale projects. A super-simple systems language that is safer than C and almost as fast as C. And, yes, Go is indeed a simple language: you can learn Go in a day (yes, really) if you are already familiar with a C-style language, and you can start writing code right away, which is really empowering. The problem is that, like with &lt;em>any other language&lt;/em>, mastering Go idioms and best practices will take &lt;em>much&lt;/em> longer though&amp;mdash;so don&amp;rsquo;t go write your next startup&amp;rsquo;s foundations in Go just with a weekend&amp;rsquo;s worth of experience; it &lt;em>feels&lt;/em> like you can do so, but it would be an uninformed choice.&lt;/p>
&lt;p>Other than being simple, Go is also extremely nice to interact with: enforced code formatting, simple build tools, a builtin profiler, a comprehensive core library, super-fast build times&amp;hellip; These are all qualities of the Go ecosystem that increase individual productivity very quickly and with little effort. You can imagine that these are very appealing when working on a greenfield project, especially with a team, because you can start shipping soon.&lt;/p>
&lt;hr>
&lt;p>OK, so Go is great. Then, why the &lt;del>hatred&lt;/del> sadness towards it? And why does Rust have anything to do with it?&lt;/p>
&lt;p>To answer this, we have to start from the assumption that Go and Rust are competing for the same systems development space; that is, finding an alternative to C and C++. This assumption may or may not be valid, but I believe that lots of people come to these languages with this preconception in mind (myself included). As a result, it&amp;rsquo;s not a matter of choosing Rust or Go for a specific task: it&amp;rsquo;s a polarizing topic.&lt;/p>
&lt;p>Once you start with this assumption, if you learn Rust &lt;em>after&lt;/em> Go, you realize &lt;em>how much better&lt;/em> (in safety, performance and agility terms) systems programming &lt;em>could be&lt;/em> if we used Rust throughout. But because Go is so widespread and gives the (false) impression of being much easier to master, Rust is likely to lose the battle. And, unfortunately, most technical decisions at the beginning of a project&amp;mdash;which language, which database, which cloud provider, etc.&amp;mdash;are made based on gut feelings and superficial information. By the time problems become apparent, it&amp;rsquo;s a gargantuan task to change those choices.&lt;/p>
&lt;p>And &lt;em>here&lt;/em> is where the sadness comes in. Once you learn Rust and experience what it brings to the table, you start finding gaps that Go doesn&amp;rsquo;t cover, which means that if we double down on Go, we perpetuate these problems. Null pointers are there. Data races are there (because, even if you have channels, you &lt;em>also&lt;/em> have mutexes and you ought to use them for performance reasons). Garbage collection is there. Duck typing and the widespread presence of &lt;code>interface{}&lt;/code> make refactoring difficult. And my pet peeve: annotations to express safety constraints on the code are notably &lt;em>not&lt;/em> there, resulting in lots of comments and &lt;a href="/2018/07/forbidden-assertions-fallacy.html">dumbed-down runtime error checking&lt;/a> that shouldn&amp;rsquo;t be necessary.&lt;/p>
&lt;p>Note that I am intentionally &lt;em>not&lt;/em> mentioning Go modules or generics here. I know these features have been controversial for a really long time and, fortunately, they do exist now in Go after years of debate. I&amp;rsquo;m glad that they do, but the core issues with Go don&amp;rsquo;t stem from the lack of features. They stem from the foundations of the language.&lt;/p>
&lt;p>I get why Go is so appealing. And I also understand that Rust can be &lt;em>very&lt;/em> off-putting at first. But I don&amp;rsquo;t like to think of a future world where systems are all written in Go. Such a world would definitely be a better than the current world dominated by C and C++ memory leaks and buffer overruns, but we would still suffer from many issues that simply wouldn&amp;rsquo;t exist if the same software had been written in Rust. And &lt;em>that&lt;/em> is what I think bothers people. Well, at least it bothers me a little bit.&lt;/p>
&lt;p>Just as an example to illustrate why these design choices get in the way, I want to go back to my sandboxfs experience. When working on the Rust rewrite, I used the Go implementation as a reference and tried to translate the code while keeping a similar design. During the process, I remember spotting many subtle bugs in the Go version that had escaped code review and testing, and I spotted them &lt;em>because&lt;/em> Rust would simply not allow me to express such faulty code. I want more of that kind of enforced validation and sanity-checking for our systems of the future, not less.&lt;/p>
&lt;p>But again, that&amp;rsquo;s just my opinion. The only thing I would say if you are reading this is: if you are in a position to decide between Go or Rust for a new codebase, don&amp;rsquo;t make the choice without learning about both languages in some level of detail. Only once you have some experience in both you&amp;rsquo;ll be able to make an informed decision. Tradeoffs!&lt;/p></description></item><item><title>Rust traits and dependency injection</title><link>https://jmmv.dev/2022/04/rust-traits-and-dependency-injection.html</link><pubDate>Fri, 22 Apr 2022 09:30:00 -0700</pubDate><guid>https://jmmv.dev/2022/04/rust-traits-and-dependency-injection.html</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Dependency_injection">Dependency injection&lt;/a> is one of my favorite design patterns to develop highly-testable and modular code. To apply this pattern, all you have to do is follow two simple guidelines:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Separate object construction from usage.&lt;/strong> In practical terms: stop creating objects inside constructors and take those objects as input arguments.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Use interfaces instead of concrete types as constructor parameters.&lt;/strong> In this way, the receiver remains agnostic to the implementation of those types and thus it becomes possible to supply different implementations.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Dependency injection is key to testability indeed, but it is also a good design principle&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> because it keeps system pieces loosely coupled. The best part is that dependency injection is a simple concept: there is no need for fancy frameworks.&lt;/p>
&lt;p>As for how to define the generic interfaces, the techniques depend on your language of choice. In C++, you would define pure abstract classes; in Java, Go and C#, you would define interfaces; and in Rust, you would use traits.&lt;/p>
&lt;p>This post is about Rust though, so let&amp;rsquo;s talk about using traits for dependency injection, how they have a nasty side-effect, and what we can do about it.&lt;/p>
&lt;h1 id="an-example">An example&lt;/h1>
&lt;p>When the dependency injection pattern is applied correctly, a library crate exposes:&lt;/p>
&lt;ol>
&lt;li>traits to represent heavyweight objects;&lt;/li>
&lt;li>a collection of objects that implement those traits; and&lt;/li>
&lt;li>functions that consume those objects via the generic traits.&lt;/li>
&lt;/ol>
&lt;p>Then, consumers of the library instantiate the specific objects they need, wire them together to create a dependency graph, and feed those into the generic business logic functions exposed by the library.&lt;/p>
&lt;p>Is that clear? Yeah&amp;hellip; I guess not. Let&amp;rsquo;s take a look at a concrete example so that we can have reference code for the explanations below. For that, I&amp;rsquo;ll use the code of &lt;a href="/2022/04/introducing-db-logger.html">the &lt;code>db_logger&lt;/code> crate I recently published&lt;/a>.&lt;/p>
&lt;p>The &lt;code>db_logger&lt;/code> crate provides a &lt;code>log&lt;/code> facade implementation that records log messages into a database. The database in which messages are stored depends on your choice between PostgreSQL and SQLite (for now), and this selection is made via dependency injection.&lt;/p>
&lt;p>You would think that Cargo features would be sufficient to choose a database backend, but Cargo features are a poor tool for this kind of configuration. Cargo features are well-suited to control which dependencies are &lt;em>built&lt;/em> and not&amp;mdash;and &lt;a href="https://github.com/jmmv/db_logger/blob/bc3adfbf4b335af84f0178f8b0331f6a6d2323c5/Cargo.toml#L13">&lt;code>db_logger&lt;/code> indeed supplies &lt;code>postgres&lt;/code> and &lt;code>sqlite&lt;/code> features&lt;/a> for this reason&amp;mdash;but you, the user, still have to choose which database to talk to via code. Configuring each backend requires different settings and you may even want to select a backend at runtime.&lt;/p>
&lt;p>To support runtime configuration, we start by defining a trait to represent the generic features we need our database connection to expose. In this way, the logging (business) logic can record log entries and remain unaware of which specific database it is talking to. Then, we add a function to initialize the logger based on an abstract connection object and we are done:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="cp">#[async_trait]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="sd">/// Operations that an arbitrary database connection can perform.
&lt;/span>&lt;span class="sd">&lt;/span>&lt;span class="k">pub&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">trait&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Db&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">async&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">put_log_entries&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">es&lt;/span>: &lt;span class="nb">Vec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">LogEntry&lt;/span>&lt;span class="o">&amp;lt;&amp;#39;&lt;/span>&lt;span class="nb">_&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;#39;&lt;/span>&lt;span class="nb">_&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="sd">/// Initializes the logging subsystem to record entries in `db`.
&lt;/span>&lt;span class="sd">&lt;/span>&lt;span class="k">pub&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">db&lt;/span>: &lt;span class="nc">Arc&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="k">dyn&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Db&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">Send&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">Sync&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;#39;&lt;/span>&lt;span class="nb">static&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c1">// ...
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With this interface at hand, consumers of &lt;code>db_logger&lt;/code> can pick which database to connect to by using alternate objects that implement the &lt;code>Db&lt;/code> trait&amp;mdash;of which there are two now: &lt;code>PostgresDb&lt;/code> and &lt;code>SqliteDb&lt;/code>.&lt;/p>
&lt;p>On the consumer side (say, from &lt;code>src/main.rs&lt;/code>), we can do something like this when we want to talk to PostgreSQL:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="kd">let&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">db&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Arc&lt;/span>::&lt;span class="n">from&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">db_logger&lt;/span>::&lt;span class="n">PostgresDb&lt;/span>::&lt;span class="n">connect_lazy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">host&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">port&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">database&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">username&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">password&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="n">db_logger&lt;/span>::&lt;span class="n">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">db&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c1">// Doesn&amp;#39;t care about which specific `db`.
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or something like this when we want to talk to SQLite:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="kd">let&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">db&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Arc&lt;/span>::&lt;span class="n">from&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">db_logger&lt;/span>::&lt;span class="n">SqliteDb&lt;/span>::&lt;span class="n">connect&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">uri&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="n">db_logger&lt;/span>::&lt;span class="n">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">db&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c1">// Doesn&amp;#39;t care about which specific `db`.
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that, thanks to this design, not only downstream consumers are better off: the business logic unit tests can use this abstraction as well to remain stable and extremely fast. In particular, the logging logic&amp;rsquo;s &lt;a href="https://github.com/jmmv/db_logger/blob/bc3adfbf4b335af84f0178f8b0331f6a6d2323c5/src/logger.rs#L347">unit tests inject a connection to an in-memory SQLite database&lt;/a>, which makes them super-fast and avoids flakiness due to misconfiguration or networking issues.&lt;/p>
&lt;p>Sounds great, right? Well, it does, but notice how the &lt;code>Db&lt;/code> trait above references the &lt;code>LogEntry&lt;/code> type in its &lt;code>put_log_entries()&lt;/code> function. That type, which users of the &lt;code>db_logger&lt;/code> crate should have no knowledge of, must now be public too because &lt;code>Db&lt;/code> is public. And this is a big transitive problem.&lt;/p>
&lt;h1 id="the-problem">The problem&lt;/h1>
&lt;p>To key issue with using traits in Rust for dependency injection is that any type referenced in a function signature must be at least as visible as the function itself. Which means that if a trait is public (like the &lt;code>Db&lt;/code> trait above), any type referenced by any of the trait&amp;rsquo;s functions (like the &lt;code>LogEntry&lt;/code> struct above), will have to be public as well.&lt;/p>
&lt;p>Too broad visibility is problematic for at least two reasons:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Broken encapsulation.&lt;/strong> Users of a library (crate) shouldn&amp;rsquo;t see APIs that are internal to the library. Otherwise, they can easily take dependencies on implementation details, which can break behavior and will make your life as a maintainer harder in the future.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Insufficient dead code detection.&lt;/strong> Once a type is marked public, the compiler cannot claim that it is unused even if nothing else uses the type within the crate. This is viral: an unused type may be referenced by unused functions, which in turn may reference other unused types, etc. Link-time optimizations can make this (almost) a non-issue at runtime, but any dead code is a liability during development because it gets in the way of maintenance.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>This problem is not exclusive to library crates. Binary crates suffer as well if you follow the recommendations of placing most code into a private library and having &lt;code>src/main.rs&lt;/code> be a simple facade over the library. Plus &lt;em>any&lt;/em> crate can suffer if it has integration tests because those can only interact with your public interface.&lt;/p>
&lt;p>So, what can we do about this to keep our architecture sane?&lt;/p>
&lt;h1 id="bad-solution-do-it-all-functions">Bad solution: &amp;ldquo;do-it-all functions&amp;rdquo;&lt;/h1>
&lt;p>A first solution is to try and hide the problematic traits behind what I&amp;rsquo;ll call &amp;ldquo;do-it-all functions&amp;rdquo; for lack of a better name.&lt;/p>
&lt;p>This is what I first did in a couple of projects last year. In those project, I used to have a &lt;code>serve_rest_api()&lt;/code> public function that took the database connection and then started a REST server backed by it. To hide the traits, I renamed this function to &lt;code>serve_rest_api_internal()&lt;/code> and then added a new &lt;code>serve_rest_api()&lt;/code> function that took configuration parameters to decide which objects to instantiate, thus subsuming most of the logic that previously existed in &lt;code>src/main.rs&lt;/code>.&lt;/p>
&lt;p>Needless to say, this is ugly because we lose &lt;a href="https://en.wikipedia.org/wiki/Composability">composability&lt;/a>, and this looks bad because we are shoehorning the responsibilities of the main program into the library&amp;mdash;all to appease some visibility issues. Not a good trade-off from an API design perspective.&lt;/p>
&lt;p>To make matters worse, this approach didn&amp;rsquo;t quite work for &lt;code>db_logger&lt;/code>. In the case of this library, you can imagine having exposed &lt;code>init_postgres()&lt;/code> and &lt;code>init_sqlite()&lt;/code> public functions (again, bad for composability) that created the database objects within them. I tried doing that, but I ran into some nightmare-ish problems in my integration tests because I had to handle lifetimes and async tasks across async runtime boundaries (&lt;code>Drop&lt;/code> being sync is&amp;hellip; troublesome).&lt;/p>
&lt;p>As a result of this trouble, I ended up having to dump this &amp;ldquo;solution&amp;rdquo; and spend a couple of head-scratching mornings finding an alternative&amp;mdash;which is a good thing because these &amp;ldquo;do-it-all functions&amp;rdquo; really sucked from a design perspective.&lt;/p>
&lt;h1 id="good-solution-newtype">Good solution: newtype&lt;/h1>
&lt;p>I don&amp;rsquo;t know why it took me so long to reach the conclusion of using the &lt;a href="https://rust-unofficial.github.io/patterns/patterns/behavioural/newtype.html">newtype idiom&lt;/a> to hide the traits. I guess I was too bogged down in trying to make the specific solution above work, and that prevented me from seeing an alternative approach. In retrospect, it sounds trivial, but here it goes.&lt;/p>
&lt;p>The idea to solve the visibility issues is to introduce a new concrete type that wraps the trait as its single member. Then, this concrete type is the one that&amp;rsquo;s made public and the trait (and all of its dependencies) can remain private.&lt;/p>
&lt;p>For our &lt;code>db_logger&lt;/code> case study, all we have to do is introduce a new type, like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="cp">#[derive(Clone)]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="k">pub&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">struct&lt;/span> &lt;span class="nc">Connection&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Arc&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="k">dyn&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Db&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">Send&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">Sync&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;#39;&lt;/span>&lt;span class="nb">static&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note how &lt;code>Connection&lt;/code> is just wrapping the &lt;code>Db&lt;/code> trait, but now, the trait is an implementation detail of the struct and does not have to be public. Note also how this hides the complexity of the &lt;code>Db&lt;/code> instance representation: the &lt;code>Arc&lt;/code> and all of the trait bounds are now hidden within the struct and don&amp;rsquo;t pollute the public API.&lt;/p>
&lt;p>With this, we can update our concrete implementations of &lt;code>Db&lt;/code> with some factory methods:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="sd">/// Factory to connect to a PostgreSQL database.
&lt;/span>&lt;span class="sd">&lt;/span>&lt;span class="k">pub&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">connect_lazy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">opts&lt;/span>: &lt;span class="nc">ConnectionOptions&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nc">Connection&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Connection&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Arc&lt;/span>::&lt;span class="n">from&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">PostgresDb&lt;/span>::&lt;span class="n">connect_lazy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">opts&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">None&lt;/span>&lt;span class="p">)))&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="sd">/// Factory to connect to a SQLite database.
&lt;/span>&lt;span class="sd">&lt;/span>&lt;span class="k">pub&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">async&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">fn&lt;/span> &lt;span class="nf">connect&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">opts&lt;/span>: &lt;span class="nc">ConnectionOptions&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>-&amp;gt; &lt;span class="nb">Result&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Connection&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SqliteDb&lt;/span>::&lt;span class="n">connect&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">opts&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="k">await&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="n">db&lt;/span>&lt;span class="o">|&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">Connection&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Arc&lt;/span>::&lt;span class="n">from&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">db&lt;/span>&lt;span class="p">)))&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And, finally, our caller code can do one of these to set up the logger with ease:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-rust" data-lang="rust">&lt;span class="kd">let&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">conn&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">if&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">use_real_db&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">postgres&lt;/span>::&lt;span class="n">connect_lazy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">..&lt;/span>&lt;span class="p">.)&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="p">}&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">else&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">sqlite&lt;/span>::&lt;span class="n">connect&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">..&lt;/span>&lt;span class="p">.)&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="p">};&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="n">db_logger&lt;/span>::&lt;span class="n">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">conn&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Voila. By hiding the trait into a struct with the newtype idiom, the trait and all of its internal dependent types can be private again. And, as expected, the compiler can now spot unused code.&lt;/p>
&lt;p>&lt;strong>Update (2022-04-23):&lt;/strong> Some people have brought up via other channels that using static dispatch might be a better approach to avoid the runtime overheads imposed by the solution above. Maybe. I hadn&amp;rsquo;t thought about that when writing this post or the referenced code. Would be interesting to explore that avenue.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Dependency injection can also be a horrible way to make your code harder to work with when applied blindly. Some people have taken the meaning of &amp;ldquo;don&amp;rsquo;t construct objects inside other objects&amp;rdquo; to the limit and have hidden even classes that represent plain data types behind interfaces. Don&amp;rsquo;t be that person. Only define interfaces for objects that interact with the real world (file systems, databases, networks, graphics, etc.) or that implement functionality that is so complex that it makes other parts of the system hard to test.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>QB64 Super Dark Blue color theme</title><link>https://jmmv.dev/2022/04/qb64-super-dark-blue.html</link><pubDate>Wed, 20 Apr 2022 19:00:00 -0700</pubDate><guid>https://jmmv.dev/2022/04/qb64-super-dark-blue.html</guid><description>&lt;p>Towards the end of 2021, I was playing with &lt;a href="https://qb64.org/">QB64&lt;/a> and thought that its default color scheme&amp;mdash;called &lt;em>Super Dark Blue&lt;/em>&amp;mdash;was quite neat. It reminded me of QuickBASIC, which is what the whole program is supposed to do, but the colors felt vivid and modern. Take a look:&lt;/p>
&lt;figure>
&lt;img src="/images/2022-04-20-qb64.png" width="100%" style="max-width: 642px"/>
&lt;figcaption>QB64 with its default color configuration.&lt;/figcaption>
&lt;/figure>
&lt;p>&amp;ldquo;Naturally,&amp;rdquo; I wondered if I could adopt those colors in VSCode and Windows Terminal, as these are the apps I look at the most throughout a work day. I quickly ruled out VSCode because defining a theme seems non-trivial, but creating a scheme for Windows Terminal was very easy.&lt;/p>
&lt;p>To create the Windows Terminal theme, I took the original QB64 colors, mapped them to the bright ANSI colors (numbers 8 through 15) and dialed their brightness down to come up with the dark colors (numbers 0 through 7). After that, I uploaded the theme to the &lt;a href="https://windowsterminalthemes.dev/?theme=QB64%20Super%20Dark%20Blue">Windows Terminal Themes&lt;/a> site and called it a day.&lt;/p>
&lt;figure>
&lt;img src="/images/2022-04-20-qb64-terminal.png" width="100%" style="max-width: 754px"/>
&lt;figcaption>Windows Terminal with my adaptation of the QB64 Super Dark Blue theme.&lt;/figcaption>
&lt;/figure>
&lt;p>But just yesterday, I took out an old PowerBook G4 from the closet, which has NetBSD on it and that I had not touched for two years. As I was staring at the ugly color scheme I had previously configured, I thought: can I port those same colors to xterm?&lt;/p>
&lt;p>Of course I can. In fact, I had previously done this almost 10 years ago when I ported &lt;a href="/2013/09/novel-color-scheme-for-xterm.html">Mac OS X&amp;rsquo;s Novel color scheme&lt;/a> to xterm&amp;mdash;which, by the way, is why I&amp;rsquo;m writing this post. Re-reading that old article from 2013 and seeing its screenshots made me nostalgic.&lt;/p>
&lt;p>To create the xterm theme, I first took the configuration I had previously written for the Windows Terminal, which looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="p">{&lt;/span>
&lt;span class="nt">&amp;#34;background&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#000027&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;black&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#000000&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;blue&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#054663&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;brightBlack&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#626262&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;brightBlue&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#457693&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;brightCyan&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#00586C&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;brightGreen&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#55CE55&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;brightPurple&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#934593&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;brightRed&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#D8624E&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;brightWhite&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#D8D8D8&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;brightYellow&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#FFA700&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;cursorColor&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#D8D8D8&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;cyan&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#00485C&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;foreground&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#D8D8D8&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;green&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#157E15&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;name&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;QB64 Super Dark Blue&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;purple&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#631563&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;red&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#98220E&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;selectionBackground&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#00586C&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;white&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#989898&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;yellow&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;#808000&amp;#34;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And mapped it to a set of X resources for &lt;code>~/.Xresources&lt;/code>, which looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">!! QB64 Super Dark Blue
XTerm*background: #000027
XTerm*foreground: #D8D8D8
XTerm*color0: #000000
XTerm*color1: #98220E
XTerm*color2: #157E15
XTerm*color3: #808000
XTerm*color4: #054663
XTerm*color5: #631563
XTerm*color6: #00485C
XTerm*color7: #989898
XTerm*color8: #626262
XTerm*color9: #D8624E
XTerm*color10: #55CE55
XTerm*color11: #FFA700
XTerm*color12: #457693
XTerm*color13: #934593
XTerm*color14: #00586C
XTerm*color15: #D8D8D8
XTerm*cursorColor: #D8D8D8
XTerm*highlightColor: #00586C
&lt;/code>&lt;/pre>&lt;/div>&lt;p>With that, I have a more pleasantly-looking X configuration on this little old machine, from which I&amp;rsquo;m typing this draft post.&lt;/p>
&lt;figure>
&lt;img src="/images/2022-04-20-qb64-powerbook.jpg" width="100%" style="max-width: 1024px"/>
&lt;figcaption>PowerBook G4 12" running a few xterms using my adaptation of the QB64 Super Dark Blue theme.&lt;/figcaption>
&lt;/figure>
&lt;p>Enjoy! And I hope to get nostalgic again when I encounter this post many years down the line ðŸ˜Š.&lt;/p></description></item><item><title>Introducing db_logger</title><link>https://jmmv.dev/2022/04/introducing-db-logger.html</link><pubDate>Tue, 12 Apr 2022 08:30:00 -0700</pubDate><guid>https://jmmv.dev/2022/04/introducing-db-logger.html</guid><description>&lt;p>Over the last couple of weeks, I have been modernizing the codebase of the &lt;a href="/2021/07/endbasic-0.7.html">EndBASIC cloud service&lt;/a> by applying many of the learnings I got from the &lt;a href="/2022/02/diy-web-analytics.html">development of EndTRACKER&lt;/a>. The latter was a fork of the former and thus the foundations were the same, but as I iterated on the latter more recently, I got to refine my approach to writing a REST API in Rust.&lt;/p>
&lt;p>During this refactoring process, there was a small piece of the system that routinely got in the way for various reasons. This piece was the &amp;ldquo;database logger&amp;rdquo;.&lt;/p>
&lt;p>You see, before shipping the EndBASIC service, I wanted to ensure I had the ability to troubleshoot any problems thar arose after the launch. This meant having easy access to the verbose logs that many Rust crates emit&amp;mdash;something that&amp;rsquo;s not easily doable from an Azure Function. I researched this topic a bit more and found that Azure provides its own native logging facilities, but I did not want to integrate with them: the process seemed complex without having first-class SDK support, the service seemed overkill for my needs, and I did not want to tie my code to a specific cloud provider too much.&lt;/p>
&lt;p>So, instead, and as a fun (and frustrating) exercise, I chose the path of implementing a custom logging facility. I wrote a module to hook into Rust&amp;rsquo;s de-facto standard logging library (the &lt;a href="https://crates.io/crates/log">log create&lt;/a>) and save any logs emitted into a PostgreSQL database or an SQLite database.&lt;/p>
&lt;p>From the inception of this logging code, I thought of making it public as its own crate, but I felt that it was unnecessary at the time. But because it was now getting in the way of my refactoring attempts, I decided to take this step as a way of &amp;ldquo;hiding the complexity elsewhere&amp;rdquo;. And thus &lt;code>db_logger&lt;/code> was born last week.&lt;/p>
&lt;p>To create the &lt;code>db_logger&lt;/code>, I ripped out the logging code from the EndBASIC service verbatim. I could have published it &amp;ldquo;as it was&amp;rdquo;, but there were many rough edges that were bothering me. So I spent last week cleaning the code up and trying to conquer some of the refactoring difficulties I previously encountered. I finally succeeded at that after a few head-scratching mornings, so expect a future blog post on the topic of &amp;ldquo;public traits and leakage of internal types&amp;rdquo;.&lt;/p>
&lt;p>Things are not perfect yet and there are many limitations, but I feel that the code is now in a reasonably good shape to cut a first release. Be aware that this crate might be a &amp;ldquo;dead end&amp;rdquo; and thus there might not be many more releases after this: I would have approached logging very differently last year if I knew what I know now, and I might pursue those alternate approaches next. That said, I can imagine &lt;code>db_logger&lt;/code> being useful to some other people already.&lt;/p>
&lt;p>&lt;strong>Head to &lt;a href="https://github.com/jmmv/db_logger">https://github.com/jmmv/db_logger&lt;/a> for more details!&lt;/strong>&lt;/p></description></item><item><title>Abandoning GAFYD</title><link>https://jmmv.dev/2022/03/abandoning-gafyd.html</link><pubDate>Mon, 28 Mar 2022 09:00:00 -0700</pubDate><guid>https://jmmv.dev/2022/03/abandoning-gafyd.html</guid><description>&lt;p>If you know what GAFYD stands for and have felt that signing up for a free account years ago was a mistake, you know you are in trouble right now. Those accounts are shutting down. The time has come to either pay up or move out, and you should decide what to do ASAP. It took me weeks of active effort to move my and my family&amp;rsquo;s data out of GAFYD and put it back into Google consumer accounts and other non-Google services.&lt;/p>
&lt;p>This post is essentially a recollection of my lab notes on what I did. The text is long and haphazard due to the many things to cover. I&amp;rsquo;ll start with a brief explanation of what GAFYD was, what&amp;rsquo;s happening now, and what&amp;rsquo;s wrong with it. After that, I&amp;rsquo;ll sketch what my new setup is and how I succeeded in moving some of the more complex services. Here is the outline:&lt;/p>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#what-is-and-whats-up-with-gafyd">What is and what&amp;rsquo;s up with GAFYD?&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#borderline-evil">Borderline evil?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#key-problem-online-identity">Key problem: Online identity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#moving-out">Moving out&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#domains">Domains&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gmail">GMail&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contacts">Contacts&lt;/a>&lt;/li>
&lt;li>&lt;a href="#drive">Drive&lt;/a>&lt;/li>
&lt;li>&lt;a href="#voice">Voice&lt;/a>&lt;/li>
&lt;li>&lt;a href="#other-services">Other services&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#disabling-gafyd">Disabling GAFYD&lt;/a>&lt;/li>
&lt;li>&lt;a href="#whats-next">What&amp;rsquo;s next?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;h1 id="what-is-and-whats-up-with-gafyd">What is and what&amp;rsquo;s up with GAFYD?&lt;/h1>
&lt;p>GAFYD, or Google Apps for your Domain, is the old name for what&amp;rsquo;s currently known as &lt;del>G Suite&lt;/del> Google Workspace. In essence, this feature allows your Google account identifier (an email address) to be an address under your own DNS domain. Google has allowed this for a while already without GAFYD, but the key difference is that GAFYD allows your GMail account to &lt;em>be&lt;/em> such an email address as well.&lt;/p>
&lt;p>When GAFYD launched in 2006, it included a free tier with support for up to 5 users. This was the perfect choice for a family&amp;mdash;because you could use addresses like &lt;code>first.name@last.name.com&lt;/code>&amp;mdash;and, if I recall correctly, was advertised as such. Many people took that path, including myself in 2010 with a domain for my blog and later in 2012 with the &lt;code>meroh.net&lt;/code> domain for the family when my first kid was born.&lt;/p>
&lt;p>Unfortunately, GAFYD&amp;rsquo;s free offering was dropped a bit later in 2012. Since then, Google kept old free accounts intact through the many product renames that GAFYD went through. But &lt;a href="https://killedbygoogle.com/">like the many products that Google has dropped&lt;/a>, I think we all knew that this benefit would disappear. A telltale was that GAFYD accounts &lt;em>looked like&lt;/em> consumer accounts when they really weren&amp;rsquo;t and they had some annoying limitations for personal usage. For example: Google lost me as a potential Google Music paid customer when I couldn&amp;rsquo;t sign up for a family plan and they showed me the finger again when I tried to set up a Nest.&lt;/p>
&lt;p>What we GAFYD free users feared is finally happening. At the beginning of the year, Google announced that it would be &lt;em>updating&lt;/em>&amp;mdash;a Google-internal euphemism for deprecating&amp;mdash;Google Workspace by turning all grandfathered free accounts into paid ones. To be honest, there is not much wrong with that. Google is under no obligation to offer Google Workspace for free to anyone for any reason, and they had given us almost 10 extra years of free service when they could just have shut this off in 2012.&lt;/p>
&lt;p>The problem, however, is that these GAFYD family domains have encroached people&amp;rsquo;s lives. Even if you wanted to &amp;ldquo;escape&amp;rdquo; these accounts&amp;mdash;as users have tried in the past and as I had been meaning to do for years&amp;mdash;there was and still is no easy way out. Yes, you can use Google Takeout to export your data but then&amp;hellip; what? There is no way to import the Takeout export into another (consumer) account and some parts of your data&amp;mdash;Google Play purchases, for example&amp;mdash;are simply lost.&lt;/p>
&lt;p>Of course, this is only a &amp;ldquo;problem&amp;rdquo; if you don&amp;rsquo;t want to keep/pay for Google Workspace. If you are happy with this service and its current offering, by all means pay and enjoy. But I, like many others, would be perfectly happy to drop our custom domain and fall back to a consumer account. I don&amp;rsquo;t &lt;em>want&lt;/em> Google Workspace for my family and being forced into it feels like extortion.&lt;/p>
&lt;h2 id="borderline-evil">Borderline evil?&lt;/h2>
&lt;p>When Google announced their intentions in January to terminate the free GAFYD offering, they did not provide any recourse for people to easily move out. It was either pay up or close your account and deal with any losses. If we look at the &lt;a href="https://support.google.com/a/answer/60217#data">Upgrade from G Suite legacy free edition&lt;/a> FAQ (emphasis mine):&lt;/p>
&lt;blockquote>
&lt;p>To maintain your services and accounts, review the information below and upgrade by &lt;strong>May 1, 2022&lt;/strong>. Upgrading to a Google Workspace subscription is a seamless transition for all customers currently on the G Suite legacy free edition. Select the Flexible payment plan when you upgrade to a Business edition and use your new subscription at no cost until at least July 1, 2022.&lt;/p>
&lt;/blockquote>
&lt;p>Things have changed slightly since the &lt;a href="https://news.ycombinator.com/item?id=29996432">huge backslash&lt;/a> that the original announcement prompted. The FAQ now contains the following as well (emphasis mine here too):&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>If I donâ€™t want to upgrade to a paid subscription, can I keep or transfer my data?&lt;/strong>&lt;/p>
&lt;p>In the coming months, we&amp;rsquo;ll provide an option for you to move your non-Google Workspace paid content and most of your data to a no-cost option. This new option won&amp;rsquo;t include premium features like custom email or multi-account management. &lt;strong>You&amp;rsquo;ll be able to evaluate this option prior to July 1, 2022 and prior to account suspension.&lt;/strong> We&amp;rsquo;ll update this article with details in the coming months.&lt;/p>
&lt;/blockquote>
&lt;p>Great; there will be a migration option. But&amp;hellip; this feels to come straight from the evil department? First of all: how could Google not anticipate the huge backslash they would cause considering that it was mostly tech enthusiasts who first signed up for GAFYD? How come they didn&amp;rsquo;t think of an off-boarding plan &lt;em>upfront&lt;/em>? Google has gone through dozens of turndowns for years and it&amp;rsquo;s unbelievable that they haven&amp;rsquo;t yet learned this lesson. Offering a way out from the get go would have gone a &lt;em>really&lt;/em> long way towards making this a non-issue. I&amp;rsquo;d have simply &amp;ldquo;downgraded&amp;rdquo; my account and moved on.&lt;/p>
&lt;p>Also note that the dates above don&amp;rsquo;t line up. You have to take action before May 1st but then have until July 1st to move out? Hmm, how is that going to work? Or: will it work at all? It has been years since &lt;del>Google employees&lt;/del> users have asked for tools to move out of GAFYD so I don&amp;rsquo;t have a ton of confidence that a functional offering will be built in just a few months.&lt;/p>
&lt;p>I am certainly not going to wait and risk it. I wasn&amp;rsquo;t going to wait in January when I read the first announcement and I am not going to see what happens. You shouldn&amp;rsquo;t wait either: as mentioned earlier, it took me various weeks of hands-on work to free my and my family&amp;rsquo;s data. Plus imagine what will happen if you lose access to your primary email account for just a few days&amp;hellip; Better take action while you are not in a rush.&lt;/p>
&lt;h1 id="key-problem-online-identity">Key problem: Online identity&lt;/h1>
&lt;p>Moving from a &lt;code>@gmail.com&lt;/code> &lt;em>email&lt;/em> address to an address under my own domain was a great move when I did it back in 2012. Using a custom email address is what allows one to remain provider-agnostic and, in theory, change providers with ease. But this is only in theory because it works for email only.&lt;/p>
&lt;p>The key mistake and most problematic aspect of using GAFYD for a family is that &lt;em>the domain email address &lt;strong>is&lt;/strong> your Google account&lt;/em>, not just GMail&amp;mdash;and Google offers tons of services. This means that you cannot simply create a consumer GMail account, move your mail, use a forwarding service for your domain address, and call it a day. No: you must transition all of your data and content from the many services behind your GAFYD account to the consumer account.&lt;/p>
&lt;p>This is a pain for Google-owned services and can be very problematic with third-party services. If you have used the &lt;strong>Sign in with Google&lt;/strong> feature across the web, your Google account is &lt;em>also&lt;/em> your account in all those services. Those services will have shadow accounts in their databases linking you to your Google SSO credentials, and breaking those chains may be impossible. It is really up to these services to let you re-link your account to a different identity, and not all of them do.&lt;/p>
&lt;p>Compare this to Microsoft&amp;rsquo;s O365 personal and family accounts. For these, you start by creating a consumer account &lt;em>and then&lt;/em> you can add your domain address as an &lt;em>alias&lt;/em> for it. This lets you sign into the O365 services and use them with your domain address, but your domain address &lt;em>is not&lt;/em> your account. And if you are a family, you simply pay for a family plan from one of the accounts and then link the other consumer accounts into it. All accounts are still primarily &lt;code>@outlook.com&lt;/code> accounts. This is not the same as using GAFYD for the whole family, but I now believe it&amp;rsquo;s a better model with fewer chances of lock in.&lt;/p>
&lt;h1 id="moving-out">Moving out&lt;/h1>
&lt;p>My plan to move out of GAFYD was &amp;ldquo;simple&amp;rdquo;: manually transfer as much data from the GAFYD accounts to some other place and then rely on Google Domains email forwarding feature to keep our domain email addresses working. I could play this card because we barely had any paid content and the kids' accounts were just a gimmick.&lt;/p>
&lt;p>So, what are those &amp;ldquo;other places&amp;rdquo;? For certain services like Domains and Voice, I kept Google. I recovered our old consumer accounts and transferred our data into them. For other services like Photos and Drive, I opted to make my FreeBSD-based NAS server the primary copy. And for productivity services like email and calendar, I opted to move to a paid Microsoft O365 family plan.&lt;/p>
&lt;p>Wait, so I&amp;rsquo;m trading GAFYD for O365? And I&amp;rsquo;m paying for the latter now? How is this different than paying for GAFYD in the first place? Well&amp;hellip; many reasons. I get a very generous discount for O365 from work so it&amp;rsquo;s hard to pass on it. The kids have an Xbox and enjoy the Office apps. As I have already explained, I don&amp;rsquo;t &lt;em>want&lt;/em> the Google Workspace features. And, more importantly, the O365 paid accounts are easy to move out of: they can be downgraded to unpaid accounts and because their &lt;code>@outlook.com&lt;/code> addresses are hidden behind email forwarding, I can reroute email when necessary.&lt;/p>
&lt;p>Let&amp;rsquo;s now see how I could deal with the &lt;em>many&lt;/em> services that Google offers.&lt;/p>
&lt;p>Note that the scariest part of the whole move out process is that there is a chance to break inbound email. Losing access to &lt;em>receiving&lt;/em> email via the domain address could have been a &lt;em>disaster&lt;/em> given that this is how all services across the web know me/us and how I get 2FA tokens for many of them, so my plan was organized to ensure this remained true.&lt;/p>
&lt;h2 id="domains">Domains&lt;/h2>
&lt;p>Let&amp;rsquo;s start with Google Domains first because this plays a key role in the transition, especially thanks to its email forwarding features.&lt;/p>
&lt;p>I have bought a few domains from Google Domains. I like their offering and prices and have no good reason to move elsewhere. All of these domains, including the one for GAFYD, were owned by the GAFYD administrator account so I had to move them out to my consumer account.&lt;/p>
&lt;p>The process was trivial:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Add the consumer identity as an owner of each domain.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Update the payment methods to point to the consumer account.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Remove the GAFYD identity from each domain.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Ta-da! Easy but anxiety-inducing. If you have more than one domain, start with the one that&amp;rsquo;s less &amp;ldquo;risky&amp;rdquo; to you if it stops working.&lt;/p>
&lt;h2 id="gmail">GMail&lt;/h2>
&lt;p>Moving email has been the hardest and scariest part of this whole ordeal.&lt;/p>
&lt;p>The move-out plan looks like this:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Disconnect all &amp;ldquo;imported accounts&amp;rdquo; from the GAFYD account, if any. I was using this feature to receive stray email sent to my consumer account from the GAFYD GMail account. Years after abandoning the consumer account, it still received email&amp;hellip; which amusingly was almost-exclusively recruiter spam. I suspect my supposedly-unused address is in some old LinkedIn data dump but don&amp;rsquo;t understand why the domain address never made it into them.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Enable POP3 support in the GAFYD account and configure it to allow fetching all email and to &lt;em>delete&lt;/em> downloaded email from the server.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Configure the consumer account to import email from the GAFYD account. This is under the &lt;strong>Accounts and Import&lt;/strong> settings panel and is the feature called &lt;strong>Check mail from other accounts&lt;/strong>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To speed things up, I took the chance to clean up my email archive. I noticed that my account had thousands upon thousands of messages I did not &lt;em>need&lt;/em> in my account. The vast majority were mailing lists messages that are already archived elsewhere and that I simply didn&amp;rsquo;t need to keep around. Mass-deleting them helped quite a bit.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Optional: Configure the GAFYD GMail account &lt;em>and&lt;/em> the consumer account to forward all incoming email to the non-Google address (Outlook.com in my case) and to delete incoming email. No need to do this if you&amp;rsquo;ll continue to use the consumer account.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Make sure to test that sending emails to yourself (from non-Google accounts if at all possible) via the domain address still works.&lt;/p>
&lt;p>Don&amp;rsquo;t forget that GMail supports plus-addressing. If you have ever used that feature (&lt;a href="/2015/06/get-a-handle-on-email-subscriptions.html">like I did&lt;/a>), double-check that plus-addressed email is properly forwarded to your desired target address. Using plus-addresses has caused me more trouble than benefit over the years though and, out of paranoia, I went and got rid of my plus-addresses from the most critical services (like banks).&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>After the steps above, the email archive in the GAFYD account will start transferring to your consumer account. This process will take &lt;em>days&lt;/em> or &lt;em>weeks&lt;/em> depending on how much email you have. And once the data transfer completes, the GAFYD GMail account should not be &amp;ldquo;operational&amp;rdquo;. Its archive should be empty and any new email that lands on it will immediately leave and go elsewhere.&lt;/p>
&lt;p>But the GAFYD GMail account cannot yet be eliminated because it&amp;rsquo;s still in the inbound path to receive email: your domain&amp;rsquo;s &lt;code>MX&lt;/code> records still point to the GAFYD services. We&amp;rsquo;ll cut those later by using the Google Domains' email forwarding feature.&lt;/p>
&lt;p>For the kids' accounts, who did not have consumer GMail accounts, I decided to just create Outlook.com accounts instead. To move their limited email archive, I set up the Outlook desktop app with both accounts (their GAFYD account and their Outlook.com account) at once and did a manual move of all of their email. One issue here is that the &lt;strong>All Mail&lt;/strong> folder does &lt;em>not&lt;/em> show up under &lt;code>[Gmail]&lt;/code> in Outlook even if you explicitly subscribe to it. The trick to make it show up is to &lt;em>create&lt;/em> a folder named &lt;code>All Mail&lt;/code> under &lt;code>[Gmail]&lt;/code> from within Outlook; after that, all email will sync and you&amp;rsquo;ll be able to move it.&lt;/p>
&lt;p>One pending task I have in this area is to set up &lt;a href="http://www.offlineimap.org/">OfflineIMAP&lt;/a> to download a copy of my email archive from GMail and then delete it from Google&amp;rsquo;s servers.&lt;/p>
&lt;h2 id="contacts">Contacts&lt;/h2>
&lt;p>Transferring contacts between the GAFYD and Google consumer accounts was easy. I went into the GAFYD accounts, generated a CSV export, and then imported those back into the consumer accounts. I do not &lt;em>need&lt;/em> the contacts to live in Google because email is moving to O365, so this was primarily for peace of mind.&lt;/p>
&lt;p>The problem came when trying to import said CSV dataset into Outlook.com. Mind you, Google offers an option to generate an Outlook-compatible CSV export, but feeding that into Outlook.com simply errors out. I spent quite some time figuring out what might be wrong in the CSV export. I suspected that the various Korean contacts I had might be problematic because of encoding reasons. I tried importing the CSV into Excel (which required adding a BOM to the UTF-8 file upfront), reexporting it, and using that second export in Outlook.com without luck.&lt;/p>
&lt;p>Eventually, I tried to &amp;ldquo;shorten&amp;rdquo; the export by trimming the CSV file to just a few people that did not have any non-ASCII characters in their data. And&amp;hellip; the import still failed! Or did it? After running the import into Outlook.com from the web and seeing an error, I unintentionally hit &amp;ldquo;Refresh&amp;rdquo; and&amp;hellip; all the contacts were there. Somehow, the UI errors out but the import completes anyway when done in small batches. So that&amp;rsquo;s what I ended up doing: curating the CSV files with batches of about 50 contacts and importing those one by one. Not&amp;hellip; great.&lt;/p>
&lt;h2 id="drive">Drive&lt;/h2>
&lt;p>Transferring Drive data has two sides: one that is easy and one that is not.&lt;/p>
&lt;p>On the easy side we have all non-Drive native files uploaded into Drive. These files can be trivially downloaded (from the UI or with the desktop Drive client) and then reimported into the consumer account if you so desire. I had hundreds of PDF files in there from document scans and I simply rsync-ed them to my NAS by relying on the Drive desktop virtual file system.&lt;/p>
&lt;p>On the difficult side we have the Drive-native documents. These documents' authoritative format is something proprietary that Google keeps in their internal databases. There is no way for you to download the originals of these documents because, well, you wouldn&amp;rsquo;t be able to do anything with those originals anyway if you did not use the web apps. Exporting those documents to other formats was an option, but exports are not originals. So I opted to move the documents to the consumer account.&lt;/p>
&lt;p>As a first attempt, I just shared all documents with the consumer account. Once they were shared, I went again into the sharing settings and tried to upgrade the permissions of the consumer account to &lt;strong>Owner&lt;/strong>. As you can imagine, this failed: Drive complained that I could not give ownership to people outside the organization. (I&amp;rsquo;m sure this worked in the past before GAFYD turned into a more business-only feature.) Uh oh.&lt;/p>
&lt;p>At this point, what I did was log into the Drive consumer account, right-click on all files shared from the GAFYD account, and then select &lt;strong>Make a copy&lt;/strong>. This is not the same as a document move because permissions and timestamps are lost, but who cares. I did not have enough documents to be worried about this. The other annoying part of this was going over each file and removing the &lt;code>Copy of&lt;/code> prefix that was added to their names. If you use the Drive desktop client, you could use something like &lt;a href="https://docs.microsoft.com/en-us/windows/powertoys/powerrename">PowerRename&lt;/a> to do this trivially though.&lt;/p>
&lt;h2 id="voice">Voice&lt;/h2>
&lt;p>Google Voice was easy to transfer to my consumer account, but I was tempted to instead move completely out of Voice.&lt;/p>
&lt;p>The reason is that Voice has caused me more trouble over the years than benefit: inbound calls seem to drop too soon (to the point where sometimes the phone doesn&amp;rsquo;t even ring), integration with iPhone is almost non-existent, and some online verification services refuse to deliver to these numbers. But&amp;hellip; the ability to receive texts and voicemails while traveling abroad has been very useful in the past, and I&amp;rsquo;m not sure I want to lose that.&lt;/p>
&lt;p>As I decided to remain with Google Voice for now, moving the account was a matter of a couple of clicks on the web UI. Those clicks were routed through old-style dialogs though, which shows that Voice is one of those products at risk of cancellation for lack of upgrades&amp;hellip; but anyway.&lt;/p>
&lt;p>I took the chance to delete tons of old garbage. It was kinda cool to see text messages I sent years ago, but those were intended to be ephemeral. Keeping them in an online service is something that can backfire so I just wiped everything. More on this later.&lt;/p>
&lt;h2 id="other-services">Other services&lt;/h2>
&lt;p>As alluded to earlier, GAFYD encompasses more than just email or the services I touched upon above. There were many more I had to deal with, but the processes were easy enough. Here is a summary:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Calendar:&lt;/strong> This was probably the simplest of all. I generated an ICS export of the Calendar data from the GAFYD account, went into the Outlook.com calendar, imported it and&amp;hellip; voila.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>YouTube:&lt;/strong> I did not have any content in YouTube (no videos, no comments) and only a few channel subscriptions so I just proceeded to ignore any takeout operations. Losing the YouTube history was probably a good thing too.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Analytics:&lt;/strong> &lt;a href="/2022/02/diy-web-analytics.html">I built my own&lt;/a> for unrelated reasons. Enough said.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Play:&lt;/strong> I only had a few digital purchases in my account: some Android apps and a movie (yes, just one). I do not use Android any longer and didn&amp;rsquo;t care about the rest, so I chose to ignore this. I realize that Play paid content may be a real problem for people that have built extensive digital collections over time and I hope this is where the upcoming tooling to abandon GAFYD will help. Good luck there.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Keep:&lt;/strong> Google&amp;rsquo;s data hoarding approach shows up everywhere. I was surprised to find here tons of old notes that I had archived instead of deleted. Those were transient notes that should not have stuck around, but they did for no good reason. I proceeded to delete most of them and, for the few notes that are important/useful in any way, I manually copy/pasted them across accounts given that there was no other option.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cloud&lt;/strong> and &lt;strong>Firebase:&lt;/strong> I had created a couple of test apps in these services so I just deleted them.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Photos&lt;/strong>: This service has been nice at times but it has never hosted my primary, full-resolution copy of my photos. I&amp;rsquo;ve only used this as a backup for the photos taken with my mobile phone; the originals have also stayed with me in local drives. Off-boarding from this service was easy, as all I had to do was to delete all online copies. I have not re-uploaded them to the consumer account.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Maps:&lt;/strong> I had a few custom maps and lots of saved places in my account. I could not find of a way to migrate those to my consumer account. I backed everything up using Takeout but it&amp;rsquo;s probably a worthless backup for a later reimport&amp;hellip; a pity, but not a big loss.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Activity history:&lt;/strong> This includes Location History, Web Activity, YouTube History, and Ad Personalization. Wow. What a trove of personal data. Very cool to see where I have been over the years, physically and virtually&amp;hellip; but not something I particulary want Google (or anyone, including myself!) to know. I wiped everything and made sure that my consumer account had all of these data collection options &lt;em>off&lt;/em> from now on.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>SSO accounts:&lt;/strong> This might be the biggest problem if you have used the &lt;strong>Sign in with Google&lt;/strong> feature as explained earlier. You&amp;rsquo;ll have to go into these apps one by one and see if you can reconnect them to other IDs. I had barely used this feature precisely because I did not want a single company to hold the keys to all my online services&amp;hellip; but you may not be as lucky if you made a different choice in the past.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="disabling-gafyd">Disabling GAFYD&lt;/h1>
&lt;p>After two weeks of active work on all of the above, I &lt;a href="/2022/03/a-year-on-windows-intro.html">got distracted&lt;/a> and didn&amp;rsquo;t come back to this project for almost a month. This was actually a good thing because it let me ensure that I did &lt;em>not&lt;/em> need to log back into my GAFYD account for anything. My move-out process was almost complete.&lt;/p>
&lt;p>The time for the scariest step (the one that risked breaking email) was here. It was time to remove the Google Workspace services from my domain. But by &amp;ldquo;remove&amp;rdquo;, I wasn&amp;rsquo;t sure what would happen. The removal button was there in plain sight within Google Domains, but I could not anticipate what the consequences of clicking it would be.&lt;/p>
&lt;p>Before doing that, I went into the Google Domains console and set up email forwarding addresses for all of us to point to the new Outlook.com addresses. When doing so, the Domains console warned me that forwarding wouldn&amp;rsquo;t work until I disabled Workspace and offered me an extra button to do that. This gave me some more confidence because it felt that this workflow had been properly thought out.&lt;/p>
&lt;p>Regardless, clicking that button was scary. I manually backed up the DNS records for the Workspace configuration, re-double-verified that email delivery to a forwarded address worked (by testing this out with another domain I have), and then clicked the button. After this happened, I confirmed with &lt;code>dig&lt;/code> that the MX records were updated&amp;mdash;but they still had a TTL of 1 hour so there was still a risk that something would stop working.&lt;/p>
&lt;p>I patiently waited for an hour and tested sending myself an email. And it was routed properly.&lt;/p>
&lt;h1 id="whats-next">What&amp;rsquo;s next?&lt;/h1>
&lt;p>I&amp;rsquo;m now GAFYD free! My grandfathered Google Workspace free plan, however, is still alive.&lt;/p>
&lt;p>Given the promise of tools to move our data out of the GAFYD accounts, I&amp;rsquo;ll wait to delete these accounts until the last moment. I want to see if those tools will let me handle the few loose ends I left behind (like Play content). I&amp;rsquo;m not too hopeful though&amp;mdash;especially now that I&amp;rsquo;ve moved most of my data my hand because I suspect this will confuse whatever automation Google builds.&lt;/p>
&lt;p>Lastly, before departing, a couple of thoughts:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Google Takeout is great in theory but pretty terrible in practice. The fact that a Takeout export cannot be used to import the same data into another Google account is sad&amp;mdash;it&amp;rsquo;s called Takeout and not Backup for a reason, I guess&amp;mdash;and the format of the exported data is lossy and extremely hard to make sense of.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Google is a data hoarder, which is obviously not a surprise to absolutely anyone. But looking at what data Google has collected about you is&amp;hellip; interesting. In the process of pruning my GAFYD account, I found tons of details of my past that, while cool to see, seem very dangerous to keep around. I chose to delete most of these data and keep whatever is important with me (in my NAS) instead of in the cloud. I&amp;rsquo;m sure that whatever profile Google has about me won&amp;rsquo;t be impacted by my data deletion&amp;mdash;but if my account ever gets compromised, it&amp;rsquo;ll be less scary.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Good luck freeing yourself from GAFYD!&lt;/p></description></item><item><title>A year on Windows: Finale</title><link>https://jmmv.dev/2022/03/a-year-on-windows-finale.html</link><pubDate>Sat, 19 Mar 2022 07:20:00 -0700</pubDate><guid>https://jmmv.dev/2022/03/a-year-on-windows-finale.html</guid><description>&lt;p>We have reached the end of this daily 2-week long series so it&amp;rsquo;s time to close with some parting thoughts. Before drafting some conclusions on how this whole year has gone, there is just one more topic I have to touch on&amp;hellip; and that&amp;rsquo;s the much dreaded telemetry.&lt;/p>
&lt;p>At its core, I do not think telemetry per se is a bad thing. &lt;em>Anonymous&lt;/em> details on how a product is used in the real world&amp;mdash;in this case, a product used by literally millions of people&amp;mdash;are extremely useful during planning and development. As a developer, I have experienced this at a much smaller scale when working on Blaze (&lt;em>not&lt;/em> Bazel), and having details on build performance company-wide was invaluable to plan what we had to improve.&lt;/p>
&lt;p>The problems start when telemetry data isn&amp;rsquo;t anonymous, and what Windows collects at its highest (default) settings may not be. For example, the most-comprehensive telemetry settings enable sending crash dumps to Microsoft and those can definitely include personal information based on the nature of what they are. If privacy is a concern to you, you might want to dial those settings down to the basics.&lt;/p>
&lt;p>Other than potential privacy violation issues, what irks me about telemetry is that &amp;ldquo;improving&amp;rdquo; a product can mean &amp;ldquo;removing features&amp;rdquo;. And I fear that telemetry data could be used to justify removing options like &amp;ldquo;icons on the left&amp;rdquo; in the new taskbar as I &lt;a href="/2022/03/a-year-on-windows-gui.html">previously mentioned&lt;/a>. But who knows. Telemetry data could also be used to justify &lt;em>keeping&lt;/em> this feature as opposed to blindly removing it.&lt;/p>
&lt;p>Anyhow. There are other features about modern Windows that concern me much more, and these are the sponsored apps that randomly appear in the Start menu, the attempts to push &lt;a href="https://www.theverge.com/2022/3/15/22979251/microsoft-file-explorer-ads-windows-11-testing">ads into the desktop&lt;/a>, and the constant nagging for attention from both the system and most apps nowadays. These are important issues that could (will?) push me off the platform. Part of my &amp;ldquo;secret plan&amp;rdquo; in abandoning macOS has been getting used to how things work outside of a Mac&amp;mdash;in particular, &lt;a href="/2022/03/a-year-on-windows-shortcuts.html">keyboard shortcuts&lt;/a>&amp;mdash;to open the door to using Windows&amp;hellip; and also going back to Unix-y open-source desktops if I must.&lt;/p>
&lt;p>Leaving these issues aside, I want to clarify that my overall experience over the last year using Windows alone has been on the positive side. I have gotten reasonably productive using the system, I do not miss macOS nor Mac hardware much, and I have &lt;a href="/2022/03/a-year-on-windows-networked-fs.html">gained a FreeBSD NAS&lt;/a> on the side. That doesn&amp;rsquo;t mean I wouldn&amp;rsquo;t love to have a Mac Studio though ðŸ˜œ.&lt;/p>
&lt;p>As I wrap up, I realize I have forgotten to talk about some other interesting topics like how neat Windows Hello is or how Windows Defender impacts day-to-day machine usage. I&amp;rsquo;ll have to leave those for another time because running this series as a daily publication has been &lt;em>exhausting&lt;/em>. What started as a 20-minute long incoherent draft has turned into a 13-post long series that apparently total an hour of reading time. Targeting daily posts was&amp;hellip; ambitious, but I wanted to get these out of the way because I have some other unrelated posts in the queue.&lt;/p>
&lt;p>Lastly, &lt;a href="https://twitter.com/jmmv/status/1500490263835275267?s=20&amp;amp;t=S4hC0HXc6-WzFgiiUIvx7Q">as I predicted&lt;/a>, running this content as a series might not have been the greatest idea from an engagement perspective. Of the 13 posts in total, &lt;a href="/2022/03/a-year-on-windows-debugging.html">only one&lt;/a> has seen noticeable readership. So if you have a minute and have enjoyed reading these articles, could you help me share them a little bit more? ðŸ˜Š Here is the summary:&lt;/p>
&lt;ul>
&lt;li>&lt;b>2022-03-07&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-intro.html">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-08&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-shortcuts.html">Keyboard shortcuts&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-09&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-input.html">Input methods&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-10&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-gui.html">Look&amp;#39;n&amp;#39;feel&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-11&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-alt-tab.html">Window switching&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-12&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-powertoys.html">PowerToys&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-13&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-tools.html">Miscellaneous tools&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-14&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-devel.html">Development experience&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-15&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-powershell.html">PowerShell&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-16&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-networked-fs.html">Networked file systems&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-17&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-debugging.html">System debugging&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-18&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-winget.html">Software installation&lt;/a>&lt;/li>
&lt;li>&lt;b>2022-03-19&lt;/b>: &lt;a href="/2022/03/a-year-on-windows-finale.html">Finale&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Thank you for reading!&lt;/p></description></item><item><title>A year on Windows: Software installation</title><link>https://jmmv.dev/2022/03/a-year-on-windows-winget.html</link><pubDate>Fri, 18 Mar 2022 06:30:00 -0700</pubDate><guid>https://jmmv.dev/2022/03/a-year-on-windows-winget.html</guid><description>&lt;p>Can you believe that Windows ships with something that looks like a package manager? By default since Windows 11? I know, right? Let&amp;rsquo;s take a look.&lt;/p>
&lt;p>Installing software on Windows has typically been a very different experience than what you get in a typical Linux or BSD system. The reason, I think, is simple: most Windows software was distributed via physical media (floppies, CDs and DVDs) and most of it was commercial. Such software was very slow to install, so the vendors used the installer&amp;rsquo;s screen space to hype up ther product features. Take a look at just one example:&lt;/p>
&lt;figure>
&lt;img src="/images/2022-03-18-word-6-setup.png" width="100%" class="with-border" style="max-width: 640px"/>
&lt;figcaption>Microsoft Word 6.0 Setup touting the AutoFormat feature while it copies files. A fairly typical installation program from the days of Windows 3.x.&lt;/figcaption>
&lt;/figure>
&lt;p>Software distribution has changed quite a bit with commonplace broadband connections, but the mess of custom installers remains. Having to visit arbitrary web sites to download installers and going through their snowflake wizards one by one is time consuming. Plus it is risky if you don&amp;rsquo;t pay attention to where you are downloading those executables from. People will type &lt;code>nvidia drivers&lt;/code> in a search engine or, worse, in a file sharing service (I&amp;rsquo;ve seen that), download the very first thing that shows up, and execute the binary without a second thought. Ta-da! Malware successfully acquired.&lt;/p>
&lt;p>Furthermore, some of those installers remain as obnoxious as ever. I&amp;rsquo;m looking at you, graphics drivers. I don&amp;rsquo;t understand why something as &amp;ldquo;simple&amp;rdquo; as that must pollute the whole machine, ranging from the notification area in the taskbar to the context menus on the desktop, passing by system-wide keyboard shortcuts. Obviously, their installers are equally &amp;ldquo;on your face&amp;rdquo;. But anyway&amp;hellip;&lt;/p>
&lt;p>Microsoft tried to fix this by introducing the Microsoft Store and UWP apps. These apps are &amp;ldquo;secure-er&amp;rdquo; and are all installed in a common way via the Store desktop app&amp;hellip; which is great, but whether an app is available in the Store or not is a hit or miss. If you want to do any &amp;ldquo;real work&amp;rdquo; on your computer&amp;mdash;&lt;a href="https://support.microsoft.com/en-us/windows/windows-10-and-windows-11-in-s-mode-faq-851057d6-1ee9-b9e5-c30b-93baebeebc85">S mode&lt;/a> didn&amp;rsquo;t last even a day on my Surface Go 2&amp;mdash;you will have to break free from the Store and go back to using scary-looking installer executables. Or do you?&lt;/p>
&lt;h1 id="chocolatey">Chocolatey&lt;/h1>
&lt;p>There have been unofficial package managers for Windows for quite a while. Chocolatey, which launched in 2011, is the prime example. I&amp;rsquo;ve used it since I got into Windows a year ago because the convenience of doing &lt;code>choco install whatever&lt;/code> vs. the process I described earlier is very valuable. Additionally, if you opt into using the PowerShell startup scripts that Chocolatey provides, the tools you install will seamlessly be available in the &lt;code>PATH&lt;/code>, so after a &lt;code>choco install vim&lt;/code>, typing &lt;code>vim&lt;/code> in the terminal just works.&lt;/p>
&lt;p>But Chocolatey &lt;em>feels&lt;/em> bloated: it&amp;rsquo;s slow to run and the hooks it adds to the PowerShell user profile make launching new PowerShell sessions slow. PowerShell has this neat feature to warn when &lt;code>$PROFILE&lt;/code> delays startup, and I often get warnings claiming that the script took between 1 to 2 &lt;em>seconds&lt;/em> to finish. This makes opening new PowerShell sessions feel even more bloated &lt;a href="/2022/03/a-year-on-windows-powershell.html">than what I mentioned in a previous post&lt;/a>, which is no bueno.&lt;/p>
&lt;h1 id="winget">winget&lt;/h1>
&lt;p>Chocolatey is all fine and dandy, but it&amp;rsquo;s one more thing to install. And didn&amp;rsquo;t I say in the opening that Windows now comes with its own package manager? That&amp;rsquo;s right, it does, and it is called &lt;a href="https://docs.microsoft.com/en-us/windows/package-manager/winget/">&lt;strong>Windows Package Manager&lt;/strong> or &lt;code>winget&lt;/code>&lt;/a> for short:&lt;/p>
&lt;figure>
&lt;img src="/images/2022-03-18-winget.png" width="100%" style="max-width: 882px"/>
&lt;figcaption>winget's built-in usage message.&lt;/figcaption>
&lt;/figure>
&lt;p>I did try &lt;code>winget&lt;/code> as soon as it launched in 2020 but couldn&amp;rsquo;t avoid Chocolatey because &lt;code>winget&lt;/code>&amp;rsquo;s software catalog was pretty limited. Things have improved over time, as expected. With my &lt;a href="/2022/03/windows-10-mac-pro-2013.html">most recent Windows install&lt;/a>, I&amp;rsquo;m trying to stay Chocolatey-free and so far I&amp;rsquo;m succeeding. There are some notable tools missing though, like &lt;code>less&lt;/code>, &lt;code>ripgrep&lt;/code> or &lt;code>fd-find&lt;/code>, which I&amp;rsquo;ve had to install on the side. But so far so good.&lt;/p>
&lt;p>&lt;code>winget&lt;/code> definitely feels faster than Chocolatey and having one less tool in the system is nice. But it also has downsides. The biggest one is that you must manually configure the &lt;code>PATH&lt;/code> to point to newly installed software. And fiddling with environment variables on Windows just feels wrong. The UI is much better than it used to be, but still&amp;hellip; a UI? Just give me a file to edit or a &lt;a href="/2020/08/config-files-vs-directories.html">package-friendly &lt;code>.d&lt;/code> directory&lt;/a> like macOS has in &lt;code>/etc/paths.d&lt;/code>!&lt;/p>
&lt;figure>
&lt;img src="/images/2022-03-18-windows-path.png" width="100%" style="max-width: 639px"/>
&lt;figcaption>The &lt;b>Environment variable editor&lt;/b> showing my &lt;tt>PATH&lt;/tt> contents and how I had to add a bunch of directories after using &lt;tt>winget&lt;/tt> to make Git, Vim and Emacs usable.&lt;/figcaption>
&lt;/figure>
&lt;h1 id="uninstallation">Uninstallation&lt;/h1>
&lt;p>OK, so software installation is now great. What about, you know, uninstallation? Having reliable uninstallation procedures is critical for peace of mind: knowing that you can clean up any piece of software you install &amp;ldquo;just for a test drive&amp;rdquo; is powerful. And I&amp;rsquo;m not sure that Chocolatey or &lt;code>winget&lt;/code> are there.&lt;/p>
&lt;p>Whenever you use these package managers for Windows to install software, you quickly realize that, for the most part, they are just command-line wrappers around the original ad-hoc installers. These executables can still do whatever they want and there is zero guarantee that their uninstaller counterpart will do the right thing. Will the uninstaller truly remove all files littered through the system? What about Registry entries?&lt;/p>
&lt;p>Compare this to the package managers you find on Linux or the BSDs. These package managers do not (typically) run installers: they &lt;em>are&lt;/em> the installers. Tools like &lt;code>dpkg&lt;/code> or &lt;code>rpm&lt;/code> are responsible for extracting a package&amp;rsquo;s files and putting those where they belong, and because they did that, they know &lt;em>exactly&lt;/em> what files were installed. When the time comes to uninstall a package, these systems can just walk the list of files they previously registered and remove those.&lt;/p>
&lt;p>Of course, these Unix-y package managers can also run arbitrary scripts at package installation and uninstallation time, so they cannot be trusted to be completely clean either. However, the &lt;em>culture&lt;/em> around these systems means that the people creating their packages go to great extents to ensure that this is true. And there is a difference there. I still remember one of my proudest contributions to Gnome, which was to extend GConf to &lt;a href="https://bugzilla.gnome.org/show_bug.cgi?id=130129">support cleaning up the system-wide database&lt;/a> when removing a package. Wow, 2003 called.&lt;/p>
&lt;p>Anyway. In the case of &lt;code>winget&lt;/code>, this situation is even more confusing because &lt;code>winget&lt;/code> is a facade over &lt;em>both&lt;/em> old-style installers and Store-based software. Take a look and notice the &lt;strong>Source&lt;/strong> column over to the right:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-plain" data-lang="plain">GO2 C:\Users\jmmv&amp;gt; winget search spotify
Name Id Version Match Source
------------------------------------------------------------------------------------------------------
Spotify - Music and Podcasts 9NCBCSZSJRSB Unknown msstore
Spotify Spotify.Spotify 1.1.80.699.gc3dac750 winget
...
&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you use &lt;code>winget&lt;/code> to uninstall software that came from the &lt;code>msstore&lt;/code> source, the uninstallation will be clean. But for software that came from the &lt;code>winget&lt;/code> source&amp;hellip; good luck because then you are subject to &amp;ldquo;regular&amp;rdquo; uninstaller executables.&lt;/p>
&lt;hr>
&lt;p>To recap, it&amp;rsquo;s awesome to see &lt;code>winget&lt;/code> as a standard tool in Windows because this is a huge step forward in system usability and administration. However, I&amp;rsquo;m not fully convinced that this tool can be trusted to do the right thing when uninstalling software, so I&amp;rsquo;m still wary of installing random stuff.&lt;/p>
&lt;p>Tomorrow, it&amp;rsquo;s time to wrap up this series (unless some other topic I did not think about comes up). I&amp;rsquo;ll briefly touch upon the much-hated telemetry and conclude with a brief summary.&lt;/p></description></item><item><title>A year on Windows: System debugging</title><link>https://jmmv.dev/2022/03/a-year-on-windows-debugging.html</link><pubDate>Thu, 17 Mar 2022 06:30:00 -0700</pubDate><guid>https://jmmv.dev/2022/03/a-year-on-windows-debugging.html</guid><description>&lt;p>As you are well-aware, Windows is a closed-source operating system. That, however, does &lt;em>not&lt;/em> mean that it is &lt;em>opaque&lt;/em>. In fact, it feels quite the opposite in many areas, which might be a surprise to you&amp;mdash;especially if you develop on/for open-source operating systems.&lt;/p>
&lt;p>To understand why that might be, put yourself in the shoes of a Windows engineer and consider how Windows must be extensible in various ways: kernel-level drivers, File Manager extensions, PowerShell cmdlets&amp;hellip; you name it. If you start with the premise that you must support these scenarios from a closed-source OS, you will have to design the product in ways that are not common in open source systems. For example:&lt;/p>
&lt;ul>
&lt;li>Supporting loadable binary plugins will be a requirement for extensibility because people can&amp;rsquo;t just edit the code.&lt;/li>
&lt;li>Having clear API documentation will be a must because people won&amp;rsquo;t be able guess what calls are available and how they behave from their implementation.&lt;/li>
&lt;li>Having backwards binary compatibility will be important because users won&amp;rsquo;t be able to recompile whichever piece of software they are using against newer versions of the system.&lt;/li>
&lt;/ul>
&lt;p>Take drivers as a thought exercise. Drivers are a gnarly thing to develop because they run inside the kernel, interact directly with the hardware &lt;em>and&lt;/em> the kernel&amp;rsquo;s interfaces, and any error in kernel-level cannot be easily debugged from userland. Or can it? I&amp;rsquo;ve been reading through the &amp;ldquo;&lt;a href="https://www.amazon.com/Advanced-Windows-Debugging-Mario-Hewardt/dp/0321374460">Advanced Windows Debugging&lt;/a>&amp;rdquo; and &amp;ldquo;&lt;a href="https://www.microsoftpressstore.com/store/windows-internals-9780735625303">Windows Internals, 5th edition&lt;/a>&amp;rdquo; books (haven&amp;rsquo;t made it very far in either though) and it&amp;rsquo;s &lt;em>impressive&lt;/em> how much you can learn about the system without actually having its source code.&lt;/p>
&lt;p>Speaking of debugging and internals, one thing that &amp;ldquo;shocked&amp;rdquo; me when joining Microsoft is how much emphasis is put on using a debugger and how incredibly good some people are at debugging extremely subtle bugs purely from a crash dump. At Google, using a debugger was quite a rarity! And it&amp;rsquo;s equally impressive how good the debugging features are: &lt;a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/time-travel-debugging-overview">Time Travel Debugging&lt;/a> is mind-blowing.&lt;/p>
&lt;h1 id="the-open-source-approach">The open source approach&lt;/h1>
&lt;p>I can&amp;rsquo;t avoid but compare this to Linux or any of the BSDs. I don&amp;rsquo;t think any of these systems reach this level of&amp;hellip; integrability? I know it sounds like a strange thing to say, but relying on their source code for interoperability purposes is both a blessing and a curse.&lt;/p>
&lt;p>It is a blessing because these systems can be simpler by assuming that people can &amp;ldquo;just edit the source and recompile&amp;rdquo;. But it is a curse &lt;em>because&lt;/em> many times you have to recompile things (Nvidia Linux drivers, anyone?) and is what makes it almost impossible to have commercial software running on these platforms. Whether this is a good or bad thing from a &amp;ldquo;freedom&amp;rdquo; perspective is an orthogonal topic.&lt;/p>
&lt;p>Mind you: one of the reasons I liked the BSDs from the very beginning was that they offer an integrated user and development experience (as opposed to Linux), much like Windows did. What I did &lt;em>not&lt;/em> like was that I had to recompile everything I wanted to use from scratch, which was and still is fairly problematic on low-powered machines. As an example, I spent a lot of time getting NetBSD to run on an old PowerBook G4 and the only way to do this was by fiddling with compile-time settings for the kernel. It&amp;rsquo;d have been much easier for me (not the developers!) if these settings had been available as runtime boot loader options.&lt;/p>
&lt;p>For a really long time now, I&amp;rsquo;ve been of the opinion that applications should be written &lt;em>as if&lt;/em> they were closed-source even if they are actually open-source. The same applies to company-internal applications where you can almost always see the source, and this reminds me of &lt;a href="https://gist.github.com/chitchcock/1281611">Steve Yegge&amp;rsquo;s platform rant&lt;/a> (can&amp;rsquo;t find the original now). This is why I spent so much time back in my NetBSD years trying to make the system completely usable and customizable only from binary builds, without having to rebuild anything from scratch (see e.g. &lt;a href="/software/sysupgrade.html">sysupgrade&lt;/a> or &lt;a href="/software/pkg_comp.html">pkg_comp&lt;/a>). My goal was to have a compiler-less home server, and I was able to do for a long time.&lt;/p>
&lt;h1 id="backwards-compatibility">Backwards compatibility&lt;/h1>
&lt;p>Binary-only interfaces come at a huge cost though. Windows is famously-known for its ~forever backwards compatibility at the binary level, which is great for software vendors but I&amp;rsquo;m sure is cringing (or not, as somebody who has actually done this for a living tells me after reading this!) for anyone having to maintain the internal system interfaces.&lt;/p>
&lt;p>There is this article that made the rounds just yesterday titled &lt;a href="https://gankra.github.io/blah/c-isnt-a-language/">&amp;ldquo;C Isn&amp;rsquo;t A Programming Language Anymore&amp;rdquo;&lt;/a> and you should go read at least its &lt;a href="https://gankra.github.io/blah/c-isnt-a-language/#case-study-minidump_handle_data">&amp;ldquo;Case Study: MINIDUMP_HANDLE_DATA&amp;rdquo;&lt;/a> section. Or you can also read &lt;a href="https://devblogs.microsoft.com/oldnewthing/20031015-00/?p=42163">one of the many posts&lt;/a> by Raymond Chen on how Windows has to deal with backwards compatibility.&lt;/p>
&lt;hr>
&lt;p>Enough for today. I&amp;rsquo;m sorry I didn&amp;rsquo;t go into details on how exactly Windows is introspectable and debuggable. The reason is, simply, that I don&amp;rsquo;t know enough yet. I mentioned a couple of books earlier on this topic and, while they are very promising&amp;hellip; I have barely had the time to read them. Tomorrow, we have just one more topic to cover before closing the series, and that will be about package management on Windows.&lt;/p></description></item><item><title>A year on Windows: Networked file systems</title><link>https://jmmv.dev/2022/03/a-year-on-windows-networked-fs.html</link><pubDate>Wed, 16 Mar 2022 06:30:00 -0700</pubDate><guid>https://jmmv.dev/2022/03/a-year-on-windows-networked-fs.html</guid><description>&lt;p>I briefly mentioned in the &lt;a href="/2022/03/a-year-on-windows-intro.html">intro to this series&lt;/a> that, as part of the transition to Windows, I recently built my most powerful home server ever. The server in question is a machine from 2011 so it&amp;rsquo;s not &amp;ldquo;powerful&amp;rdquo;, but it&amp;rsquo;s the best I have ever had as a home server! And it is running FreeBSD 13.&lt;/p>
&lt;p>Wait, wait, wait. What does this have to do with Windows?&lt;/p>
&lt;p>Well, &lt;em>networked file systems&lt;/em>! I had previously tried to use remote storage from my Mac but macOS is pretty awful at accessing network shares:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>SMB, to these days, is still slow and unstable. Is that FUD? Maybe, but I encountered issues just &lt;em>yesterday&lt;/em> when backing up files from my wife&amp;rsquo;s 2-year old MacBook Pro.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>NFS is&amp;hellip; maybe better? I recall trying v3 and not being satisfied, and I have never bothered with v4.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I hear AppleTalk used to be great but it is not a thing anymore.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>FUSE is a headache nowadays given the impeding removal of kernel extensions, and I&amp;rsquo;ll be forever bitter that this essentially killed &lt;a href="/software/sandboxfs.html">sandboxfs&lt;/a> as a real option to &lt;a href="/2018/04/preliminary-sandboxfs-support-in-bazel.html">make Bazel sandboxing high-performance on macOS&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>And so on.&lt;/p>
&lt;p>Yet, from Windows, network shares seem to Just Workâ„¢ï¸.&lt;/p>
&lt;p>Want to connect to an SMB server? Trivial, fast, and stable. Want to connect to an iSCSI volume? Easy too. Do the connections cause system instability when they flake out? Uh, no. Do the connections vanish when you reboot? Heck no. I&amp;rsquo;m sure there are horror stories around this topic&amp;mdash;like any other&amp;mdash;but, for me, things have worked great and I haven&amp;rsquo;t experienced any random problems nor stability issues.&lt;/p>
&lt;hr>
&lt;p>So how did Windows push me to experiment with networking file systems? It wasn&amp;rsquo;t Windows per se. It was me running Windows on an Optiplex 9020 with very limited SSD space. I wanted to see how some of my workflows worked on Windows as part of the adoption of the OS, and I needed extra disk space on that machine to make that possible.&lt;/p>
&lt;p>At the time, my home server was a cool PowerMac G5. I upgraded its RAM, set up a one-drive 4TB ZFS &amp;ldquo;pool&amp;rdquo; to play around, and started using the pool lightly from Windows via Samba. Then I discovered ZFS volumes, trivially set up an iSCSI target to provide scratch space for my machine running Windows, and was marveled at how neat ZFS was.&lt;/p>
&lt;p>The ZFS experiment proved successful: Windows as a client was great when compared to macOS and having my most precious data (photos) scrubbed and snapshotted by ZFS was too hard to pass on.&lt;/p>
&lt;p>Unfortunately, it was clear that the PowerMac G5 was unfit to drive fast I/O. Furthermore, if I wanted to upgrade to a real multi-drive ZFS pool, I needed to fit extra drives into the machine somehow, and it only has two 3.5&amp;quot; drive bays. The real issues came when I upgraded my Internet connection to a Gigabit fiber line: the PowerMac seemed to reach its CPU limits just routing traffic.&lt;/p>
&lt;p>And that&amp;rsquo;s when I got a slightly faster machine to act as a home server and why it&amp;rsquo;s running FreeBSD. I jumped from a machine from 2005 to a machine from 2011, which gave me a custom-built PC with an AMD FX-8120 CPU, 8GB of RAM, an SSD for the OS, and 2x 4TB Seagate 7200RPM drives&amp;mdash;all for a little over $100. I now regret not buying something much more powerful to also run VMs on with &lt;a href="https://bhyve.org/">bhyve&lt;/a>&amp;hellip;&lt;/p>
&lt;p>Anyhow. The server is now exposing my photo collection via Samba and I can access it from Windows with Adobe Lightroom. That machine is also serving an iSCSI volume to offer scratch space for Windows. And I am still amazed at how frictionless the whole thing has been.&lt;/p>
&lt;p>Tomorrow in this series: how Windows as a closed-source OS is highly-introspectable and debuggable. Come back for more!&lt;/p></description></item><item><title>A year on Windows: PowerShell</title><link>https://jmmv.dev/2022/03/a-year-on-windows-powershell.html</link><pubDate>Tue, 15 Mar 2022 07:20:00 -0700</pubDate><guid>https://jmmv.dev/2022/03/a-year-on-windows-powershell.html</guid><description>&lt;p>The native Windows command line, the one derived from DOS, is objectively painful.&lt;/p>
&lt;p>On the one hand, the batch language is &lt;a href="https://devblogs.microsoft.com/oldnewthing/20050909-24/?p=34263">full of hacks&lt;/a> that have cropped up over the years. These hacks exist to offer new features while maintaining strict backwards compatibility, a heroic effort with nasty consequences. On the other hand, the interactive editing features of &lt;code>cmd.exe&lt;/code> are rudimentary&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Fortunately, PowerShell exists as a first-party, built-into-Windows alternative to &lt;code>cmd.exe&lt;/code>.&lt;/p>
&lt;p>PowerShell is a very powerful command-line environment originally designed to support Windows administration. As an advanced shell, it even has neat editing features. Early on in my experimentation, I discovered that you can have a vi editing mode in PowerShell:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-powershell" data-lang="powershell">&lt;span class="nb">Set-PSReadlineOption&lt;/span> &lt;span class="n">-EditMode&lt;/span> &lt;span class="n">vi&lt;/span> &lt;span class="n">-BellStyle&lt;/span> &lt;span class="n">None&lt;/span>
&lt;span class="nb">Set-PSReadlineKeyHandler&lt;/span> &lt;span class="n">-Key&lt;/span> &lt;span class="n">Tab&lt;/span> &lt;span class="n">-Function&lt;/span> &lt;span class="n">MenuComplete&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And just because of this, I was an immediate convert. Navigating the command line efficiently is a must for productivity while programming and this little customization was a game-changer for me.&lt;/p>
&lt;p>Knowing that I had to deal with PowerShell and get better at it, I bought the &lt;a href="https://www.manning.com/books/windows-powershell-in-action">PowerShell in Action&lt;/a> book (the first edition, second hand) and read it cover-to-cover. I didn&amp;rsquo;t &lt;em>really&lt;/em> learn the language this way but I got a general glimpse of it and the original design principles behind this environment. And I have to confess that most of the design principles behind the language are well-aligned with my own sense of design. Objects! Orthogonal syntax! Extensibility via plugins! Yes! So I must like it.&lt;/p>
&lt;p>Mustn&amp;rsquo;t I?&lt;/p>
&lt;h1 id="syntax-difficulties">Syntax difficulties&lt;/h1>
&lt;p>I have not been able to come to terms with PowerShell&amp;rsquo;s verbosity yet. Yes, piping objects and doing fancy things on them is extremely powerful, but most of the time I just want to do things like &lt;code>ps ax | grep badprocess&lt;/code> and&amp;hellip; it hurts. &lt;code>Get-Process | ? { $_.Name -like &amp;quot;*badprocess*&amp;quot; }&lt;/code> or its simpler variant &lt;code>ps | ? Name -like badprocess&lt;/code> don&amp;rsquo;t seem much longer than their Unix equivalent, but the presence of various non-alphanumeric characters make them more tedious to type.&lt;/p>
&lt;p>Array-like arguments are another difference that bites me every single time. I still haven&amp;rsquo;t been able to get over the fact that &lt;code>cp a b c&lt;/code> should be &lt;code>cp a,b c&lt;/code>, and that &lt;code>rm a b c&lt;/code> should be &lt;code>rm a,b,c&lt;/code>. These make a ton sense from a design perspective but my muscle memory hasn&amp;rsquo;t accepted them.&lt;/p>
&lt;h1 id="system-and-tools-integration">System and tools integration&lt;/h1>
&lt;p>As the book I read puts it, PowerShell is&amp;hellip; a late addition to Windows administration. It would be awesome if there were cmdlets for all imaginable things you would want to do from the command line and interact with Windows without touching the GUI at all&amp;hellip; but that&amp;rsquo;s not the case.&lt;/p>
&lt;p>There will be cases where you have to fall back to &amp;ldquo;old&amp;rdquo; tools that spit out plain text instead of objects because they don&amp;rsquo;t have counterparts in PowerShell. The book mentioned that this would get better over time (and it has), but the book edition that I read was published in 2007 and I&amp;rsquo;m still finding these scenarios in 2022.&lt;/p>
&lt;p>Similarly, a lot of the tools we use these days on the command line come from Unix systems. Git is a must for any developer&amp;rsquo;s toolbox. This heterogeneity of tools is problematic because they all have different command-line parsing and quoting rules, which as I mentioned before is &lt;a href="/2020/11/cmdline-args-unix-vs-windows.html">a core problem&lt;/a> of Windows' process execution. Path delimiter differences between Windows and what original Unix tools expect are a nightmare as well in this regard.&lt;/p>
&lt;p>And that&amp;rsquo;s just scratching the surface. There are other scenarios where you&amp;rsquo;ll have to use the Windows Management Instrumentation (WMI) to get what you want, and in some others you&amp;rsquo;ll have to write actual code&amp;mdash;in the shell!&amp;mdash;to load DLLs and execute their functions. These are both extremely powerful, but the syntaxes are not consistent with native PowerShell&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> and thus are not super convenient to use.&lt;/p>
&lt;h1 id="speed-or-not">Speed&amp;hellip; or not&lt;/h1>
&lt;p>PowerShell is yet another Windows component that feels bloated. On my Surface Go 2, opening a PowerShell session after a cold boot takes about &lt;strong>5 seconds&lt;/strong> while opening a &lt;code>cmd.exe&lt;/code> instance takes a few milliseconds only. Subsequent executions of PowerShell are in the hundreds of milliseconds range as well, but that first execution is painful and I&amp;rsquo;m sure it&amp;rsquo;s because of the need to bring up the .NET VM.&lt;/p>
&lt;p>And you&amp;rsquo;d say that this problem is because the Go 2 is underpowered, which is partially true, but we are talking about massive differences in load times that are noticeable even on powerful hardware. Launching terminals feels sluggish in all machines I&amp;rsquo;ve tried so far, which still gives me this feeling that the console is not an integral part of the system. (Yeah, it has never been, but we already knew that.)&lt;/p>
&lt;h1 id="file-encodings">File encodings&lt;/h1>
&lt;p>And because we are talking about the shell, it&amp;rsquo;s important that we talk about file encodings and line endings as well.&lt;/p>
&lt;p>In the Unix world, UTF-8 and LF line endings are pretty much all you will see these days. Not so much on Windows. For historical reasons, Windows APIs favor UTF-16 and text files use CRLF as line endings in addition to being UTF-16.&lt;/p>
&lt;p>Differences in encodings and line endings become a problem when you try to interoperate with Unix systems and, of course, WSL. Creating files on Windows, via the GUI or PowerShell, and then interacting with them via WSL is a &amp;ldquo;fun&amp;rdquo; experience. Say you create a text file in UTF-16 by &amp;ldquo;mistake&amp;rdquo;. Voila. &lt;code>grep&lt;/code> won&amp;rsquo;t work on it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">PS C:\Users\jmmv&amp;gt; echo &amp;#39;hello&amp;#39; &amp;gt;test.txt
PS C:\Users\jmmv&amp;gt; wsl
$ grep &amp;#39;hell&amp;#39; test.txt
$ file test.txt
test.txt: Unicode text, UTF-16, little-endian text, with CRLF line terminators
&lt;/code>&lt;/pre>&lt;/div>&lt;p>I&amp;rsquo;ve found myself reaching for &lt;code>iconv&lt;/code> and &lt;code>dos2unix&lt;/code> quite frequently, which are tools I had not touched &lt;em>in years&lt;/em>, and have also had to learn how to add a BOM to an UTF-8 file by hand. Granted, this is my privilege because I primarily write in English only these days&amp;hellip;&lt;/p>
&lt;p>On a more positive side of things, BOM-less UTF-8 output is the default for PowerShell (Core) v6+ so you would not notice the above problem if you are running a modern version of this shell. Unfortunately, the version that ships with Windows is older and I haven&amp;rsquo;t yet dared to install the more modern one; maybe I should.&lt;/p>
&lt;p>As a tip: if you are going to work on the same Git repository from Unix systems (including WSL) &lt;em>and&lt;/em> Windows, configure Git to normalize line endings. This is as easy as creating a &lt;code>.gitattributes&lt;/code> file at the top of the repository with the following contents:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">* text=auto
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Do this ASAP before you create a mess of different line endings in the checked-in files.&lt;/p>
&lt;h1 id="up-next">Up next&lt;/h1>
&lt;p>I use PowerShell daily now for work purposes and I&amp;rsquo;m much more comfortable with it than I was a year ago. I know that there are tons of people out there happily using this shell on a regular basis and are much more productive than I am at it, so my difficulties are primarily due to my own lack of knowledge. Unfortunately, I&amp;rsquo;m not convinced that it&amp;rsquo;s worth investing much more of my learning effort on it because &lt;a href="/2022/03/a-year-on-windows-devel.html">most of my time goes into WSL anyway&lt;/a>.&lt;/p>
&lt;p>If you like PowerShell at all, be it its specific implementation or the design concepts behind it, I should point you towards &lt;a href="https://www.nushell.sh/">Nushell&lt;/a>&amp;mdash;a new shell for Unix platforms that shares many of these design ideas. I haven&amp;rsquo;t used it though.&lt;/p>
&lt;p>Tomorrow, let&amp;rsquo;s talk about networked file systems and we&amp;rsquo;ll be approaching the end of this long series!&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Now that I&amp;rsquo;m &lt;em>finally&lt;/em> &lt;a href="/2022/03/a-year-on-windows-shortcuts.html">adjusting to Windows shortcuts&lt;/a>, I&amp;rsquo;m discovering that the old console is a bit richer than what I originally thought regarding editing features, but it&amp;rsquo;s still&amp;hellip; very limited when compared to other shells.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>As I was writing this article, I have found out that &lt;code>wmic.exe&lt;/code> is finally deprecated and that, as of Windows 10 21H2, PowerShell has &lt;a href="https://docs.microsoft.com/en-us/powershell/scripting/learn/ps101/07-working-with-wmi?view=powershell-7.2">native interaction points with WMI&lt;/a>. One less source of inconsistencies but it has taken years to get here!&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item></channel></rss>